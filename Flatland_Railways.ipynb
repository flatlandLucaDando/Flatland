{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flatlandLucaDando/Flatland/blob/flatland_v_3_deterministic/Flatland_Railways.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C2D4VVH366B"
      },
      "source": [
        "# ISTRUZIONI\n",
        "\n",
        "Bisogna eseguire le caselle in ordine, alcune caselle sono molto importanti per configurare l'esempio da utilizzare.\n",
        "\n",
        "*   Structure rail definisce i binari (destri, sinistri, su e giù)\n",
        "*   Example definisce l'esempio (l'infrastruttura) che si vuole considerare\n",
        "*   Rewards and Penalities definisce i valori delle rewards\n",
        "*   Training flag definisce il training che si vuole considerare\n",
        "*   In Configuration si può variare volendo la configurazione\n",
        "*   Simulation Values sono i valori della simulazione, come la lunghezza degli episodi, il numero di episodi etc etc\n",
        "\n",
        "Tutte queste celle di codice possono essere modificate per variare i test e training che si vogliono eseguire\n",
        "\n",
        "---\n",
        "\n",
        "P.s. La prima cella restituisce un errore ma non ci sono problemi, il tutto funge lo stesso\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ClfqgfTCe9"
      },
      "source": [
        "# Introduction \n",
        "\n",
        "---\n",
        "\n",
        "This is a Workbook to execute the Flatland-RealWorld 3.06 code on colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CucPq4ISq0N"
      },
      "source": [
        "First thing is important to import some libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5WlT8daadWG",
        "outputId": "85cd7612-6d19-4b6f-cf0e-97a52003f294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Flatland'...\n",
            "remote: Enumerating objects: 3479, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 3479 (delta 8), reused 0 (delta 0), pack-reused 3464\u001b[K\n",
            "Receiving objects: 100% (3479/3479), 19.42 MiB | 20.89 MiB/s, done.\n",
            "Resolving deltas: 100% (1806/1806), done.\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Collecting flatland-rl\n",
            "  Downloading flatland_rl-3.0.8-py2.py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 3.5 MB/s \n",
            "\u001b[?25hCollecting crowdai-api>=0.1.21\n",
            "  Downloading crowdai_api-0.1.22.tar.gz (9.2 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (21.2.0)\n",
            "Requirement already satisfied: pyarrow>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (3.0.0)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: Pillow>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (7.1.2)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (7.1.2)\n",
            "Collecting pytest-runner>=4.2\n",
            "  Using cached pytest_runner-5.3.1-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (2.6.3)\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (3.2.2)\n",
            "Collecting tox>=3.5.2\n",
            "  Downloading tox-3.24.4-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting svgutils>=0.3.1\n",
            "  Downloading svgutils-0.3.4-py3-none-any.whl (10 kB)\n",
            "Collecting pytest<5,>=3.8.2\n",
            "  Downloading pytest-4.6.11-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 37.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (0.10.1)\n",
            "Collecting timeout-decorator>=0.4.1\n",
            "  Downloading timeout-decorator-0.5.0.tar.gz (4.8 kB)\n",
            "Collecting recordtype>=1.3\n",
            "  Downloading recordtype-1.3-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (1.1.5)\n",
            "Collecting msgpack-numpy>=0.4.4.0\n",
            "  Downloading msgpack_numpy-0.4.7.1-py2.py3-none-any.whl (6.7 kB)\n",
            "Collecting ipycanvas\n",
            "  Downloading ipycanvas-0.10.2-py2.py3-none-any.whl (253 kB)\n",
            "\u001b[K     |████████████████████████████████| 253 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting gym==0.14.0\n",
            "  Downloading gym-0.14.0.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 32.8 MB/s \n",
            "\u001b[?25hCollecting msgpack==0.6.1\n",
            "  Downloading msgpack-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (245 kB)\n",
            "\u001b[K     |████████████████████████████████| 245 kB 31.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (2.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.17 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (4.8.2)\n",
            "Collecting importlib-resources<2,>=1.0.1\n",
            "  Downloading importlib_resources-1.5.0-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.14.0->flatland-rl) (1.4.1)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "  Downloading pyglet-1.3.2-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 31.8 MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=1.2.0\n",
            "  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from crowdai-api>=0.1.21->flatland-rl) (2.23.0)\n",
            "Collecting python-gitlab>=1.3.0\n",
            "  Downloading python_gitlab-2.10.1-py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 37.4 MB/s \n",
            "\u001b[?25hCollecting redis\n",
            "  Downloading redis-4.0.2-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.17->flatland-rl) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.17->flatland-rl) (3.6.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->flatland-rl) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->flatland-rl) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->flatland-rl) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->flatland-rl) (3.0.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->flatland-rl) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.14.0->flatland-rl) (0.16.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (8.12.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (0.2.5)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (1.11.0)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (1.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (21.3)\n",
            "Collecting requests>=2.18.4\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 713 kB/s \n",
            "\u001b[?25hCollecting requests-toolbelt>=0.9.1\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2.0.8)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from svgutils>=0.3.1->flatland-rl) (4.2.6)\n",
            "Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.7/dist-packages (from tox>=3.5.2->flatland-rl) (0.10.2)\n",
            "Collecting virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0\n",
            "  Downloading virtualenv-20.10.0-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 33.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from tox>=3.5.2->flatland-rl) (3.4.0)\n",
            "Collecting backports.entry-points-selectable>=1.0.4\n",
            "  Downloading backports.entry_points_selectable-1.1.1-py2.py3-none-any.whl (6.2 kB)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 38.5 MB/s \n",
            "\u001b[?25hCollecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.4.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: ipywidgets>=7.6.0 in /usr/local/lib/python3.7/dist-packages (from ipycanvas->flatland-rl) (7.6.5)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.1.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (4.10.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (3.5.2)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (1.0.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (4.8.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (4.9.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (2.6.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (4.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.5.1)\n",
            "Collecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->redis->crowdai-api>=0.1.21->flatland-rl) (1.13.3)\n",
            "Building wheels for collected packages: gym, crowdai-api, timeout-decorator\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.14.0-py3-none-any.whl size=1637522 sha256=a78cd5a5f27cb94adbe1853b52a982cfd34ab1bbcba980c12bf1c262c0f60b94\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/34/78/36550f249167fda9e42e1dd9af84b400abf6c162d1c07ab4e1\n",
            "  Building wheel for crowdai-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crowdai-api: filename=crowdai_api-0.1.22-py2.py3-none-any.whl size=10001 sha256=668aec58d841dcdb9ccdc52d6e887cf6115c5d16e27ccb4aedf962de9ff7f4ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/f9/9b/8d1b851e4636aee2ba22b033bdb893e75b4342fa9865c39e23\n",
            "  Building wheel for timeout-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timeout-decorator: filename=timeout_decorator-0.5.0-py3-none-any.whl size=5028 sha256=e355927aca11813b00a35cdc929b4185458eb58da49282ecac3d834403c6dd33\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/64/ac/de1dd54f9a6e48b846e9cb5e4176d6f063380e7f83d69807ad\n",
            "Successfully built gym crowdai-api timeout-decorator\n",
            "Installing collected packages: requests, requests-toolbelt, platformdirs, distlib, deprecated, backports.entry-points-selectable, virtualenv, redis, python-gitlab, pyglet, pluggy, msgpack, cloudpickle, tox, timeout-decorator, svgutils, recordtype, pytest-runner, pytest, msgpack-numpy, ipycanvas, importlib-resources, gym, dataclasses, crowdai-api, flatland-rl\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyglet\n",
            "    Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.0.3\n",
            "    Uninstalling msgpack-1.0.3:\n",
            "      Successfully uninstalled msgpack-1.0.3\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 5.4.0\n",
            "    Uninstalling importlib-resources-5.4.0:\n",
            "      Successfully uninstalled importlib-resources-5.4.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires cloudpickle>=1.3, but you have cloudpickle 1.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed backports.entry-points-selectable-1.1.1 cloudpickle-1.2.2 crowdai-api-0.1.22 dataclasses-0.6 deprecated-1.2.13 distlib-0.3.4 flatland-rl-3.0.8 gym-0.14.0 importlib-resources-1.5.0 ipycanvas-0.10.2 msgpack-0.6.1 msgpack-numpy-0.4.7.1 platformdirs-2.4.0 pluggy-0.13.1 pyglet-1.3.2 pytest-4.6.11 pytest-runner-5.3.1 python-gitlab-2.10.1 recordtype-1.3 redis-4.0.2 requests-2.26.0 requests-toolbelt-0.9.1 svgutils-0.3.4 timeout-decorator-0.5.0 tox-3.24.4 virtualenv-20.10.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    # Clone GitHub repository\n",
        "    !git clone --single-branch --branch flatland_v_3_deterministic https://github.com/flatlandLucaDando/Flatland.git\n",
        "    # Copy files required to run the code\n",
        "    !pip install cloudpickle\n",
        "    !pip install flatland-rl\n",
        "\n",
        "    # Restart Runtime\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M0SHU5OFSwlp"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "import random\n",
        "import sys\n",
        "from argparse import ArgumentParser, Namespace\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "import psutil\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch\n",
        "from typing import Callable, Tuple, Optional, Dict, List\n",
        "from numpy.random.mtrand import RandomState"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e5ILy1PYV9o"
      },
      "source": [
        "# Structure Rail\n",
        "Contein the basilar structure of the rail, with the high velocity rails and the right, left, nord or sud rails\n",
        "\n",
        "With i_flag you can select the different rails configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q8M6jhCIYvPL"
      },
      "outputs": [],
      "source": [
        "i = 3\n",
        "\n",
        "if i == 1:\n",
        "\t# One rail, so no right or left rails  \n",
        "\tright_rails = [(0,0)]\n",
        "\tleft_rails = [(0,0)]\n",
        "\tdown_rails = [(0,0)]\n",
        "\tup_rails = [(0,0)]\n",
        "\n",
        "\tav_line = [(0,0)]\n",
        "\n",
        "if i ==2:\n",
        "\t# Rails where the direction is right\n",
        "\tright_rails = []\n",
        "\tfor i in range(8):\n",
        "\t\tright_rails.append((5,i+6))\n",
        "\t# Rails where the direction is left\n",
        "\tleft_rails = []\n",
        "\tfor i in range(8):\n",
        "\t\tleft_rails.append((6,i+6))\n",
        "\n",
        "\tdown_rails = [(0,0)]\n",
        "\tup_rails = [(0,0)]\n",
        "\n",
        "\tav_line = [(0,0)]\n",
        "\n",
        "if i == 3:\n",
        "\t# One rail, so no right or left rails  \n",
        "\tright_rails = [(0,0)]\n",
        "\tleft_rails = [(0,0)]\n",
        "\tdown_rails = [(0,0)]\n",
        "\tup_rails = [(0,0)]\n",
        "\n",
        "\tav_line = [(0,0)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25VWq_QpYWH0"
      },
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GZmMdgAZO5Nc"
      },
      "outputs": [],
      "source": [
        "from flatland.core.grid.rail_env_grid import RailEnvTransitions\n",
        "from flatland.core.transition_map import GridTransitionMap\n",
        "\n",
        "'''\n",
        "###############################################################\n",
        "######################   EXAMPLE LUCA  #########################\n",
        "###############################################################\n",
        "'''\n",
        "\n",
        "# Example generate a rail given a manual specification,\n",
        "# a map of tuples (cell_type, rotation)\n",
        "\n",
        "# Example 2 simple railway with 2 stations and two rails and multiple rails in the stations\n",
        "railway_example = [[(0,0)] * 25,\n",
        "\t\t\t\t   [(0,0)] * 6 + [(9,270)] +[(1,90)]*13 + [(8,90)]  +[(0,0)] *4,\n",
        "\t\t\t\t   [(0,0)] * 6 +[(1,0)]+ [(9,270)] + [(1,90)]*10 + [(8,90)]  + [(0,0)]+ [(1,0)] +[(0,0)] *4,\n",
        "\t\t\t\t   [(0,0)] * 6 +[(1,0)]*2 +  [(0,0)] * 10+ [(1,0)] + [(0,0)]+ [(1,0)]  + [(0,0)] *4,\n",
        "\t\t\t\t   [(7, 270)] + [(1,90)] * 3 + [(8,90)] + [(0,0)] + [(1,0)]*2 +[(0,0)] *2 + [(8,0)] +  [(1,90)] * 3+ [(8,90)]+ [(0,0)] *3 + [(1,0)] +[(0,0)]+  [(1,0)] +[(0,0)]+ [(8,0)]+   [(1,90)] + [(7, 90)] ,\n",
        "\t\t\t\t   [(7, 270)] + [(1,90)] * 3 + [(5,270)] + [(2,270)] + [(5,0)] + [(2,90)] + [(10,90)] + [(2,270)]+ [(2,90)]  +[(1,90)] * 3+ [(10,270)] + [(10,90)]  + [(2,270)] + [(1,90)] + [(5,180)] + [(1,90)] + [(5,270)] + [(1,90)]+  [(5,180)] +[(1,90)]  + [(7, 90)] ,\n",
        "\t\t\t\t   [(7, 270)] + [(1,90)] * 3 + [(10,270)] + [(2,90)] +[(2,90)] +[(1,90)] + [(10,270)] + [(2,90)] + [(1,90)] *5+ [(10,270)] + [(2,90)]+ [(1,90)] + [(10,270)]+ [(1,90)] + [(10,270)]  +[(1,90)] + [(10,270)] +[(1,90)] + [(7, 90)],\n",
        "\t\t\t\t   [(0,0)] * 25,\n",
        "\t\t\t\t   [(0,0)] * 25]\n",
        "\n",
        "# wheight and height of the grid\n",
        "height = len(railway_example)\n",
        "width = len(railway_example[0])\n",
        "\n",
        "# creating the transition map\n",
        "rail_env_transitions = RailEnvTransitions()\n",
        "rail = GridTransitionMap(width=width, height=height, transitions=rail_env_transitions)\n",
        "\n",
        "for r in range(height):\n",
        "\tfor c in range(width):\n",
        "\t\trail_spec_of_cell = railway_example[r][c]\n",
        "\t\tindex_basic_type_of_cell_ = rail_spec_of_cell[0]\n",
        "\t\trotation_cell_ = rail_spec_of_cell[1]\n",
        "\t\tif index_basic_type_of_cell_ < 0 or index_basic_type_of_cell_ >= len(rail_env_transitions.transitions):\n",
        "\t\t\tprint(\"ERROR - invalid rail_spec_of_cell type=\", index_basic_type_of_cell_)\n",
        "\t\tbasic_type_of_cell_ = rail_env_transitions.transitions[index_basic_type_of_cell_]\n",
        "\t\teffective_transition_cell = rail_env_transitions.rotate_transition(basic_type_of_cell_, rotation_cell_)\n",
        "\t\trail.set_transitions((r, c), effective_transition_cell)\n",
        "\n",
        "# No high velocity lines, so make a (0,0) position\n",
        "\tav_line = (0,0)\n",
        "\t\n",
        "\t# TODO sistema i binari destra e sinistra e su e giù\n",
        "\t# Rails where the direction is right\n",
        "\tright_rails = [(0,0)]\n",
        "\t# Rails where the direction is left\n",
        "\tleft_rails = [(0,0)]\n",
        "\tdown_rails = [(0,0)]\n",
        "\tup_rails = [(0,0)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEEkAbhEXvTb"
      },
      "source": [
        "# Data Structures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tCkoOAPX3WZ"
      },
      "source": [
        "## Convoy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mH2WxoxeXyDg"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "import itertools\n",
        "\n",
        "# Type of a convoy, can be high velocity or regional\n",
        "# The velocity are given by default depending on the type of convoy, 360 for HV, 180 for IC, 120 for Regional\n",
        "class Type_of_convoy(Enum):\n",
        "\tHIGH_VELOCITY = 1\n",
        "\tINTERCITY = 2\n",
        "\tREGIONAL = 3\n",
        "\n",
        "# A convoy is a locomotive + wagons.\n",
        "class Convoy:\n",
        "\n",
        "\t# For the id\n",
        "\tid_iter = itertools.count()\n",
        "\n",
        "\tdef __init__(self, train_type, schedule = []):\n",
        "\n",
        "\t\t# identifier of the train\n",
        "\t\tself.id = next(Convoy.id_iter)\n",
        "\t\t# type of train (High velocity, Intercity, regional)\n",
        "\t\tself.train_type = train_type\n",
        "\t\t# schedule of the train\n",
        "\t\tself.schedule = []\n",
        "\n",
        "\t\tif train_type == Type_of_convoy.HIGH_VELOCITY:\n",
        "\t\t\tself.maximum_velocity = 1\n",
        "\t\tif train_type == Type_of_convoy.INTERCITY:\n",
        "\t\t\tself.maximum_velocity = 1/2\n",
        "\t\tif train_type == Type_of_convoy.REGIONAL:\n",
        "\t\t\tself.maximum_velocity = 1/3\n",
        "\n",
        "\tdef add_train_run(self, train_run):\n",
        "\t\tself.schedule.append(train_run)\n",
        "\n",
        "\t# Discover the starting time of a certain run\n",
        "\tdef starting_time(self, run_number):\n",
        "\t\treturn self.schedule[run_number][0]\n",
        "\n",
        "\t# Convert velocity (maximum possible velocity is 360)\n",
        "\tdef velocity_conversion(self):\n",
        "\t\tprint(self.maximum_velocity * 360)\n",
        "\n",
        "\t# Verificate that a schedule is possible (if someone want to write manually)\n",
        "\tdef schedule_verification(self, schedule, num_trains_run):\n",
        "\t\tif type(schedule) == list:\n",
        "\t\t\trow_num = len(schedule)\n",
        "\t\t\tcolumn_num = len(schedule[0])\n",
        "\t\t\tif row_num >  1:\n",
        "\t\t\t\tfor num_of_runs in range(num_trains_run - 1):\n",
        "\t\t\t\t\tfor steps in range(len(schedule) - 1):\n",
        "\t\t\t\t\t\tif schedule[num_of_runs][step + 1] <= schedule[num_of_runs][step]:\n",
        "\t\t\t\t\t\t\tprint('==========================================================')\n",
        "\t\t\t\t\t\t\tprint('The time to connect stations',step,'and',step + 1,'have to be > 0')\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\tfor steps in range(len(schedule) - 1):\n",
        "\t\t\t\t\tif schedule[num_of_runs][step + 1] <= schedule[num_of_runs][step]:\n",
        "\t\t\t\t\t\tprint('==========================================================')\n",
        "\t\t\t\t\t\tprint('The time to connect stations',step,'and',step + 1,'have to be > 0')\n",
        "\t\telse:\n",
        "\t\t\tprint('A schedule should comprend different stations')\n",
        "\t\treturn True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q80GIJJEX8Yk"
      },
      "source": [
        "## Line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gG_B-nASX-r1"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "# A line is considered from a city to another, tipically joining several cities\n",
        "class Line:\n",
        "\n",
        "\t# For the id\n",
        "\tid_iter = itertools.count()\n",
        "\n",
        "\tdef __init__(self, type_line, stations, stops):\n",
        "\t\t# ID of the line\n",
        "\t\tself.id = next(Line.id_iter)\n",
        "\t\t# type of line (High velocity or regional)\n",
        "\t\tself.type_line = type_line\n",
        "\t\t# Stations where the line pass from\n",
        "\t\tself.stations = stations\n",
        "\t\t# Stops are stations where the train have to stop\n",
        "\t\t# Is an array with 0 where not stop and 1 where train stops\n",
        "\t\tself.stops = stops\n",
        "\n",
        "\t\tif type(stations) == int or type(stops) == int:\n",
        "\t\t\tprint('The stations of a line should be more than one, and the dimension of the stops should be the same of the stations')\n",
        "\t\telse:\n",
        "\t\t\tif(len(stations)) != (len(stops)):\n",
        "\t\t\t\tprint('Stations and Stops have to be the same lenght')\n",
        "\n",
        "\n",
        "\tdef inversion_of_line(self):\n",
        "\t\tself.direction = self.direction * -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcqtL8kcYBb3"
      },
      "source": [
        "## Rail Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KZXAlq4oYFw6"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "import itertools\n",
        "\n",
        "# A connection can be: High velocity or normal\n",
        "# The velocity are given by default depending on the type of connection, 360 for HV and 120 for normal\n",
        "class Connection_type(Enum):\n",
        "\tHIGH_VELOCITY_RAIL = 1\n",
        "\tNORMAL_RAIL = 2\n",
        "\n",
        "\n",
        "# A physical connection between two stations\n",
        "class Rail_connection:\n",
        "\n",
        "\t# For the id\n",
        "\tid_iter = itertools.count()\n",
        "\n",
        "\tdef __init__(self, station_a, station_b, rail_connection_type, additional_runtime_percent, max_speed_usable: int = 1/2):\n",
        "\t\t# each railway section has an id\n",
        "\t\tself.id = next(Rail_connection.id_iter)\n",
        "\t\t# Station A and B are the two connected stations\n",
        "\t\tself.station_a = station_a\n",
        "\t\tself.station_b = station_b\n",
        "\t\t# A connection can be: High velocity or normal\n",
        "\t\tself.rail_connection_type = rail_connection_type\n",
        "\t\t# Maximum speed usable in the rails\n",
        "\t\tself.max_speed_usable = max_speed_usable\n",
        "\t\t# Additional Runtime Percent is the percent [0-1] of the min run time that is added to the min run time, if the train is on schedule.\n",
        "\t\t# In general, he actual run time is computed as min run time + max(0, (min run time*additionalRuntimePercent)-actual_delay).\n",
        "\t\tself.additional_runtime_percent = additional_runtime_percent\n",
        "\n",
        "\t\tif rail_connection_type == Connection_type.HIGH_VELOCITY_RAIL:\n",
        "\t\t\tself.max_speed_usable = 1\n",
        "\t\tif rail_connection_type == Connection_type.NORMAL_RAIL:\n",
        "\t\t\tself.max_speed_usable = 1 / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDV9yKClYIy_"
      },
      "source": [
        "## Station"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t8gBNXfkYMss"
      },
      "outputs": [],
      "source": [
        "from flatland.core.grid.grid_utils import Vec2dOperations as Vec2d\n",
        "\n",
        "# Station\n",
        "class Station:\n",
        "\n",
        "\t# TODO metti valori precisi per min wait time (lis)\n",
        "\tdef __init__(self, name, position, capacity, min_wait_time, additional_wait_percent, importance, railway_topology):\n",
        "\t\t# Name of the station (e.g. Milano, Torino etc etc)\n",
        "\t\tself.name = name\n",
        "\t\t# Position (y,x) of the station in the railway\n",
        "\t\tself.position = position\n",
        "\t\t# Capacity of the station, num of rails in the station\n",
        "\t\tself.capacity = capacity\n",
        "\t\t# Minimum wait time for the trains, a train can't stop less than the min wait time\n",
        "\t\tself.min_wait_time = min_wait_time\n",
        "\t\t# Additional wait percent is the percent [0-1] of the minWaitTime that is added to the minWait at each stop, if the train is on schedule. \n",
        "\t\t# In general, the actual (runtime) stopping time is computed as minWaitTime + max(0, (minWaitTime*additionalWaitTimePercent)-actual_delay).\n",
        "\t\tself.additional_wait_percent = additional_wait_percent\n",
        "\t\t# Stations have different importance depending on how much they are big and how much people they transport depending on the time\n",
        "\t\tself.importance = importance\n",
        "\t\t\t\t# Rails of the station\n",
        "\t\tself.rails = self.calculate_rails(railway_topology)\n",
        "\t\treturn\n",
        "\n",
        "\n",
        "\tdef time_in_station(self, train_velocity):\n",
        "\t\t# The len of the rails is given by the station\n",
        "\t\tlen_rails = len(self.rails_in_station[0])\n",
        "\t\t# The time needed is given by the formula (len * 1/velocity + waiting time + 10% of time)\n",
        "\t\ttime_needed =  len_rails * int(pow(train_velocity, -1)) + self.min_wait_time[0]\n",
        "\t\ttime_needed += int(time_needed/10)\n",
        "\t\treturn time_needed\n",
        "\n",
        "\tdef calculate_rails(self, railway_topology):\n",
        "\t\t# Number of rails of the station\n",
        "\t\tnum_of_rails = self.capacity\n",
        "\t\tcenter_of_station = self.position \n",
        "\t\trail_shape = railway_topology.grid.shape\n",
        "\n",
        "\t\t#Flag\n",
        "\t\tleft = True  # Flag to understand where to go right or left\n",
        "\t\tnorth = True    # Flag to understand where to go right or left\n",
        "\n",
        "\t\t# Indicating the incrementing number north or sud (in case of horizontal station), east ovest (in case of vertical stations)\n",
        "\t\tdifference_from_original = 0 \n",
        "\n",
        "\t\t# Counter\n",
        "\t\tcounter_of_rails = 1\n",
        "\t\t# Rails of the station, has the position of the rails. Each row is a rail\n",
        "\t\tself.rails = []\n",
        "  \n",
        "\t\t# Counter to check the station is well positioned, to avoid the while goes for eternity\n",
        "\t\tcounter = 0\n",
        "\n",
        "\t\t# Starting position the center of station\n",
        "\t\tcurrent_position = (self.position[0], self.position[1])\n",
        "  \n",
        "\t\t# The first rail coincide with the position of the station\n",
        "\t\tself.rails.append(current_position)\n",
        "\n",
        "\t\twhile counter_of_rails < num_of_rails:\n",
        "\n",
        "\t\t\tcounter += 1\n",
        "\n",
        "\t\t\tif counter > 500:\n",
        "\t\t\t\traise ImportError('The position of the station, or the capacity should be different, check for the right position or capacity, cant calculate the rails')\n",
        "\n",
        "\t\t\t# Horizontal rail\n",
        "\t\t\t# is position inside the grid?\n",
        "\t\t\telif current_position[0] >= rail_shape[0] or current_position[0] < 0 or current_position[1] >= rail_shape[1] or current_position[1] < 0:\n",
        "\t\t\t\tcontinue \n",
        "\t\t\t\n",
        "\t\t\t# Starting going up to check the rails\n",
        "\t\t\telif railway_topology.grid[current_position] == 1025 and north:\n",
        "\t\t\t\t# Going to up\n",
        "\t\t\t\tnew_pos = (-1, 0)\n",
        "\t\t\t\t# Update the position to left or right\n",
        "\t\t\t\tcurrent_position = Vec2d.add(current_position, new_pos)\n",
        "\t\t\t\t# If not 0 the rail is in the station\n",
        "\t\t\t\tif railway_topology.grid[current_position] != 0:\n",
        "\t\t\t\t\tself.rails.append(current_position)\n",
        "\t\t\t\t\tcounter_of_rails += 1\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tcurrent_position = (self.position[0], self.position[1])\n",
        "\t\t\t\t\tnorth = False\n",
        "\t\t\t\t\tcontinue\t\t\t\n",
        "\n",
        "\t\t\telif railway_topology.grid[current_position] == 1025 and not north:\n",
        "\t\t\t\t# Going to up\n",
        "\t\t\t\tnew_pos = (1, 0)\n",
        "\t\t\t\t# Update the position to left or right\n",
        "\t\t\t\tcurrent_position = Vec2d.add(current_position, new_pos)\n",
        "\t\t\t\t# If not 0 the rail is in the station\n",
        "\t\t\t\tif railway_topology.grid[current_position] != 0:\n",
        "\t\t\t\t\tself.rails.append(current_position)\n",
        "\t\t\t\t\tcounter_of_rails += 1\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tcontinue\n",
        " \n",
        "\t\t\t# Vertical rail same as for horizontal, but starting from south and north, and then left and right\n",
        "\t\t\telif railway_topology.grid[current_position] == 32800 and left:\n",
        "\t\t\t\t# Going to up\n",
        "\t\t\t\tnew_pos = (0, -1)\n",
        "\t\t\t\t# Update the position to left or right\n",
        "\t\t\t\tcurrent_position = Vec2d.add(current_position, new_pos)\n",
        "\t\t\t\t# If not 0 the rail is in the station\n",
        "\t\t\t\tif railway_topology.grid[current_position] != 0:\n",
        "\t\t\t\t\tself.rails.append(current_position)\n",
        "\t\t\t\t\tcounter_of_rails += 1\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tcurrent_position = (self.position[0], self.position[1])\n",
        "\t\t\t\t\tleft = False\n",
        "\t\t\t\t\tcontinue\t\t\t\n",
        "\n",
        "\t\t\telif railway_topology.grid[current_position] == 32800 and not left:\n",
        "\t\t\t\t# Going to up\n",
        "\t\t\t\tnew_pos = (0, 1)\n",
        "\t\t\t\t# Update the position to left or right\n",
        "\t\t\t\tcurrent_position = Vec2d.add(current_position, new_pos)\n",
        "\t\t\t\t# If not 0 the rail is in the station\n",
        "\t\t\t\tif railway_topology.grid[current_position] != 0:\n",
        "\t\t\t\t\tself.rails.append(current_position)\n",
        "\t\t\t\t\tcounter_of_rails += 1\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t# Eliminate duplicates\n",
        "\t\trails_position = []\n",
        "\t\trails_position_single = []\n",
        "\t\tfor i in range(len(self.rails)):\n",
        "\t\t\tfor j in range(len(self.rails[i])):\n",
        "\t\t\t\tif self.rails[i][j] not in rails_position_single:\n",
        "\t\t\t\t\trails_position_single.append(self.rails[i][j])\n",
        "\t\t\trails_position.append(rails_position_single)\n",
        "\t\t\trails_position_single = []\n",
        "\t\tself.rails = rails_position"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B01cYfcYNyt"
      },
      "source": [
        "## Train Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wdHNpaAeYQjE"
      },
      "outputs": [],
      "source": [
        "# A train run is the run on a line for a train (e.g. Genova-Milano --- Milano-Genova)\n",
        "# The train run consider each intermediate station the train has to pass between the two \"principal\" stations\n",
        "class Train_run:\n",
        "\n",
        "\tdef __init__(self, line_belongin, starting_time, from_depot: bool = False, to_depot: bool = False, inverse_train_direction: bool = False):\n",
        "\t\t# Line in which the train run is, so it contein informations about the stations to stop and etc etc\n",
        "\t\tself.line_belongin = line_belongin\n",
        "\t\t# Starting time of the run\n",
        "\t\tself.starting_time = starting_time\n",
        "\t\t# FromDepot indicates whether this run starts from a depot. Similar for ToDepot.\n",
        "\t\tself.from_depot = from_depot\n",
        "\t\tself.to_depot = to_depot\n",
        "\t\t# If inverse line direction \n",
        "\t\tself.inverse_train_direction = inverse_train_direction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7kzefAwQidY"
      },
      "source": [
        "# Utils function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nSx1S5lWQ2de"
      },
      "outputs": [],
      "source": [
        "from enum import IntEnum\n",
        "from typing import NamedTuple\n",
        "from flatland.core.grid.grid4 import Grid4TransitionsEnum\n",
        "\n",
        "class RailEnvActions(IntEnum):\n",
        "    DO_NOTHING = 0  # implies change of direction in a dead-end!\n",
        "    MOVE_LEFT = 1\n",
        "    MOVE_FORWARD = 2\n",
        "    MOVE_RIGHT = 3\n",
        "    STOP_MOVING = 4\n",
        "    REVERSE = 5\n",
        "\n",
        "    @staticmethod\n",
        "    def to_char(a: int):\n",
        "        return {\n",
        "            0: 'B',\n",
        "            1: 'L',\n",
        "            2: 'F',\n",
        "            3: 'R',\n",
        "            4: 'S',\n",
        "            5: 'I',\n",
        "        }[a]\n",
        "\n",
        "    @classmethod\n",
        "    def is_action_valid(cls, action):\n",
        "        return action in cls._value2member_map_\n",
        "\n",
        "    def is_moving_action(self):\n",
        "        return self.value in [self.MOVE_RIGHT, self.MOVE_LEFT, self.MOVE_FORWARD, self.REVERSE]\n",
        "\n",
        "\n",
        "RailEnvGridPos = NamedTuple('RailEnvGridPos', [('r', int), ('c', int)])\n",
        "RailEnvNextAction = NamedTuple('RailEnvNextAction', [('action', RailEnvActions), ('next_position', RailEnvGridPos),\n",
        "                                                     ('next_direction', Grid4TransitionsEnum)])\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from flatland.core.grid.grid_utils import IntVector2D, IntVector2DDistance\n",
        "from flatland.core.grid.grid_utils import IntVector2DArray\n",
        "from flatland.core.grid.grid_utils import Vec2dOperations as Vec2d\n",
        "from flatland.core.transition_map import GridTransitionMap\n",
        "from flatland.utils.ordered_set import OrderedSet\n",
        "\n",
        "\n",
        "class AStarNode:\n",
        "    \"\"\"A node class for A* Pathfinding\"\"\"\n",
        "\n",
        "    def __init__(self, pos: IntVector2D, parent=None):\n",
        "        self.parent = parent\n",
        "        self.pos: IntVector2D = pos\n",
        "        self.g = 0.0\n",
        "        self.h = 0.0\n",
        "        self.f = 0.0\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        \"\"\"\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        other : AStarNode\n",
        "        \"\"\"\n",
        "        return self.pos == other.pos\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.pos)\n",
        "\n",
        "    def update_if_better(self, other):\n",
        "        if other.g < self.g:\n",
        "            self.parent = other.parent\n",
        "            self.g = other.g\n",
        "            self.h = other.h\n",
        "            self.f = other.f\n",
        "\n",
        "\n",
        "def a_star(grid_map: GridTransitionMap, start: IntVector2D, end: IntVector2D,\n",
        "           a_star_distance_function: IntVector2DDistance = Vec2d.get_manhattan_distance, avoid_rails=False,\n",
        "           respect_transition_validity=True, forbidden_cells: IntVector2DArray = None) -> IntVector2DArray:\n",
        "    \"\"\"\n",
        "\n",
        "    :param avoid_rails:\n",
        "    :param grid_map: Grid Map where the path is found in\n",
        "    :param start: Start positions as (row,column)\n",
        "    :param end:  End position as (row,column)\n",
        "    :param a_star_distance_function: Define the distance function to use as heuristc:\n",
        "            -get_euclidean_distance\n",
        "            -get_manhattan_distance\n",
        "            -get_chebyshev_distance\n",
        "    :param respect_transition_validity: Whether or not a-star respect allowed transitions on the grid map.\n",
        "            - True: Respects the validity of transition. This generates valid paths, of no path if it cannot be found\n",
        "            - False: This always finds a path, but the path might be illegal and thus needs to be fixed afterwards\n",
        "    :param forbidden_cells: List of cells where the path cannot pass through. Used to avoid certain areas of Grid map\n",
        "    :return: IF a path is found a ordered list of al cells in path is returned\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Returns a list of tuples as a path from the given start to end.\n",
        "    If no path is found, returns path to closest point to end.\n",
        "    \"\"\"\n",
        "    rail_shape = grid_map.grid.shape\n",
        "\n",
        "    start_node = AStarNode(start, None)\n",
        "    end_node = AStarNode(end, None)\n",
        "    open_nodes = OrderedSet()\n",
        "    #print('open nodes all inizio',open_nodes)\n",
        "    closed_nodes = OrderedSet()\n",
        "    open_nodes.add(start_node)\n",
        "\n",
        "    while len(open_nodes) > 0:\n",
        "        # get node with current shortest est. path (lowest f)\n",
        "        current_node = None\n",
        "        for item in open_nodes:\n",
        "            if current_node is None:\n",
        "                current_node = item\n",
        "                #print('Current node all inizio',current_node)\n",
        "                continue\n",
        "            if item.f < current_node.f:\n",
        "                current_node = item\n",
        "                #print('Current node in generale',current_node.pos, current_node.parent.pos, current_node.f)\n",
        "\n",
        "            #print('open nodes nel ciclo ', open_nodes.pos)\n",
        "\n",
        "        # pop current off open list, add to closed list\n",
        "        open_nodes.remove(current_node)\n",
        "        closed_nodes.add(current_node)\n",
        "\n",
        "        # found the goal\n",
        "        if current_node == end_node:\n",
        "            path = []\n",
        "            current = current_node\n",
        "            while current is not None:\n",
        "                path.append(current.pos)\n",
        "                current = current.parent\n",
        "\n",
        "                #print(path)\n",
        "\n",
        "            # return reversed path\n",
        "            return path[::-1]\n",
        "\n",
        "        # generate children\n",
        "        children = []\n",
        "        if current_node.parent is not None:\n",
        "            prev_pos = current_node.parent.pos\n",
        "        else:\n",
        "            prev_pos = None\n",
        "\n",
        "        for new_pos in [(0, -1), (0, 1), (-1, 0), (1, 0)]:\n",
        "            # update the \"current\" pos\n",
        "            node_pos: IntVector2D = Vec2d.add(current_node.pos, new_pos)\n",
        "\n",
        "            # is node_pos inside the grid?\n",
        "            if node_pos[0] >= rail_shape[0] or node_pos[0] < 0 or node_pos[1] >= rail_shape[1] or node_pos[1] < 0:\n",
        "                continue\n",
        "\n",
        "            # validate positions\n",
        "            #\n",
        "\n",
        "            if not grid_map.check_transition_is_possible(prev_pos, current_node.pos, node_pos) \\\n",
        "             and respect_transition_validity:\n",
        "                continue\n",
        "\n",
        "            '''\n",
        "            if grid_map.validate_new_transition(prev_pos, current_node.pos, node_pos, end_node.pos) and respect_transition_validity:\n",
        "                   # and grid_map.check_direction_of_railroad(prev_pos, current_node.pos, node_pos):\n",
        "                print('=========================================')\n",
        "                print(prev_pos, current_node.pos, node_pos)\n",
        "                continue\n",
        "            '''\n",
        "\n",
        "\n",
        "            # create new node\n",
        "\n",
        "            new_node = AStarNode(node_pos, current_node)\n",
        "            #print(new_node.pos)\n",
        "\n",
        "            # Skip paths through forbidden regions if they are provided\n",
        "            if forbidden_cells is not None:\n",
        "                if node_pos in forbidden_cells and new_node != start_node and new_node != end_node:\n",
        "                    continue\n",
        "\n",
        "            children.append(new_node)\n",
        "\n",
        "        # loop through children\n",
        "        for child in children:\n",
        "            # already in closed list?\n",
        "            if child in closed_nodes:\n",
        "                continue\n",
        "\n",
        "            # create the f, g, and h values\n",
        "            child.g = current_node.g + 1.0\n",
        "            # this heuristic avoids diagonal paths\n",
        "            if avoid_rails:\n",
        "                child.h = a_star_distance_function(child.pos, end_node.pos) + np.clip(grid_map.grid[child.pos], 0, 1)\n",
        "            else:\n",
        "                child.h = a_star_distance_function(child.pos, end_node.pos)\n",
        "            child.f = child.g + child.h\n",
        "\n",
        "            # already in the open list?\n",
        "            if child in open_nodes:\n",
        "                continue\n",
        "\n",
        "            # add the child to the open list\n",
        "            open_nodes.add(child)\n",
        "\n",
        "        # no full path found\n",
        "        if len(open_nodes) == 0:\n",
        "            return []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKFkmlccTHt9"
      },
      "source": [
        "# Flatland Custom Rail Generator\n",
        "\n",
        "--- \n",
        "A custom rail is a rail defined by a matrix. Each cell of the matrix define the type of cell in the 2-D grid of the flatland environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SSP2ltVXxHv"
      },
      "source": [
        "## Different type of cell\n",
        "\n",
        "<img src=\"https://i.imgur.com/ruiRuep.jpg\" width=\"500\"/> <img src=\"https://i.imgur.com/sABiSuc.jpg\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecRto8qiVvXz"
      },
      "source": [
        "* This is the function to create a custom rail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wlyvYqEJViux"
      },
      "outputs": [],
      "source": [
        "\"\"\"Rail generators (infrastructure manager, \"Infrastrukturbetreiber\").\"\"\"\n",
        "from typing import Callable, Tuple, Optional, Dict, List\n",
        "\n",
        "from numpy.random.mtrand import RandomState\n",
        "\n",
        "from flatland.core.grid.grid4 import Grid4TransitionsEnum\n",
        "from flatland.core.grid.rail_env_grid import RailEnvTransitions\n",
        "from flatland.core.transition_map import GridTransitionMap\n",
        "\n",
        "\n",
        "RailGeneratorProduct = Tuple[GridTransitionMap, Optional[Dict]]\n",
        "\"\"\" A rail generator returns a RailGenerator Product, which is just\n",
        "    a GridTransitionMap followed by an (optional) dict/\n",
        "\"\"\"\n",
        "\n",
        "RailGenerator = Callable[[int, int, int, int], RailGeneratorProduct]\n",
        "\n",
        "# Number of agents are given by timetables (the num of the rows), target stations are also given by timetable \n",
        "def rail_custom_generator(rail_spec, train_stations_position: list = None, timetable: list = None):\n",
        "\n",
        "    \"\"\"\n",
        "    Utility to convert a rail given by manual specification as a map of tuples\n",
        "    (cell_type, rotation), to a transition map with the correct 16-bit\n",
        "    transitions specifications.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rail_spec : list of list of tuples\n",
        "        List (rows) of lists (columns) of tuples, each specifying a rail_spec_of_cell for\n",
        "        the RailEnv environment as (cell_type, rotation), with rotation being\n",
        "        clock-wise and in [0, 90, 180, 270].\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    function\n",
        "        Generator function that always returns a GridTransitionMap object with\n",
        "        the matrix of correct 16-bit bitmaps for each rail_spec_of_cell.\n",
        "\n",
        "    New features:\n",
        "    -------------\n",
        "        Train station poition: to define the position of the station\n",
        "        Target station: Define the target of the different agents\n",
        "        Timetable: timetable contein the intermediate station and the time at which pass across them\n",
        "    \"\"\"\n",
        "\n",
        "    def custom_generator(width: int, height: int, num_agents: int, num_resets: int = 0,\n",
        "                  np_random: RandomState = None) -> RailGenerator:\n",
        "\n",
        "        # All the cities are oriented in the same way in my model\n",
        "        city_orientations = 1\n",
        "\n",
        "        # Taking the number of agents from timetable\n",
        "        num_of_agents = len(timetable)\n",
        "\n",
        "        # Taking the target stations from timetable\n",
        "        target_stations = []\n",
        "\n",
        "        for agent in range(num_agents):\n",
        "            target_stations.append(timetable[agent][-1])\n",
        "      \n",
        "        rail_env_transitions = RailEnvTransitions()\n",
        "\n",
        "        height = len(rail_spec)\n",
        "        width = len(rail_spec[0])\n",
        "        rail = GridTransitionMap(width=width, height=height, transitions=rail_env_transitions)\n",
        "\n",
        "        for r in range(height):\n",
        "            for c in range(width):\n",
        "                rail_spec_of_cell = rail_spec[r][c]\n",
        "                index_basic_type_of_cell_ = rail_spec_of_cell[0]\n",
        "                rotation_cell_ = rail_spec_of_cell[1]\n",
        "                if index_basic_type_of_cell_ < 0 or index_basic_type_of_cell_ >= len(rail_env_transitions.transitions):\n",
        "                    print(\"ERROR - invalid rail_spec_of_cell type=\", index_basic_type_of_cell_)\n",
        "                    return []\n",
        "                basic_type_of_cell_ = rail_env_transitions.transitions[index_basic_type_of_cell_]\n",
        "                effective_transition_cell = rail_env_transitions.rotate_transition(basic_type_of_cell_, rotation_cell_)\n",
        "                rail.set_transitions((r, c), effective_transition_cell)\n",
        "\n",
        "        return rail,  {'agents_hints': {\n",
        "            'num_agents': num_of_agents,            \n",
        "            'city_positions': train_stations_position,\n",
        "            'train_stations': train_stations_position,\n",
        "            'city_orientations': city_orientations,\n",
        "            'targets' : target_stations,\n",
        "            'timetable' : timetable \n",
        "        }}\n",
        "\n",
        "    return custom_generator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBH5g0GFWHld"
      },
      "source": [
        "* these are the utilities for the environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P-PIRK0HWP_t"
      },
      "outputs": [],
      "source": [
        "from flatland.core.env_observation_builder import ObservationBuilder\n",
        "from flatland.envs.observations import TreeObsForRailEnv\n",
        "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n",
        "from flatland.envs.rail_env import RailEnv\n",
        "from flatland.envs.rail_generators import rail_from_file\n",
        "from flatland.envs.line_generators import line_from_file\n",
        "from flatland.envs.step_utils.states import TrainState\n",
        "\n",
        "\n",
        "def load_flatland_environment_from_file(file_name: str,\n",
        "                                        load_from_package: str = None,\n",
        "                                        obs_builder_object: ObservationBuilder = None,\n",
        "                                        record_steps = False,\n",
        "                                        ) -> RailEnv:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_name : str\n",
        "        The pickle file.\n",
        "    load_from_package : str\n",
        "        The python module to import from. Example: 'env_data.tests'\n",
        "        This requires that there are `__init__.py` files in the folder structure we load the file from.\n",
        "    obs_builder_object: ObservationBuilder\n",
        "        The obs builder for the `RailEnv` that is created.\n",
        "\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    RailEnv\n",
        "        The environment loaded from the pickle file.\n",
        "    \"\"\"\n",
        "    if obs_builder_object is None:\n",
        "        obs_builder_object = TreeObsForRailEnv(\n",
        "            max_depth=2,\n",
        "            predictor=ShortestPathPredictorForRailEnv(max_depth=10))\n",
        "    environment = RailEnv(width=1, height=1, rail_generator=rail_from_file(file_name, load_from_package),\n",
        "                          line_generator=line_from_file(file_name, load_from_package),\n",
        "                          number_of_agents=1,\n",
        "                          obs_builder_object=obs_builder_object,\n",
        "                          record_steps=record_steps,\n",
        "                          )\n",
        "    return environment\n",
        "\n",
        "# TODO riferisciti a un agente (EnvAgent)\n",
        "# Usa le azioni dell'agente specifico\n",
        "def delay_a_train(delay: int, delay_time: int, time_of_train_generation: int, actions, train):\n",
        "    \n",
        "    i_agent = train.handle\n",
        "    train_velocity = train.speed_counter.speed\n",
        "\n",
        "    \n",
        "    actions_scheduled = [0] * (len(actions[i_agent]) + delay)\n",
        "    \n",
        "    # Copy the actions scheduled for the train before the delay\n",
        "    for i in range(delay_time - time_of_train_generation):\n",
        "        actions_scheduled[i] = actions[i_agent][i]\n",
        "    # Delay the train (STOP)  \n",
        "    for i in range(delay):\n",
        "        if (i + delay_time - time_of_train_generation) < 0 or (delay_time - time_of_train_generation) > len(actions[i_agent]):\n",
        "            raise ImportError('The train is not present in the environment, check the delay time')\n",
        "        actions_scheduled[i + delay_time - time_of_train_generation] = RailEnvActions.STOP_MOVING\n",
        "    # Copy the actions scheduled for the train after the delay\n",
        "    for i in range(len(actions[i_agent]) - (delay_time - time_of_train_generation)):\n",
        "        actions_scheduled[i + delay_time - time_of_train_generation + delay] = actions[i_agent][i + delay_time - time_of_train_generation]\n",
        "    \n",
        "    actions[i_agent] = actions_scheduled\n",
        "    return \n",
        "\n",
        "\n",
        "# Function to convert decimal number to base number\n",
        "def actions_decimal_to_base(base, number_to_convert, num_agents):\n",
        "    division = number_to_convert\n",
        "    result = []\n",
        "    while division != 0:\n",
        "        result.append(division % base)\n",
        "        division = int(division / base)\n",
        "    actions_to_perform = result[::-1]\n",
        "    if len(actions_to_perform) < num_agents:\n",
        "        zero_to_add = num_agents - len(actions_to_perform)\n",
        "        for i in range(zero_to_add):\n",
        "            actions_to_perform.insert(0,0)\n",
        "    return actions_to_perform\n",
        "\n",
        "\n",
        "def make_a_deterministic_interruption(agent_to_interrupt, interruption_time):\n",
        "    if agent_to_interrupt.state.is_on_map_state():\n",
        "        agent_to_interrupt.malfunction_handler.malfunction_down_counter = interruption_time\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hRGASyFWbbg"
      },
      "source": [
        "# Custom schedule generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fyOVeCkwWi3y"
      },
      "outputs": [],
      "source": [
        "\"\"\"Schedule generators (railway undertaking, \"EVU\").\"\"\"\n",
        "from typing import Tuple, List, Callable, Mapping, Optional, Any\n",
        "\n",
        "from enum import IntEnum\n",
        "from numpy.random.mtrand import RandomState\n",
        " \n",
        "from flatland.core.transition_map import GridTransitionMap\n",
        "from typing import List, NamedTuple, Callable\n",
        "\n",
        "from flatland.core.grid.grid4 import Grid4TransitionsEnum\n",
        "from flatland.core.grid.grid_utils import IntVector2DArray\n",
        "\n",
        "\n",
        "Schedule = NamedTuple('Schedule', [('agent_positions', IntVector2DArray),\n",
        "                                   ('agent_directions', List[Grid4TransitionsEnum]),\n",
        "                                   ('agent_targets', IntVector2DArray),\n",
        "                                   ('agent_speeds', List[float]),\n",
        "                                   ('agent_malfunction_rates', List[int]),\n",
        "                                   ('max_episode_steps', int)]) \n",
        "\n",
        "def check_rail_road_direction(rail: GridTransitionMap, timetable):\n",
        "    # To establish the direction of trains in the railroas I define a simple law, as for the cars, each trains has to \n",
        "    # go the direction that let them to have the right free\n",
        "\n",
        "    agents_direction = [0]*len(timetable)\n",
        "    path_result = [0]*len(timetable)\n",
        "\n",
        "    #print(rail.grid[7 ,13],rail.grid[7 ,12],rail.grid[7 ,11],rail.grid[7 ,10],rail.grid[7 ,9],rail.grid[7 ,8],rail.grid[7 ,7],rail.grid[7 ,6], rail.grid[7 ,5])\n",
        "    \n",
        "    for i in range (len(timetable)):\n",
        "        # Consider the a_star result to calculate the direction\n",
        "        path_result[i] = (a_star(rail,timetable[i][0][0],timetable[i][0][1]))\n",
        "        if path_result[i] == []:\n",
        "            agents_direction[i] = 1\n",
        "            continue\n",
        "        if len(path_result[i]) == 1:\n",
        "            agents_direction[i] = 1\n",
        "            continue\n",
        "        difference_x = path_result[i][0][1] - path_result[i][1][1]\n",
        "        difference_y = path_result[i][0][0] - path_result[i][1][0]\n",
        "        if difference_y == 1:\n",
        "            agents_direction[i] = 0\n",
        "        if difference_x ==  -1:\n",
        "            agents_direction[i] = 1\n",
        "        if difference_y == -1:\n",
        "            agents_direction[i] = 2\n",
        "        if difference_x == 1:\n",
        "            agents_direction[i] = 3\n",
        "\n",
        "    return agents_direction\n",
        "\n",
        "AgentPosition = Tuple[int, int]\n",
        "ScheduleGenerator = Callable[[GridTransitionMap, int, Optional[Any], Optional[int]], Schedule]\n",
        "\n",
        "\n",
        "def custom_schedule_generator(timetable, speed_ratio_map: Mapping[float, float] = None, seed: int = 1) -> ScheduleGenerator:\n",
        "\n",
        "#class Custom_schedule_generator(BaseSchedGen):\n",
        "\t\"\"\"\n",
        "\n",
        "\tThis is a custom schedule generator, create a schedule with the timetable, and the station where the trains should pass\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef generate_custom(rail: GridTransitionMap, num_agents: int, hints: Any = None, num_resets: int = 0,\n",
        "\t\t\t\t  np_random: RandomState = None) -> Schedule:\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tThe generator that assigns tasks to all the agents\n",
        "\t\t:param rail: Rail infrastructure given by the rail_generator\n",
        "\t\t:param num_agents: Number of agents to include in the schedule\n",
        "\t\t:param hints: Hints provided by the rail_generator These include positions of start/target positions\n",
        "\t\t:param num_resets: How often the generator has been reset.\n",
        "\t\t:return: Returns the generator to the rail constructor\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\ttrain_stations = hints['train_stations']\n",
        "\t\tcity_positions = hints['city_positions']\n",
        "\t\tcity_orientation = hints['city_orientations']\n",
        "\t\tmax_num_agents = hints['num_agents']\n",
        "\n",
        "\t\tif num_agents > max_num_agents:\n",
        "\t\t\tnum_agents = max_num_agents\n",
        "\t\t\twarnings.warn(\"Too many agents! Changes number of agents.\")\n",
        "\t\t# Place agents and targets within available train stations\n",
        "\t\tagents_position = []\n",
        "\t\tagents_target = []\n",
        "\t\tagents_direction = []\n",
        "\n",
        "\t\t# Define the agent positions\n",
        "\t\t# TODO capisci se così va bene o no\n",
        "\t\tfor agent_i in range (len(timetable)):\n",
        "\t\t\tagents_position.append(timetable[agent_i][0][0])\n",
        "\t\t\tagents_target.append(timetable[agent_i][0][-1])\n",
        "\n",
        "\n",
        "\t\t# Define the direction of the trains based on the rail they occupy\n",
        "\t\t# Input --> the topology of the network, the position of the trains\n",
        "\t\t# Output --> an array with the directions of the trains\n",
        "\t\t# DIRECTIONS: 0 = UP, 1 = RIGHT, 2 = DOWN, 3 = LEFT\n",
        "\n",
        "\t\tagents_direction = check_rail_road_direction(rail, timetable)\n",
        "\n",
        "\n",
        "\t\t_runtime_seed = seed + num_resets\n",
        "\n",
        "\t\tif speed_ratio_map:\n",
        "\t\t\tspeeds = speed_initialization_helper(num_agents, speed_ratio_map, seed=_runtime_seed, np_random=np_random)\n",
        "\t\telse:\n",
        "\t\t\tspeeds = [1.0] * len(agents_position)\n",
        "\n",
        "\t\t# We add multiply factors to the max number of time steps to simplify task in Flatland challenge.\n",
        "\t\t# These factors might change in the future.\n",
        "\t\ttimedelay_factor = 4\n",
        "\t\talpha = 2\n",
        "\t\tmax_episode_steps = 1000\n",
        "\n",
        "\t\t#print(agents_position, agents_target, agents_direction)\n",
        "\n",
        "\t\treturn Schedule(agent_positions=agents_position, agent_directions=agents_direction,\n",
        "\t\t\t\t\t\tagent_targets=agents_target, agent_speeds=speeds, agent_malfunction_rates=None,\n",
        "\t\t\t\t\t\tmax_episode_steps=max_episode_steps)\n",
        "\n",
        "\treturn generate_custom  #(station_to_traverse = [(21, 37), (15, 51)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11emYfouXAL5"
      },
      "source": [
        "* These are the plan to follow utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MKROm4gyXOr6"
      },
      "outputs": [],
      "source": [
        "# import chain\n",
        "from itertools import chain\n",
        "from enum import IntEnum\n",
        "import random\n",
        "# This is to test if the timetable is valid or not\n",
        "from flatland.core.grid.grid4_astar import a_star\n",
        "\n",
        "class RailEnvActions(IntEnum):\n",
        "    DO_NOTHING = 0  # implies change of direction in a dead-end!\n",
        "    MOVE_LEFT = 1\n",
        "    MOVE_FORWARD = 2\n",
        "    MOVE_RIGHT = 3\n",
        "    STOP_MOVING = 4\n",
        "    REVERSE = 5\n",
        "\n",
        "    @staticmethod\n",
        "    def to_char(a: int):\n",
        "        return {\n",
        "            0: 'B',\n",
        "            1: 'L',\n",
        "            2: 'F',\n",
        "            3: 'R',\n",
        "            4: 'S',\n",
        "            5: 'I'\n",
        "        }[a]\n",
        "\n",
        "# Check if the timetable is feaseble or not\n",
        "def control_timetable(timetable, railway_topology):\n",
        "    # Check for all the trains\n",
        "    for trains in range (len(timetable)):       \n",
        "        # Check for all the stations\n",
        "        # Calculate the difference of two different times, so i don't need the last term to cycle          \n",
        "        for stations in range (len(timetable[trains][1]) - 1):   \n",
        "            if (timetable[trains][1][stations] - timetable[trains][1][stations + 1]) >= 0:\n",
        "                print('===================================================================================================================================')\n",
        "                print('Attention!!! The agent number', trains, 'has a problem in the timetable, times to reach stations', stations, 'and', (stations+1), 'are not right')\n",
        "                print('The time to reach the successive station SHOULD BE > 0, pay attenction to the timetable')\n",
        "                # Function that check if the time to reach a station defined by the timetable are possible or not,\n",
        "                # Return the time minimum time to reach two different stations depending on the distance and on the line type (high velocity, regional...)\n",
        "            time_to_next_station = time_to_reach_next_station(timetable[trains][0][stations], timetable[trains][0][stations + 1], railway_topology, timetable, trains)\n",
        "            # Control if the time to reach the next station is possible (considering maximum velocities of lines and the distances between two stations)\n",
        "            if time_to_next_station > (timetable[trains][1][stations+1]- timetable[trains][1][stations]):\n",
        "                print('===================================================================================================================================')\n",
        "                print('Attention!!! Agent number', trains, 'has a problem in the timetable, times to reach stations', stations, 'and', (stations+1), 'are not right')\n",
        "                print('The time to reach the next station SHOULD BE HIGHER. The minimum time to reach the station should be:', time_to_next_station)\n",
        "    return\n",
        "\n",
        "# TODO aggiungere dei controlli \n",
        "# - controllare che la posizione in cui si mette il goal del secondo treno sia possibile\n",
        "# - controllare il tempo necessario nella stazione (in base anche alla rialzatura in cui si mette il treno \n",
        "#   e.g. treno spostato su di due caselle, tempo necessario + 4 * velocità alla meno uno)\n",
        "\n",
        "def divide_trains_in_station_rails(timetable, railway_topology):\n",
        "    # The number of different stations presented in the timetable\n",
        "    different_stations = 0\n",
        "    # The position of different stations presented in the timetable\n",
        "    station_positions = []\n",
        "    # Check how many different stations are in the timetable\n",
        "    for i in range(len(timetable)): # for all the trains\n",
        "        for k in range(len(timetable[i][0])): # for all the stations\n",
        "            if timetable[i][0][k] not in station_positions:\n",
        "                station_positions.append(timetable[i][0][k])\n",
        "                different_stations += 1\n",
        "\n",
        "    # Indexes contein the indexes of the station like this\n",
        "    # 0 for the first station in station position \n",
        "    # 1 for the second and so on\n",
        "    indexes = []\n",
        "    # Local variable for the loop\n",
        "    single_index = []\n",
        "\n",
        "    for j in range(len(timetable)): # for all the trains\n",
        "        single_index = [0]*len(timetable[j][0])\n",
        "        for k in range(len(timetable[j][0])): # for all the stations\n",
        "            for i in range(len(station_positions)): # for all the different stations discovered\n",
        "                if station_positions[i] == timetable[j][0][k]:\n",
        "                    single_index[k] = i\n",
        "        indexes.append(single_index)\n",
        "    \n",
        "    # Stations have a capacity, if more trains with respect to the capacity\n",
        "    # of the station are present there is a problem\n",
        "    counter_of_trains = [0] * different_stations  # counter of trains in a certain station\n",
        "    \n",
        "    # TODO fai un ciclo for che cicli gli step nella tabella oraria per aggiornare il counter trains\n",
        "    # quando più treni sono nella stessa stazione aumenta, quando un treno esce dalla stazione diminuisce\n",
        "    # aggiungi una lettera che cicli sulle due stazioni\t(ad ora le stazioni girano su tutta la timetable)\n",
        "    # Calculating the maximum time the agents have to stay in env \n",
        "    \n",
        "     # Here controll if two or more trains have to reach the same station at the same time\n",
        "    for m in range(len(timetable)):  # for all the trains m\n",
        "        for n in range(len(timetable)): # for all the trains n with m!=n\n",
        "            for k in range(len(timetable[m][1])): # for all the stations of m\n",
        "                for l in range(len(timetable[n][1])): # for all the stations of n\n",
        "                    if m!=n and m<n:\n",
        "                        # time at which the trains have to reach the stations\n",
        "                        time_train_a = timetable[m][1][k]\n",
        "                        time_train_b = timetable[n][1][l]\n",
        "                        # if they have to reach the same stations\n",
        "                        if indexes[m][k] == indexes[n][l]: \n",
        "                            threshold_time = calculate_time_in_station(timetable,m,n,k,l)\n",
        "                            # if the time at which they have to reach the station is similar (calculated the time needed in the stations for the trains)\n",
        "                            if time_train_a - time_train_b < threshold_time and time_train_a - time_train_b > -threshold_time:\n",
        "                                # I change the goal of the convoy that first reach the station\n",
        "                                timetable[n][0][l] = (station_positions[indexes[n][l]][0] - 1, station_positions[indexes[n][l]][1])\n",
        "                            \n",
        "\n",
        "\n",
        "# Define the scheduled actions the agents have to do\n",
        "def action_to_do(timetable, railway_topology):\n",
        "    # Path to do to arrive to the right station\n",
        "\n",
        "    # L'idea vuole essere quella di avere un tempo necessario per stare in stazione\n",
        "    # Se due treni nell'intorno di tempo devono stare nella stessa stazione bisogna indirizzarli \n",
        "    # in binari liberi diversi, quindi il passaggio diventa diverso.\n",
        "\n",
        "    path_result = []\n",
        "    # Calculate the path for all the trains\n",
        "    for train_i in range (len(timetable)):\n",
        "        # Number of stations in the train i\n",
        "        num_of_stations = len(timetable[train_i][0])\n",
        "        # The partial result a train run\n",
        "        path_partial_result = []\n",
        "        for station in range(num_of_stations - 1): \n",
        "            path_partial_result.append(a_star(railway_topology,timetable[train_i][0][station],timetable[train_i][0][station + 1]))\n",
        "            if path_partial_result == []:\n",
        "                raise ImportError('There s not a path between station', station, 'and station', station + 1 )\n",
        "\n",
        "        # Final result for all the trains and train runs\n",
        "        path_result.append(path_partial_result)\n",
        "\n",
        "    # Calculate the actions that have to be done\n",
        "    actions_to_do = []\n",
        "    for train_i in range (len(timetable)):\n",
        "        # Number of stations in the train i\n",
        "        num_of_stations = len(timetable[train_i][0])\n",
        "        # Flag that tells me that the next step is particular\n",
        "        next = False\n",
        "        # Each train occupy a row in the action_to_do matrix \n",
        "        actions_single_train = []\n",
        "        # I need the direction of the last run for the reverse action\n",
        "        direction_last_run = 0\n",
        "        for station in range (num_of_stations - 1):\n",
        "            # Each train occupy a row in the action_to_do matrix \n",
        "            actions_single_train_run = []\n",
        "            for step in range (len(path_result[train_i][station])):\n",
        "                # If i'm restarting from the final station of the train run, i have to wait till is the time to restart\n",
        "                if len(path_result[train_i][station]) == 1:\n",
        "                    time_to_wait = timetable[train_i][1][station + 1] - timetable[train_i][1][station]\n",
        "                    for i in range(time_to_wait):\n",
        "                        actions_single_train_run.append(RailEnvActions.STOP_MOVING)\n",
        "                    continue\n",
        "                # Calculate the direction of the trains at each step\n",
        "                if step == 0:\n",
        "                    difference_y = path_result[train_i][station][step][0] - path_result[train_i][station][step + 1][0]\n",
        "                    difference_x = path_result[train_i][station][step][1] - path_result[train_i][station][step + 1][1]\n",
        "                    if difference_y == 1:\n",
        "                        direction = 0\n",
        "                    if difference_x ==  -1:\n",
        "                        direction = 1\n",
        "                    if difference_y == -1:\n",
        "                        direction = 2\n",
        "                    if difference_x == 1:\n",
        "                        direction = 3 \n",
        "                else:\n",
        "                    difference_y = path_result[train_i][station][step - 1][0] - path_result[train_i][station][step][0]\n",
        "                    difference_x = path_result[train_i][station][step - 1][1] - path_result[train_i][station][step][1]\n",
        "                    if difference_y == 1:\n",
        "                        direction = 0\n",
        "                    if difference_x ==  -1:\n",
        "                        direction = 1\n",
        "                    if difference_y == -1:\n",
        "                        direction = 2\n",
        "                    if difference_x == 1:\n",
        "                        direction = 3 \n",
        "                # Variable to count the number of possible path at each cell, is an int with the number of possible path\n",
        "                if not step == 0:\n",
        "                    # Specific case, a train is at the boarder of two different lines, \n",
        "                    # if this appen I have to consider the previous transition at the next time stamp due to the fact the velocity changes\n",
        "                    if next:\n",
        "                        multiple_path = railway_topology.get_transitions(path_result[train_i][station][step-1][0],path_result[train_i][station][step-1][1],prev_direction).count(1)\n",
        "                        next = False\n",
        "                    elif (path_result[train_i][station][step] in av_line) and not (path_result[train_i][station][step - 1] in av_line):\n",
        "                        multiple_path = railway_topology.get_transitions(path_result[train_i][station][step][0],path_result[train_i][station][step][1],prev_direction).count(1)\n",
        "                        next = True\n",
        "                    else:\n",
        "                        multiple_path = railway_topology.get_transitions(path_result[train_i][station][step-1][0],path_result[train_i][station][step-1][1],prev_direction).count(1)\n",
        "                # Starting with a move forward direction for the train\n",
        "                if step == 0:\n",
        "                    #actions_single_train.append(RailEnvActions.MOVE_FORWARD)\n",
        "                    prev_direction = direction\n",
        "                # If I'm not at the start of the train \n",
        "                else:\n",
        "                    # The direction doesn't change\n",
        "                    if num_of_stations > 1 and station >= 1 and step <= 1:\n",
        "                        if (direction - direction_last_run == 2 or direction - direction_last_run == -2):\n",
        "                            # If I'm in an hig velocity line velocity is define only by the type of train\n",
        "                            actions_single_train_run.append(RailEnvActions.REVERSE)\n",
        "                            prev_direction = direction\n",
        "                            continue\n",
        "\n",
        "                    if direction - prev_direction == 0:\n",
        "                        # If I'm in an hig velocity line velocity is define only by the type of train\n",
        "                        if path_result[train_i][station][step - 1] in av_line:\n",
        "                            velocity = timetable[train_i][2]\n",
        "                        # If I'm in other line velocity is the minimum between 1/2 (the velocity of the line) and the type of train velocity\n",
        "                        else:\n",
        "                            velocity = min(timetable[train_i][2], 1/2)\n",
        "                        for i in range(int(pow(velocity, -1))):\n",
        "                            #print('Test per capire come varia',i, 'Treno numero', train_i)\n",
        "                            actions_single_train_run.append(RailEnvActions.MOVE_FORWARD)\n",
        "                        # I'm arrived at the station?\n",
        "                        if step == (len(path_result[train_i][station]) - 1):\n",
        "                            # If the next station is the last one of the train run I don't have to stop for the min wait time\n",
        "                            if station != (num_of_stations - 2):\n",
        "                                if len(path_result[train_i][station + 1]) == 1:\n",
        "                                    continue\n",
        "                            # If is an intermediate station I need to stop the min wait time\n",
        "                            for i in range(3):   # TODO ADD THE WAITING TIMES OF THE STATIONS\n",
        "                                actions_single_train_run.append(RailEnvActions.STOP_MOVING)\n",
        "\n",
        "                        prev_direction = direction\n",
        "                    # I have to reverse the train direction when I arrive at the ending station\n",
        "                    elif ((direction - prev_direction) == -2) or ((direction - prev_direction) == 2):\n",
        "                        # If I'm in an hig velocity line velocity is define only by the type of train\n",
        "                        if path_result[train_i][station][step - 1] in av_line:\n",
        "                            velocity = timetable[train_i][2]\n",
        "                        else:\n",
        "                            velocity = min(timetable[train_i][2], 1/2)\n",
        "                        for i in range(int(pow(velocity, -1))):\n",
        "                            actions_single_train_run.append(RailEnvActions.REVERSE)\n",
        "                        prev_direction = direction\n",
        "                    # I have to move to left \n",
        "                    # and I have more then one possible path, so I go left at the deviation\n",
        "                    # Depending on the direction of march the results can be -1 or -3\n",
        "                    elif ((direction - prev_direction == -1) and (multiple_path > 1)) or ((direction - prev_direction == +3)):\n",
        "                        # If I'm in an hig velocity line velocity is define only by the type of train\n",
        "                        if path_result[train_i][station][step - 1] in av_line:\n",
        "                            velocity = timetable[train_i][2]\n",
        "                        # If I'm in other line velocity is the minimum between 1/2 (the velocity of the line) and the type of train velocity\n",
        "                        else:\n",
        "                            velocity = min(timetable[train_i][2], 1/2)\n",
        "                        for i in range(int(pow(velocity, -1))):\n",
        "                            actions_single_train_run.append(RailEnvActions.MOVE_LEFT)\n",
        "                        prev_direction = direction\n",
        "                    # I have to move right \n",
        "                    # and I have more then one possible path, so I go left at the deviation \n",
        "                    # Depending on the direction of march the results can be +1 or -3\n",
        "                    elif ((direction - prev_direction == 1) and (multiple_path > 1)) or ((direction - prev_direction == -3) ):\n",
        "                        # If I'm in an hig velocity line velocity is define only by the type of train\n",
        "                        if path_result[train_i][station][step - 1] in av_line:\n",
        "                            velocity = timetable[train_i][2]\n",
        "                        # If I'm in other line velocity is the minimum between 1/2 (the velocity of the line) and the type of train velocity\n",
        "                        else:\n",
        "                            velocity = min(timetable[train_i][2], 1/2)\n",
        "                        for i in range(int(pow(velocity, -1))):\n",
        "                            actions_single_train_run.append(RailEnvActions.MOVE_RIGHT)\n",
        "                        prev_direction = direction\n",
        "                    else:\n",
        "                        if path_result[train_i][station][step - 1] in av_line:\n",
        "                            velocity = timetable[train_i][2]\n",
        "                        else:\n",
        "                            velocity = min(timetable[train_i][2], 1/2)\n",
        "                        for i in range(int(pow(velocity, -1))):\n",
        "                            actions_single_train_run.append(RailEnvActions.MOVE_FORWARD)\n",
        "                        # I'm arrived at the station?\n",
        "                        if step == (len(path_result[train_i][station]) - 1):\n",
        "                            for i in range(3):   # TODO ADD THE WAITING TIMES OF THE STATIONS\n",
        "                                actions_single_train_run.append(RailEnvActions.STOP_MOVING)\n",
        "                        prev_direction = direction\n",
        "            direction_last_run = direction\n",
        "\n",
        "            actions_single_train.append(actions_single_train_run)\n",
        " \n",
        "        if isinstance(actions_single_train[0], list):\n",
        "            len(actions_single_train)\n",
        "            actions_single_train = list(chain.from_iterable(actions_single_train))\n",
        "        actions_to_do.append(actions_single_train)\n",
        "\n",
        "    return actions_to_do\n",
        "\n",
        "# Calculate the time to reach the stations to understand if timetable is right\n",
        "def time_to_reach_next_station(departure_station_position, arrival_station_position, railway_topology, schedule, train_number):\n",
        "    # First thing check the distance between two stations \n",
        "    result = a_star(railway_topology, departure_station_position, arrival_station_position)\n",
        "    # Maximum velocity a train can achieve\n",
        "    train_velocity = schedule[train_number][2]\n",
        "\n",
        "    lenght_path = len(result)  # distance between stations\n",
        "\n",
        "    # Array when I put at each step the time needed to make the path\n",
        "    # The total time is the sum of the numbers\n",
        "    time_array = []\n",
        "    # Check the at each step which train i am and which line im in\n",
        "    for step in range(lenght_path):\n",
        "        if (result[step]) in av_line:\n",
        "            time_array.append(pow(train_velocity,-1))\n",
        "        else:\n",
        "            time_array.append(pow(min(train_velocity, 1/2), -1))\n",
        "    time_needed = sum(time_array)\n",
        "\n",
        "    #print((time_needed + int(time_needed/10))) DEBUG\n",
        "\n",
        "    # Adding to the time a 10% to face with problems in case it's neaded\n",
        "    return (time_needed + int(time_needed/10))\n",
        "\n",
        "# TODO calculate the av_rails, in order to distinguish them\n",
        "# TODO calculate the right,left,up,down rails, in order to distinguish them\n",
        "# TODO understad if the velocities are realistic or not (360 km for high velocity, 180 and 120 is realistic or not?)\n",
        "\n",
        "def calculate_timetable(convoys, railway_topology):\n",
        "    # The timetable that should be returned\n",
        "    timetable = []\n",
        "    # For each convoy\n",
        "    for convoy_i in range(len(convoys)):\n",
        "        # For each train run defined\n",
        "        single_convoy_schedule = convoys[convoy_i].schedule\n",
        "        single_convoy_schedule_len = len(convoys[convoy_i].schedule)\n",
        "        single_convoy = []\n",
        "        for num_of_runs in range(single_convoy_schedule_len):\n",
        "            # The single train run\n",
        "            single_train_run = []\n",
        "            # The number of station to pass\n",
        "            num_of_stations = len(single_convoy_schedule[num_of_runs].line_belongin.stations)\n",
        "            # The station to stop\n",
        "            stations_to_stop_position = []\n",
        "            # Direction not inverted?\n",
        "            if not single_convoy_schedule[num_of_runs].inverse_train_direction:\n",
        "                for i in range(num_of_stations):\n",
        "                    # append the station position in the right order\n",
        "                    stations_to_stop_position.append(single_convoy_schedule[num_of_runs].line_belongin.stations[i].position)\n",
        "            # Direction inverdet?\n",
        "            else:\n",
        "                for i in range(num_of_stations):\n",
        "                    # append the station position in inverted order\n",
        "                    stations_to_stop_position.append(single_convoy_schedule[num_of_runs].line_belongin.stations[num_of_stations - 1 - i].position)\n",
        "            # Adding the starting time\n",
        "            single_train_run.append(single_convoy_schedule[num_of_runs].starting_time)\n",
        "\n",
        "            for stations in range(num_of_stations -1):\n",
        "                departure_station_position = stations_to_stop_position[stations]\n",
        "                arrival_station_position = stations_to_stop_position[stations + 1]\n",
        "                # First thing check the distance between two stations\n",
        "                result = a_star(railway_topology, departure_station_position, arrival_station_position)\n",
        "                # Maximum velocity a train can achieve\n",
        "                train_velocity = convoys[convoy_i].maximum_velocity \n",
        "\n",
        "                lenght_path = len(result)  # distance between stations\n",
        "\n",
        "                # Array when I put at each step the time needed to make the path\n",
        "                # The total time is the sum of the numbers\n",
        "                time_array = []\n",
        "\n",
        "                # Check the at each step which train i am and which line im in\n",
        "                # Train should be in the middle of two line type\n",
        "                for step in range(lenght_path):\n",
        "                    if (result[step]) in av_line:\n",
        "                        time_array.append(pow(train_velocity,-1))\n",
        "                    else:\n",
        "                        time_array.append(pow(min(train_velocity, 1/2), -1))\n",
        "                time_needed = sum(time_array)\n",
        "                # Adding to the time a 10% to face with problems in case it's neaded\n",
        "                time_needed = time_needed + int(time_needed/10)\n",
        "\n",
        "                # Adding the precedence time \n",
        "                if stations != (num_of_stations - 1):\n",
        "                    if len(single_train_run) == 1 and type(single_convoy_schedule[num_of_runs].line_belongin.stations[stations].min_wait_time) == int:\n",
        "                        single_train_run.append(int(time_needed + single_train_run[0] + single_convoy_schedule[num_of_runs].line_belongin.stations[stations].min_wait_time))\n",
        "                    else:\n",
        "                        # sum of time needed, the precedence time and the waiting time at the station\n",
        "                        single_train_run.append(int(time_needed + single_train_run[stations] + single_convoy_schedule[num_of_runs].line_belongin.stations[stations].min_wait_time[0]))\n",
        "\n",
        "            single_convoy.append(stations_to_stop_position)\n",
        "            single_convoy.append(single_train_run)\n",
        "            \n",
        "        timetable.append(single_convoy)\n",
        "\n",
        "    # This is needed in order to obtein the timetable-standard structure\n",
        "    final_timetable = [] # The final timetable\n",
        "    timetable_single_convoy = [] # Timetable of a single convoy\n",
        "    timetable_position_example = [] # Partial timetable with the position of the stations to pass\n",
        "    timetable_time_example = []  # Partial timetable with the time at which reach the stations\n",
        "    single_position_timetable = []  # position and time of a single train run\n",
        "    single_time_timetable = []\n",
        "\n",
        "    for i in range(len(timetable)):\n",
        "        for j in range(len(timetable[i])):\n",
        "            if j % 2 == 0:\n",
        "                timetable_position_example.append(timetable[i][j])\n",
        "            else:\n",
        "                timetable_time_example.append(timetable[i][j])\n",
        "        for k in range(len(timetable_position_example)):\n",
        "            single_position_timetable += (timetable_position_example[k])\n",
        "        for k in range(len(timetable_time_example)):\n",
        "            single_time_timetable += timetable_time_example[k]\n",
        "        # The standard timetable form is (positions, time, train velocity)\n",
        "        timetable_single_convoy.append(single_position_timetable)\n",
        "        timetable_single_convoy.append(single_time_timetable)\n",
        "        timetable_single_convoy.append(convoys[i].maximum_velocity)\n",
        "        # Final timetable\n",
        "        final_timetable.append(timetable_single_convoy)\n",
        "        # Restart the partial results\n",
        "        single_position_timetable = []\n",
        "        timetable_position_example = []\n",
        "        single_time_timetable = []\n",
        "        timetable_time_example = []\n",
        "        timetable_single_convoy = []\n",
        "\n",
        "    return final_timetable\n",
        "\n",
        "def calculate_time_in_station(timetable,train_a,train_b,index_a,index_b):\n",
        "    time_a = 15\n",
        "    time_b = 15\n",
        "    if index_a != 0:\n",
        "        time_a = timetable[train_a][1][index_a] - timetable[train_a][1][index_a - 1] + 15\n",
        "    if index_b != 0:\n",
        "        time_b = timetable[train_b][1][index_b] - timetable[train_b][1][index_b - 1] + 15\n",
        "    return max(time_a,time_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNlNKBMYZgVW"
      },
      "source": [
        "# Rail Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsKfoyboORdc"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f0uJ7VZYOUTA"
      },
      "outputs": [],
      "source": [
        "from typing import Type\n",
        "import numpy as np\n",
        "import os\n",
        "# Import the structures\n",
        "\n",
        "\n",
        "example_training = 'training0'\n",
        "'''\n",
        "###############################################################\n",
        "######################   EXAMPLE 1  #########################\n",
        "###############################################################\n",
        "'''\n",
        "\n",
        "if example_training == 1:\n",
        "\t# Import the examples\n",
        "\tfrom examples.new_example_1 import rail, railway_example, av_line\n",
        "\t# Define the stations\n",
        "\tquarto_station = Station('Quarto', position = (3, 2), capacity = 2, min_wait_time = [2, 2, 1], \n",
        "\t\tadditional_wait_percent = [0.5, 1, 1.5], importance = 80, railway_topology = rail)\n",
        "\tquinto_station = Station('Quinto', position = (3, 9), capacity = 2, min_wait_time = [2, 2, 1], \n",
        "\t\tadditional_wait_percent = [0.5, 1, 1.5], importance = 80, railway_topology = rail)\n",
        "\n",
        "\t# Define the rail connection beetween the two stations\n",
        "\tconnection_quarto_quinto = Rail_connection(station_a = quarto_station, \n",
        "\t\tstation_b = quinto_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\t# Define the lines\n",
        "\tgenova_urbana = Line(type_line = Connection_type.NORMAL_RAIL, \n",
        "\t\tstations = (quarto_station, quinto_station), stops = (1, 1))\n",
        "\n",
        "\tstations = []\n",
        "\tstations.append([quarto_station.position, 0.5])\n",
        "\tstations.append([quinto_station.position, 0.5])\n",
        "\n",
        "\tstations_objects = [quarto_station, quinto_station]\n",
        "\n",
        "\t# Define the train runs\n",
        "\ttrain_run_0 = Train_run(genova_urbana, starting_time = 3, from_depot = True)\n",
        "\ttrain_run_1 = Train_run(genova_urbana, starting_time = 10, from_depot = True, inverse_train_direction = True)\n",
        "\ttrain_run_2 = Train_run(genova_urbana, starting_time = 40, inverse_train_direction = True)\n",
        "\ttrain_run_3 = Train_run(genova_urbana, starting_time = 70)\n",
        "\n",
        "\t# Define the convoys\n",
        "\tR1079_convoy = Convoy( Type_of_convoy.INTERCITY)\n",
        "\tR1078_convoy = Convoy( Type_of_convoy.INTERCITY)\n",
        "\n",
        "\tconvoys = [R1079_convoy, R1078_convoy]\n",
        "\n",
        "\t# Adding the train runs to the convoys\n",
        "\tR1079_convoy.add_train_run(train_run_0)\n",
        "\tR1079_convoy.add_train_run(train_run_2)\n",
        "\tR1078_convoy.add_train_run(train_run_1)\n",
        "\tR1078_convoy.add_train_run(train_run_3)\n",
        "\n",
        "\t# Generating the timetable\n",
        "\t# The timetable is composed by (station positions, time at which reach the stations, maximum train velocity)\n",
        "\ttimetable_example = calculate_timetable(convoys, rail)\n",
        "\"\"\" \n",
        "\t# TODO crea una funzione (per ora non è una funzione, serve quella) per esportare le timetable come excel\n",
        "\tfrom pandas import DataFrame\n",
        "\timport pandas as pd\n",
        "\n",
        "\tdf_quarto = [0]* len(convoys)\n",
        "\tdf_quinto = [0]* len(convoys)\n",
        "\tdf = [0]* len(convoys)\n",
        "\tquarto = []\n",
        "\tquinto = []\n",
        "\tfor i in range(len(convoys)):\n",
        "\t\tfor j in range(len(stations_objects)):\n",
        "\t\t\tfor k in range(len(timetable_example[i][0])):\n",
        "\t\t\t\tif j == 0:\n",
        "\t\t\t\t\tif timetable_example[i][0][k] == stations_objects[j].position:\n",
        "\t\t\t\t\t\tquarto.append(timetable_example[i][1][k])\n",
        "\t\t\t\tif j == 1:\n",
        "\t\t\t\t\tif timetable_example[i][0][k] == stations_objects[j].position:\n",
        "\t\t\t\t\t\tquinto.append(timetable_example[i][1][k])\n",
        "\t\tdf_quarto[i] = DataFrame({stations_objects[0].name : quarto})\n",
        "\t\tdf_quinto[i]  = DataFrame({stations_objects[1].name : quinto})\n",
        "\t\tframes = [df_quarto[i], df_quinto[i]]\n",
        "\t\tdf[i] = pd.concat(frames, axis=1)\n",
        "\t\tquarto = []\n",
        "\t\tquinto = []\n",
        "\n",
        "\t\t# funtion\n",
        "\tdef multiple_dfs(df_list, sheets, file_name, spaces):\n",
        "\t    writer = pd.ExcelWriter(file_name, engine='xlsxwriter')   \n",
        "\t    row = 0\n",
        "\t    for dataframe in df_list:\n",
        "\t        dataframe.to_excel(writer,sheet_name=sheets,startrow=row , startcol=0)   \n",
        "\t        row = row + len(dataframe.index) + spaces + 1\n",
        "\t    writer.save()\n",
        "\n",
        "\t# run function\n",
        "\tos.makedirs(\"output/timetables\", exist_ok=True)\n",
        "\tmultiple_dfs(df, 'Validation', 'output/timetables/timetable_test_1.xlsx', 1)\"\"\"\n",
        "\n",
        "if example_training == 2:\n",
        "\t# Import the examples\n",
        "\tfrom examples.new_example_2 import rail, railway_example, av_line\n",
        "\n",
        "\t# Define the stations\n",
        "\t# TODO raagiona sul come definire meglio la capacità delle stazioni e gestire i binari....\n",
        "\tquarto_station = Station('Quarto', position = (6, 2), capacity = 5, min_wait_time = [2, 2, 1], \n",
        "\t\tadditional_wait_percent = [0.5, 1, 1.5], importance = 80, railway_topology = rail)\n",
        "\tquinto_station = Station('Quinto', position = (6, 17), capacity = 5, min_wait_time = [2, 2, 1], \n",
        "\t\tadditional_wait_percent = [0.5, 1, 1.5], importance = 80, railway_topology = rail)\n",
        "\n",
        "\n",
        "\t# Define the rail connection beetween the two stations\n",
        "\tconnection_quarto_quinto = Rail_connection(station_a = quarto_station, \n",
        "\t\tstation_b = quinto_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\n",
        "\t# Define the lines\n",
        "\tgenova_urbana = Line(type_line = Connection_type.NORMAL_RAIL, \n",
        "\t\tstations = (quarto_station, quinto_station), stops = (1, 1))\n",
        "\n",
        "\tstations = []\n",
        "\tstations.append([quarto_station.position, 0.5])\n",
        "\tstations.append([quinto_station.position, 0.5])\n",
        "\tstations_objects = [quarto_station, quinto_station]\n",
        "\n",
        "\t# Define the train runs\n",
        "\ttrain_run_0 = Train_run(genova_urbana, starting_time = 3, from_depot = True)\n",
        "\ttrain_run_1 = Train_run(genova_urbana, starting_time = 40, from_depot = True, inverse_train_direction = True)\n",
        " \n",
        "\ttrain_run_2 = Train_run(genova_urbana, starting_time = 55, inverse_train_direction = True)\n",
        " \n",
        "\ttrain_run_3 = Train_run(genova_urbana, starting_time = 20, from_depot = True)\n",
        "\ttrain_run_5 = Train_run(genova_urbana, starting_time = 70, inverse_train_direction = True, to_depot = True)\n",
        " \n",
        "\ttrain_run_4 = Train_run(genova_urbana, starting_time = 20, from_depot = True)\n",
        "\ttrain_run_6 = Train_run(genova_urbana, starting_time = 75, inverse_train_direction = True, to_depot = True)\n",
        "\n",
        "\t# Define the convoys\n",
        "\tR1079_convoy = Convoy( Type_of_convoy.INTERCITY)\n",
        "\tR1078_convoy = Convoy( Type_of_convoy.INTERCITY)\n",
        "\tR1077_convoy = Convoy( Type_of_convoy.INTERCITY)\n",
        "\n",
        "\t# SOME MODIFICATION TO STUDY TRAINING\n",
        "\t#R1076_convoy = Convoy( Type_of_convoy.INTERCITY)\n",
        "\n",
        "\tconvoys = [R1079_convoy, R1078_convoy, R1077_convoy] #, R1076_convoy]\n",
        "\n",
        "\t# Adding the train runs to the convoys\n",
        "\t# R1079\n",
        "\tR1079_convoy.add_train_run(train_run_0)\n",
        "\tR1079_convoy.add_train_run(train_run_2)\n",
        "\t# R1078\n",
        "\tR1078_convoy.add_train_run(train_run_1)\n",
        "\t# R1077\n",
        "\tR1077_convoy.add_train_run(train_run_3)\n",
        "\tR1077_convoy.add_train_run(train_run_5)\n",
        "\t# R1076\n",
        "\t#R1076_convoy.add_train_run(train_run_4)\n",
        "\n",
        "\t# Generating the timetable\n",
        "\t# The timetable is composed by (station positions, time at which reach the stations, maximum train velocity)\n",
        "\ttimetable_example = calculate_timetable(convoys, rail)\n",
        "\n",
        "'''\n",
        "###############################################################\n",
        "######################   EXAMPLE 3  #########################\n",
        "###############################################################\n",
        "'''\n",
        "\n",
        "if example_training == 3:\n",
        "\t# Import the examples\n",
        "\tfrom examples.new_example_3 import rail, railway_example, av_line\n",
        "\n",
        "\t# Define the stations\n",
        "\tgenova_station = Station('Genova', position = (21,2), capacity = 2, min_wait_time = [2, 2, 1], \n",
        "\t\tadditional_wait_percent = [0.5, 1, 1.5], importance = 80, railway_topology = rail)\n",
        "\tpavia_station = Station('Pavia', position = (21,18), capacity = 2, min_wait_time = [2, 2, 1],\n",
        "\t\tadditional_wait_percent = [0.5, 1, 1.5], importance = 80, railway_topology = rail)\n",
        "\tmilano_station = Station('Milano', position = (15,49), capacity = 2, min_wait_time = [2, 2, 1],\n",
        "\t\tadditional_wait_percent = [0.5, 1, 1.5], importance = 80, railway_topology = rail)\n",
        "\ttorino_station = Station('Torino', position = (9,1), capacity = 2, min_wait_time = [2, 2, 1],\n",
        "\t\tadditional_wait_percent = [0.5, 1, 1.5], importance = 80, railway_topology = rail)\n",
        "\tnovara_station = Station('Novara', position = (3,40), capacity = 2, min_wait_time = [2, 2, 1],\n",
        "\t\tadditional_wait_percent = [0.5, 1, 1.5], importance = 80, railway_topology = rail)\n",
        "\n",
        "\t# Define the connections between station\n",
        "\tconnection_genova_pavia = Rail_connection(station_a = genova_station, \n",
        "\t\tstation_b = pavia_station, rail_connection_type = Connection_type.HIGH_VELOCITY_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\tconnection_pavia_milano = Rail_connection(station_a = pavia_station, \n",
        "\t\tstation_b = milano_station, rail_connection_type = Connection_type.HIGH_VELOCITY_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\tconnection_genova_torino = Rail_connection(station_a = pavia_station, \n",
        "\t\tstation_b = milano_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\tconnection_torino_novara = Rail_connection(station_a = torino_station, \n",
        "\t\tstation_b = novara_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\tconnection_novara_pavia = Rail_connection(station_a = novara_station, \n",
        "\t\tstation_b = pavia_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\tconnection_torino_pavia = Rail_connection(station_a = torino_station, \n",
        "\t\tstation_b = pavia_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\t# Define the lines\n",
        "\tgenova_milano = Line(type_line = Connection_type.HIGH_VELOCITY_RAIL, \n",
        "\t\tstations = (genova_station, pavia_station, milano_station), stops = (1, 1, 1))\n",
        "\ttorino_milano = Line(type_line = Connection_type.NORMAL_RAIL, \n",
        "\t\tstations = (torino_station, novara_station, milano_station), stops = (1, 1, 1))\n",
        "\tgenova_torino = Line(type_line = Connection_type.NORMAL_RAIL, \n",
        "\t\tstations = (genova_station, torino_station), stops = (1, 1))\n",
        "\n",
        "\tstations = []\n",
        "\tstations.append([genova_station.position, 1])\n",
        "\tstations.append([pavia_station.position, 1])\n",
        "\tstations.append([milano_station.position, 1])\n",
        "\tstations.append([torino_station.position, 0.5])\n",
        "\tstations.append([novara_station.position, 0.5])\n",
        "\n",
        "\tstations_objects = [genova_station, pavia_station, milano_station, torino_station, novara_station]\n",
        "\n",
        "\t# Defining the train runs\n",
        "\t# convoy 1\n",
        "\ttrain_run_0 = Train_run(genova_milano, starting_time = 5, from_depot = True)\n",
        "\t# convoy 2\n",
        "\ttrain_run_1 = Train_run(genova_milano, starting_time = 7, from_depot = True, inverse_train_direction = True)\n",
        "\t# convoy 3\n",
        "\ttrain_run_2 = Train_run(torino_milano, starting_time = 12, from_depot = True)\n",
        "\t# convoy 4\n",
        "\ttrain_run_3 = Train_run(torino_milano, starting_time = 3, from_depot = True, inverse_train_direction = True)\n",
        "\t# convoy 5\n",
        "\ttrain_run_4 = Train_run(genova_torino, starting_time = 11, from_depot = True)\n",
        "\t# convoy 6\n",
        "\ttrain_run_5 = Train_run(genova_torino, starting_time = 20, from_depot = True, inverse_train_direction = True)\n",
        "\n",
        "\t# Define the convoysss\n",
        "\tFR1001_convoy = Convoy( Type_of_convoy.HIGH_VELOCITY)\n",
        "\tR1002_convoy = Convoy( Type_of_convoy.INTERCITY)\t\n",
        "\tR1003_convoy = Convoy( Type_of_convoy.INTERCITY)\n",
        "\tFR1004_convoy = Convoy( Type_of_convoy.HIGH_VELOCITY)\n",
        "\tR1005_convoy = Convoy( Type_of_convoy.INTERCITY)\n",
        "\tR1006_convoy = Convoy( Type_of_convoy.INTERCITY)\n",
        "\n",
        "\tconvoys = [FR1001_convoy, R1002_convoy, R1003_convoy, FR1004_convoy, R1005_convoy, R1006_convoy]\n",
        "\n",
        "\t# Add the train runs to convoys\n",
        "\tFR1001_convoy.add_train_run(train_run_0)\n",
        "\n",
        "\tR1002_convoy.add_train_run(train_run_1)\n",
        "\n",
        "\tR1003_convoy.add_train_run(train_run_2)\n",
        "\n",
        "\tFR1004_convoy.add_train_run(train_run_3)\n",
        "\n",
        "\tR1005_convoy.add_train_run(train_run_4)\n",
        "\n",
        "\tR1006_convoy.add_train_run(train_run_5)\n",
        "\n",
        "\t#Calculate plan to do\n",
        "\n",
        "\ttimetable_example = calculate_timetable(convoys, rail)\n",
        "\n",
        "'''\n",
        "###############################################################\n",
        "######################   EXAMPLE 0  #########################\n",
        "###############################################################\n",
        "'''\n",
        "\n",
        "if example_training == 0:\n",
        "\n",
        "\t# Import the examples\n",
        "\tfrom examples.esempio_prova import rail, railway_example, av_line\n",
        "\n",
        "\tstazione_prova = Station('Prova', position = (3,2), capacity = 1, min_wait_time = 1, additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\tstazione_prova_2 = Station('Prova 2', position = (3, 9), capacity = 1, min_wait_time = 1, additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\n",
        "\tconnection_prova = Rail_connection(station_a = stazione_prova, \n",
        "\t\tstation_b = stazione_prova_2, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tlinea_prova = Line(type_line = Connection_type.NORMAL_RAIL, \n",
        "\t\tstations = (stazione_prova, stazione_prova_2), stops = (1, 1))\n",
        "\n",
        "\tstations = []\n",
        "\tstations.append([stazione_prova.position, 0.5])\n",
        "\tstations.append([stazione_prova_2.position, 0.5])\n",
        "\n",
        "\t# Define the train runs\n",
        "\ttrain_run_0 = Train_run(linea_prova, starting_time = 3, from_depot = True)\n",
        "\ttrain_run_1 = Train_run(linea_prova, starting_time = 40, from_depot = True, inverse_train_direction = True)\n",
        "\n",
        "\t# Define the convoys\n",
        "\tR1079_convoy = Convoy( Type_of_convoy.INTERCITY)\n",
        "\tR1078_convoy = Convoy( Type_of_convoy.INTERCITY)\n",
        "\n",
        "\tconvoys = [R1079_convoy, R1078_convoy]\n",
        "\n",
        "\t# Adding the train runs to the convoys\n",
        "\tR1079_convoy.add_train_run(train_run_0)\n",
        "\tR1078_convoy.add_train_run(train_run_1)\n",
        "\n",
        "\t# Generating the timetable\n",
        "\t# The timetable is composed by (station positions, time at which reach the stations, maximum train velocity)\n",
        "\ttimetable_example = calculate_timetable(convoys, rail)\n",
        "\n",
        "'''\n",
        "###############################################################\n",
        "######################   Training 0  #########################\n",
        "###############################################################\n",
        "'''\n",
        "\n",
        "if example_training == 'training0':\n",
        "\t# Import the examples\n",
        "\n",
        "\tgenova_station = Station('Genova', position = (6,2), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\trecco_station = Station('Recco', position = (6,13), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\tchiavari_station = Station('Chiavari', position = (6,23), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\n",
        "\tconnection_genova_recco = Rail_connection(station_a = genova_station, \n",
        "\t\tstation_b = recco_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tconnection_recco_chiavari = Rail_connection(station_a = recco_station, \n",
        "\t\tstation_b = chiavari_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tlinea_genova_levante = Line(type_line = Connection_type.NORMAL_RAIL, \n",
        "\t\tstations = (genova_station, recco_station, chiavari_station), stops = (1, 1, 1))\n",
        "\n",
        "\tstations = []\n",
        "\n",
        "\tstations.append([genova_station.position, 0.5])\n",
        "\tstations.append([recco_station.position, 0.5])\n",
        "\tstations.append([chiavari_station, 0.5])\n",
        "\n",
        "\ttrain_run_0 = Train_run(linea_genova_levante, starting_time = 3, from_depot = True)\n",
        "\n",
        "\tR102_convoy = Convoy(Type_of_convoy.INTERCITY)\n",
        "\n",
        "\tconvoys = [R102_convoy]\n",
        "\n",
        "\tR102_convoy.add_train_run(train_run_0)\n",
        "\n",
        "\ttimetable_example = calculate_timetable(convoys, rail)\n",
        "\n",
        "\t# Interruption\n",
        "\ttimetable_example.append([[(6,8), (6,13)],[1, 13], 0.5])\n",
        "\ttimetable_example.append([[(5,8), (5,13)],[1, 13], 0.5])\n",
        "\n",
        "\n",
        "'''\n",
        "###############################################################\n",
        "######################   Training 0.1 (FOUR STATIONS)  #########################\n",
        "###############################################################\n",
        "'''\n",
        "\n",
        "if example_training == 'training0.1':\n",
        "\t# Import the examples\n",
        "\tfrom examples.four_stations import rail, railway_example, av_line\n",
        "\n",
        "\tgenova_station = Station('Genova', position = (6,2), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\trecco_station = Station('Recco', position = (6,13), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\tchiavari_station = Station('Chiavari', position = (6,23), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "   \n",
        "\tspezia_station = Station('Spezia', position = (6,32),capacity=3, min_wait_time=[1,1,1],additional_wait_percent=1,importance=1, railway_topology = rail)\n",
        "\n",
        "\n",
        "\tconnection_genova_recco = Rail_connection(station_a = genova_station, \n",
        "\t\tstation_b = recco_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tconnection_recco_chiavari = Rail_connection(station_a = recco_station, \n",
        "\t\tstation_b = chiavari_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tconnection_chiavari_spezia = Rail_connection(station_a = chiavari_station,\n",
        "\t\tstation_b = spezia_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable=[0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tlinea_genova_levante = Line(type_line = Connection_type.NORMAL_RAIL, \n",
        "\t\tstations = (genova_station, recco_station, chiavari_station, spezia_station), stops = (1, 1, 1, 1))\n",
        "\n",
        "\tstations = []\n",
        "\n",
        "\tstations.append([genova_station.position, 0.5])\n",
        "\tstations.append([recco_station.position, 0.5])\n",
        "\tstations.append([chiavari_station, 0.5])\n",
        "\n",
        "\tstations.append([spezia_station, 0.5])\t\n",
        "\n",
        "\ttrain_run_0 = Train_run(linea_genova_levante, starting_time = 3, from_depot = True)\n",
        "\n",
        "\tR102_convoy = Convoy(Type_of_convoy.INTERCITY)\n",
        "\n",
        "\tconvoys = [R102_convoy]\n",
        "\n",
        "\tR102_convoy.add_train_run(train_run_0)\n",
        "\n",
        "\ttimetable_example = calculate_timetable(convoys, rail)\n",
        "\n",
        "\t# Interruption\n",
        "\ttimetable_example.append([[(6,8), (6,11)],[1, 225], 0.5])\n",
        "\ttimetable_example.append([[(5,8), (5,11)],[1, 225], 0.5])\n",
        "\n",
        "\n",
        "'''\n",
        "###############################################################\n",
        "######################   Training 1  #########################\n",
        "###############################################################\n",
        "'''\n",
        "\n",
        "if example_training == 'training1':\n",
        "\t# Import the examples\n",
        "\tfrom examples.ferrovia_luca import rail, railway_example, av_line\n",
        "\n",
        "\tgenova_station = Station('Genova', position = (6,2), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\trecco_station = Station('Recco', position = (6,13), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\tchiavari_station = Station('Chiavari', position = (6,23), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\n",
        "\tconnection_genova_recco = Rail_connection(station_a = genova_station, \n",
        "\t\tstation_b = recco_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tconnection_recco_chiavari = Rail_connection(station_a = recco_station, \n",
        "\t\tstation_b = chiavari_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tlinea_genova_levante = Line(type_line = Connection_type.NORMAL_RAIL, \n",
        "\t\tstations = (genova_station, recco_station, chiavari_station), stops = (1, 1, 1))\n",
        "\n",
        "\tstations = []\n",
        "\n",
        "\tstations.append([genova_station.position, 0.5])\n",
        "\tstations.append([recco_station.position, 0.5])\n",
        "\tstations.append([chiavari_station, 0.5])\n",
        "\n",
        "\ttrain_run_0 = Train_run(linea_genova_levante, starting_time = 3, from_depot = True)\n",
        "\n",
        "\ttrain_run_1 = Train_run(linea_genova_levante, starting_time = 3, from_depot = True, inverse_train_direction = True)\n",
        "\n",
        "\tR102_convoy = Convoy(Type_of_convoy.INTERCITY)\n",
        "\tR103_convoy = Convoy(Type_of_convoy.INTERCITY)\n",
        "\n",
        "\tconvoys = [R102_convoy, R103_convoy]\n",
        "\n",
        "\tR102_convoy.add_train_run(train_run_0)\n",
        "\n",
        "\tR103_convoy.add_train_run(train_run_1)\n",
        "\n",
        "\ttimetable_example = calculate_timetable(convoys, rail)\n",
        "\n",
        "\t# Interruption\n",
        "\ttimetable_example.append([[(6,8), (6,11)],[1, 225], 0.5])\n",
        "\ttimetable_example.append([[(5,8), (5,11)],[1, 225], 0.5])\n",
        "\n",
        "\n",
        "'''\n",
        "###############################################################\n",
        "######################   Training 1.1  #########################\n",
        "###############################################################\n",
        "'''\n",
        "\n",
        "if example_training == 'training1.1':\n",
        "\t# Import the examples\n",
        "\tfrom examples.ferrovia_luca import rail, railway_example, av_line\n",
        "\n",
        "\tgenova_station = Station('Genova', position = (6,2), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\trecco_station = Station('Recco', position = (6,13), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\tchiavari_station = Station('Chiavari', position = (6,23), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\n",
        "\tconnection_genova_recco = Rail_connection(station_a = genova_station, \n",
        "\t\tstation_b = recco_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tconnection_recco_chiavari = Rail_connection(station_a = recco_station, \n",
        "\t\tstation_b = chiavari_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tlinea_genova_levante = Line(type_line = Connection_type.NORMAL_RAIL, \n",
        "\t\tstations = (genova_station, recco_station, chiavari_station), stops = (1, 1, 1))\n",
        "\n",
        "\tstations = []\n",
        "\n",
        "\tstations.append([genova_station.position, 0.5])\n",
        "\tstations.append([recco_station.position, 0.5])\n",
        "\tstations.append([chiavari_station, 0.5])\n",
        "\n",
        "\ttrain_run_0 = Train_run(linea_genova_levante, starting_time = 3, from_depot = True)\n",
        "\n",
        "\ttrain_run_1 = Train_run(linea_genova_levante, starting_time = 3, from_depot = True, inverse_train_direction = True)\n",
        "\n",
        "\tR102_convoy = Convoy(Type_of_convoy.INTERCITY)\n",
        "\tR103_convoy = Convoy(Type_of_convoy.INTERCITY)\n",
        "\n",
        "\tconvoys = [R102_convoy, R103_convoy]\n",
        "\n",
        "\tR102_convoy.add_train_run(train_run_0)\n",
        "\n",
        "\tR103_convoy.add_train_run(train_run_1)\n",
        "\n",
        "\ttimetable_example = calculate_timetable(convoys, rail)\n",
        "\n",
        "\t# Interruption\n",
        "\ttimetable_example.append([[(6,8), (6,11)],[1, 225], 0.5])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t'''\n",
        "###############################################################\n",
        "######################      DEBUG     #########################\n",
        "###############################################################\n",
        "'''\n",
        "\n",
        "if example_training == 'debug':\n",
        "\t# Import the examples\n",
        "\tfrom examples.ferrovia_luca import rail, railway_example, av_line\n",
        "\n",
        "\tgenova_station = Station('Genova', position = (6,2), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\trecco_station = Station('Recco', position = (6,13), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\tchiavari_station = Station('Chiavari', position = (6,23), capacity = 3, min_wait_time = [1, 1, 1], additional_wait_percent =1, importance = 1, railway_topology = rail)\n",
        "\n",
        "\tconnection_genova_recco = Rail_connection(station_a = genova_station, \n",
        "\t\tstation_b = recco_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tconnection_recco_chiavari = Rail_connection(station_a = recco_station, \n",
        "\t\tstation_b = chiavari_station, rail_connection_type = Connection_type.NORMAL_RAIL,\n",
        "\t\tmax_speed_usable = [0.9, 0.6, 0.3], additional_runtime_percent = [0.1, 0.1, 0.1])\n",
        "\n",
        "\tlinea_genova_levante = Line(type_line = Connection_type.NORMAL_RAIL, \n",
        "\t\tstations = (genova_station, recco_station, chiavari_station), stops = (1, 1, 1))\n",
        "\n",
        "\tstations = []\n",
        "\n",
        "\tstations.append([genova_station.position, 0.5])\n",
        "\tstations.append([recco_station.position, 0.5])\n",
        "\tstations.append([chiavari_station, 0.5])\n",
        "\n",
        "\ttrain_run_0 = Train_run(linea_genova_levante, starting_time = 3, from_depot = True)\n",
        "\n",
        "\tR102_convoy = Convoy(Type_of_convoy.INTERCITY)\n",
        "\n",
        "\tconvoys = [R102_convoy]\n",
        "\n",
        "\tR102_convoy.add_train_run(train_run_0)\n",
        "\n",
        "\ttimetable_example = calculate_timetable(convoys, rail)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hNWqYVGZmPi"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QT5xWtspZlzI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import itertools\n",
        "import warnings\n",
        "from typing import Tuple, List, Callable, Mapping, Optional, Any\n",
        "from flatland.envs.timetable_utils import Timetable\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random.mtrand import RandomState\n",
        "\n",
        "from flatland.envs.agent_utils import EnvAgent\n",
        "from flatland.envs.distance_map import DistanceMap\n",
        "from flatland.envs.rail_env_shortest_paths import get_shortest_paths\n",
        "\n",
        "def len_handle_none(v):\n",
        "    if v is not None:\n",
        "        return len(v)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "from flatland.envs.rail_env import RailEnvActions\n",
        "\n",
        "# Import your own Agent or use RLlib to train agents on Flatland\n",
        "# As an example we use a random agent instead\n",
        "class RandomAgent:\n",
        "\n",
        "\tdef __init__(self, state_size, action_size):\n",
        "\t\tself.state_size = state_size\n",
        "\t\tself.action_size = action_size\n",
        "\n",
        "\n",
        "\t# HERE DEFINE THE ACTIONS TO DO IN CASE THE AGENT IS NOT IN THE DETERMINISTIC PART \n",
        "\t# For now the agents can only move forward (for DEBUG)\n",
        "\tdef act(self, state):\n",
        "\t\treturn RailEnvActions.MOVE_FORWARD\n",
        "\n",
        "\tdef step(self, memories):\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tStep function to improve agent by adjusting policy given the observations\n",
        "\n",
        "\t\t:param memories: SARS Tuple to be\n",
        "\t\t:return:\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\treturn\n",
        "\n",
        "\tdef save(self, filename):\n",
        "\t\t# Store the current policy\n",
        "\t\treturn\n",
        "\n",
        "\tdef load(self, filename):\n",
        "\t\t# Load a policy\n",
        "\t\treturn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1kfDM1pUxaCt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def timetable_generator(timetable, agents: List[EnvAgent], distance_map: DistanceMap, \n",
        "                            agents_hints: dict, np_random: RandomState = None) -> Timetable:\n",
        "    \"\"\"\n",
        "    Calculates earliest departure and latest arrival times for the agents\n",
        "    This is the new addition in Flatland 3\n",
        "    Also calculates the max episodes steps based on the density of the timetable\n",
        "\n",
        "    inputs: \n",
        "        agents - List of all the agents rail_env.agents\n",
        "        distance_map - Distance map of positions to tagets of each agent in each direction\n",
        "        agent_hints - Uses the number of cities\n",
        "        np_random - RNG state for seeding\n",
        "    returns:\n",
        "        Timetable with the latest_arrivals, earliest_departures and max_episdode_steps\n",
        "    \"\"\"\n",
        "    # max_episode_steps calculation\n",
        "    if agents_hints:\n",
        "        city_positions = agents_hints['city_positions']\n",
        "        num_cities = len(city_positions)\n",
        "    else:\n",
        "        num_cities = 2\n",
        "\n",
        "    timedelay_factor = 4\n",
        "    alpha = 2\n",
        "    max_episode_steps = int(timedelay_factor * alpha * \\\n",
        "        (distance_map.rail.width + distance_map.rail.height + (len(agents) / num_cities)))\n",
        "    \n",
        "    # Multipliers\n",
        "    old_max_episode_steps_multiplier = 3.0\n",
        "    new_max_episode_steps_multiplier = 1.5\n",
        "    travel_buffer_multiplier = 1.3 # must be strictly lesser than new_max_episode_steps_multiplier\n",
        "    assert new_max_episode_steps_multiplier > travel_buffer_multiplier\n",
        "    end_buffer_multiplier = 0.05\n",
        "    mean_shortest_path_multiplier = 0.2\n",
        "    \n",
        "    shortest_paths = get_shortest_paths(distance_map)\n",
        "    shortest_paths_lengths = [len_handle_none(v) for k,v in shortest_paths.items()]\n",
        "\n",
        "    # Find mean_shortest_path_time\n",
        "    agent_speeds = [agent.speed_counter.speed for agent in agents]\n",
        "    agent_shortest_path_times = np.array(shortest_paths_lengths)/ np.array(agent_speeds)\n",
        "    mean_shortest_path_time = np.mean(agent_shortest_path_times)\n",
        "\n",
        "    # Deciding on a suitable max_episode_steps\n",
        "    longest_speed_normalized_time = np.max(agent_shortest_path_times)\n",
        "    mean_path_delay = mean_shortest_path_time * mean_shortest_path_multiplier\n",
        "    max_episode_steps_new = int(np.ceil(longest_speed_normalized_time * new_max_episode_steps_multiplier) + mean_path_delay)\n",
        "    \n",
        "    max_episode_steps_old = int(max_episode_steps * old_max_episode_steps_multiplier)\n",
        "\n",
        "    max_episode_steps = min(max_episode_steps_new, max_episode_steps_old)\n",
        "    \n",
        "    end_buffer = int(max_episode_steps * end_buffer_multiplier)\n",
        "    latest_arrival_max = max_episode_steps-end_buffer\n",
        "\n",
        "    # Useless unless needed by returning\n",
        "    earliest_departures = []\n",
        "    latest_arrivals = []\n",
        "\n",
        "    for agent in agents:\n",
        "        i_agent = agent.handle\n",
        "        agent_shortest_path_time = agent_shortest_path_times[agent.handle]\n",
        "        agent_travel_time_max = int(np.ceil((agent_shortest_path_time * travel_buffer_multiplier) + mean_path_delay))\n",
        "        \n",
        "        departure_window_max = max(latest_arrival_max - agent_travel_time_max, 1)\n",
        "        \n",
        "        #earliest_departure = np_random.randint(0, departure_window_max)\n",
        "        earliest_departure = timetable[i_agent][1][0]\n",
        "        \n",
        "        #latest_arrival = earliest_departure + agent_travel_time_max\n",
        "        latest_arrival = timetable[i_agent][1][-1]\n",
        "        \n",
        "        earliest_departures.append(earliest_departure)\n",
        "        latest_arrivals.append(latest_arrival)\n",
        "\n",
        "        agent.earliest_departure = earliest_departure\n",
        "        agent.latest_arrival = latest_arrival\n",
        "\n",
        "    return Timetable(earliest_departures=earliest_departures, latest_arrivals=latest_arrivals,\n",
        "                    max_episode_steps=max_episode_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXdkQKaRadDL"
      },
      "source": [
        "### Rewards and Penalities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImedOM4oaKbh"
      },
      "source": [
        "Here define the penalities and rewards for the agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "X4zDuy13aJVH"
      },
      "outputs": [],
      "source": [
        "# Penalities \n",
        "step_penality = - 0.01               # a step is time passing, so a penality for each step is needed\n",
        "stop_penality = -0.05                # penalty for stopping a moving agent\n",
        "reverse_penality = -0.07            # penalty for reversing the march of an agent\n",
        "skip_penality = 0                   # penalty for skipping a station\n",
        "target_not_reached_penalty = -20     # penalty for not reaching the final target (depot)\n",
        "default_skip_penalty = 100\n",
        "cancellation_factor = 1\n",
        "cancellation_time_buffer = 0\n",
        "\n",
        "target_reward = 30         # reward for an agent reaching his final target\n",
        "station_passage_reward = 10 # reward for an agent reaching intermediate station, the reward is wheighted with the delay of the agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVWfJEALailv"
      },
      "source": [
        "### Training Flag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "afVzyw0Pak3a"
      },
      "outputs": [],
      "source": [
        "example_training = 'training0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9B5UGAXYgJq"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2_86tpr-Z7Au"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Definition of the RailEnv environment.\n",
        "\"\"\"\n",
        "import random\n",
        "from random import *\n",
        "\n",
        "from typing import List, Optional, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from gym.utils import seeding\n",
        "\n",
        "from flatland.utils.rendertools import RenderTool, AgentRenderVariant\n",
        "from flatland.core.env import Environment\n",
        "from flatland.core.env_observation_builder import ObservationBuilder\n",
        "from flatland.core.grid.grid4 import Grid4Transitions\n",
        "from flatland.core.transition_map import GridTransitionMap\n",
        "from flatland.envs.agent_utils import EnvAgent\n",
        "from flatland.envs.distance_map import DistanceMap\n",
        "\n",
        "from flatland.envs import malfunction_generators as mal_gen\n",
        "from flatland.envs import rail_generators as rail_gen\n",
        "from flatland.envs import line_generators as line_gen\n",
        "from flatland.envs import persistence\n",
        "from flatland.envs import agent_chains as ac\n",
        "\n",
        "from flatland.envs.observations import GlobalObsForRailEnv\n",
        "\n",
        "from flatland.envs.step_utils.states import TrainState, StateTransitionSignals\n",
        "from flatland.envs.step_utils.transition_utils import check_valid_action\n",
        "from flatland.envs.step_utils import action_preprocessing\n",
        "from flatland.envs.step_utils import env_utils\n",
        "\n",
        "training = example_training\n",
        "\n",
        "class RailEnv(Environment):\n",
        "    \"\"\"\n",
        "    RailEnv environment class.\n",
        "\n",
        "    RailEnv is an environment inspired by a (simplified version of) a rail\n",
        "    network, in which agents (trains) have to navigate to their target\n",
        "    locations in the shortest time possible, while at the same time cooperating\n",
        "    to avoid bottlenecks.\n",
        "\n",
        "    The valid actions in the environment are:\n",
        "\n",
        "     -   0: do nothing (continue moving or stay still)\n",
        "     -   1: turn left at switch and move to the next cell; if the agent was not moving, movement is started\n",
        "     -   2: move to the next cell in front of the agent; if the agent was not moving, movement is started\n",
        "     -   3: turn right at switch and move to the next cell; if the agent was not moving, movement is started\n",
        "     -   4: stop moving\n",
        "     -   5: invert the direction of march\n",
        "\n",
        "    Moving forward in a dead-end cell makes the agent turn 180 degrees and step\n",
        "    to the cell it came from.\n",
        "\n",
        "    The actions of the agents are executed in order of their handle to prevent\n",
        "    deadlocks and to allow them to learn relative priorities.\n",
        "\n",
        "    Reward Function:\n",
        "\n",
        "    It costs each agent a step_penalty for every time-step taken in the environment. Independent of the movement\n",
        "    of the agent. Currently all other penalties such as penalty for stopping, starting and invalid actions are set to 0.\n",
        "\n",
        "    alpha = 1\n",
        "    beta = 1\n",
        "    Reward function parameters:\n",
        "\n",
        "    - invalid_action_penalty = 0\n",
        "    - step_penalty = -alpha\n",
        "    - global_reward = beta\n",
        "    - epsilon = avoid rounding errors\n",
        "    - stop_penalty = 0  # penalty for stopping a moving agent\n",
        "    - start_penalty = 0  # penalty for starting a stopped agent\n",
        "\n",
        "    Stochastic malfunctioning of trains:\n",
        "    Trains in RailEnv can malfunction if they are halted too often (either by their own choice or because an invalid\n",
        "    action or cell is selected.\n",
        "\n",
        "    Every time an agent stops, an agent has a certain probability of malfunctioning. Malfunctions of trains follow a\n",
        "    poisson process with a certain rate. Not all trains will be affected by malfunctions during episodes to keep\n",
        "    complexity managable.\n",
        "\n",
        "    TODO: currently, the parameters that control the stochasticity of the environment are hard-coded in init().\n",
        "    For Round 2, they will be passed to the constructor as arguments, to allow for more flexibility.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    cancellation_factor = 1\n",
        "    cancellation_time_buffer = 0\n",
        "\n",
        "    def __init__(self,\n",
        "                 width,\n",
        "                 height,\n",
        "                 max_episode_steps,\n",
        "                 rail_generator=None,\n",
        "                 line_generator=None,  # : line_gen.LineGenerator = line_gen.random_line_generator(),\n",
        "                 number_of_agents=2,\n",
        "                 obs_builder_object: ObservationBuilder = GlobalObsForRailEnv(),\n",
        "                 malfunction_generator_and_process_data=None,  # mal_gen.no_malfunction_generator(),\n",
        "                 malfunction_generator=None,\n",
        "                 remove_agents_at_target=True,\n",
        "                 random_seed=None,\n",
        "                 record_steps=False,\n",
        "                 close_following=True,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Environment init.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rail_generator : function\n",
        "            The rail_generator function is a function that takes the width,\n",
        "            height and agents handles of a  rail environment, along with the number of times\n",
        "            the env has been reset, and returns a GridTransitionMap object and a list of\n",
        "            starting positions, targets, and initial orientations for agent handle.\n",
        "            The rail_generator can pass a distance map in the hints or information for specific line_generators.\n",
        "            Implementations can be found in flatland/envs/rail_generators.py\n",
        "        line_generator : function\n",
        "            The line_generator function is a function that takes the grid, the number of agents and optional hints\n",
        "            and returns a list of starting positions, targets, initial orientations and speed for all agent handles.\n",
        "            Implementations can be found in flatland/envs/line_generators.py\n",
        "        width : int\n",
        "            The width of the rail map. Potentially in the future,\n",
        "            a range of widths to sample from.\n",
        "        height : int\n",
        "            The height of the rail map. Potentially in the future,\n",
        "            a range of heights to sample from.\n",
        "        number_of_agents : int\n",
        "            Number of agents to spawn on the map. Potentially in the future,\n",
        "            a range of number of agents to sample from.\n",
        "        obs_builder_object: ObservationBuilder object\n",
        "            ObservationBuilder-derived object that takes builds observation\n",
        "            vectors for each agent.\n",
        "        remove_agents_at_target : bool\n",
        "            If remove_agents_at_target is set to true then the agents will be removed by placing to\n",
        "            RailEnv.DEPOT_POSITION when the agent has reach it's target position.\n",
        "        random_seed : int or None\n",
        "            if None, then its ignored, else the random generators are seeded with this number to ensure\n",
        "            that stochastic operations are replicable across multiple operations\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if malfunction_generator_and_process_data is not None:\n",
        "            print(\"DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator\")\n",
        "            self.malfunction_generator, self.malfunction_process_data = malfunction_generator_and_process_data\n",
        "        elif malfunction_generator is not None:\n",
        "            self.malfunction_generator = malfunction_generator\n",
        "            # malfunction_process_data is not used\n",
        "            # self.malfunction_generator, self.malfunction_process_data = malfunction_generator_and_process_data\n",
        "            self.malfunction_process_data = self.malfunction_generator.get_process_data()\n",
        "        # replace default values here because we can't use default args values because of cyclic imports\n",
        "        else:\n",
        "            self.malfunction_generator = mal_gen.NoMalfunctionGen()\n",
        "            self.malfunction_process_data = self.malfunction_generator.get_process_data()\n",
        "        \n",
        "        self.number_of_agents = number_of_agents\n",
        "\n",
        "        if rail_generator is None:\n",
        "            rail_generator = rail_gen.sparse_rail_generator()\n",
        "        self.rail_generator = rail_generator\n",
        "        if line_generator is None:\n",
        "            line_generator = line_gen.sparse_line_generator()\n",
        "        self.line_generator = line_generator\n",
        "\n",
        "        self.rail: Optional[GridTransitionMap] = None\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "\n",
        "        self.remove_agents_at_target = remove_agents_at_target\n",
        "\n",
        "        self.obs_builder = obs_builder_object\n",
        "        self.obs_builder.set_env(self)\n",
        "\n",
        "        self._max_episode_steps: Optional[int] = max_episode_steps\n",
        "        self._elapsed_steps = 0\n",
        "\n",
        "        self.obs_dict = {}\n",
        "        self.rewards_dict = {}\n",
        "        self.dev_obs_dict = {}\n",
        "        self.dev_pred_dict = {}\n",
        "\n",
        "        self.agents: List[EnvAgent] = []\n",
        "        self.num_resets = 0\n",
        "        self.distance_map = DistanceMap(self.agents, self.height, self.width)\n",
        "\n",
        "        self.action_space = [6]\n",
        "        \n",
        "        self.previous_station = [[(-1,0)]] * number_of_agents\n",
        "        \n",
        "        self.dones_for_position = [False] * number_of_agents\n",
        "        \n",
        "\n",
        "        self._seed()\n",
        "        if random_seed:\n",
        "            self._seed(seed=random_seed)\n",
        "\n",
        "        self.agent_positions = None\n",
        "\n",
        "        self.run_once = [0]*(self.number_of_agents)   # Flag to check when a train has started   \n",
        "\n",
        "        # save episode timesteps ie agent positions, orientations.  (not yet actions / observations)\n",
        "        self.record_steps = record_steps  # whether to save timesteps\n",
        "        # save timesteps in here: [[[row, col, dir, malfunction],...nAgents], ...nSteps]\n",
        "        self.cur_episode = []\n",
        "        self.list_actions = []  # save actions in here\n",
        "\n",
        "        self.motionCheck = ac.MotionCheck()\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        random.seed(seed)\n",
        "        self.random_seed = seed\n",
        "\n",
        "        # Keep track of all the seeds in order\n",
        "        if not hasattr(self, 'seed_history'):\n",
        "            self.seed_history = [seed]\n",
        "        if self.seed_history[-1] != seed:\n",
        "            self.seed_history.append(seed)\n",
        "\n",
        "        return [seed]\n",
        "\n",
        "    def find_indices(self, array, index_to_find):\n",
        "        indeces = []\n",
        "        for i in range(len(array)):\n",
        "            if array[i] == index_to_find:\n",
        "                indeces.append(i)\n",
        "        return (indeces)\n",
        "\n",
        "    # no more agent_handles\n",
        "    def get_agent_handles(self):\n",
        "        return range(self.get_num_agents())\n",
        "    \n",
        "    def get_num_agents(self) -> int:\n",
        "        return len(self.agents)\n",
        "\n",
        "    def add_agent(self, agent):\n",
        "        \"\"\" Add static info for a single agent.\n",
        "            Returns the index of the new agent.\n",
        "        \"\"\"\n",
        "        self.agents.append(agent)\n",
        "        return len(self.agents) - 1\n",
        "\n",
        "    def reset_agents(self):\n",
        "        \"\"\" Reset the agents to their starting positions\n",
        "        \"\"\"\n",
        "        for agent in self.agents:\n",
        "            agent.reset()\n",
        "        self.active_agents = [i for i in range(len(self.agents))]\n",
        "\n",
        "    def action_required(self, agent):\n",
        "        \"\"\"\n",
        "        Check if an agent needs to provide an action\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        agent: RailEnvAgent\n",
        "        Agent we want to check\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        True: Agent needs to provide an action\n",
        "        False: Agent cannot provide an action\n",
        "        \"\"\"\n",
        "        return agent.state == TrainState.READY_TO_DEPART or \\\n",
        "               ( agent.state.is_on_map_state() and agent.speed_counter.is_cell_entry )\n",
        "\n",
        "    def reset(self, regenerate_rail: bool = True, regenerate_schedule: bool = True, *,\n",
        "              random_seed: int = None) -> Tuple[Dict, Dict]:\n",
        "        \"\"\"\n",
        "        reset(regenerate_rail, regenerate_schedule, activate_agents, random_seed)\n",
        "\n",
        "        The method resets the rail environment\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        regenerate_rail : bool, optional\n",
        "            regenerate the rails\n",
        "        regenerate_schedule : bool, optional\n",
        "            regenerate the schedule and the static agents\n",
        "        random_seed : int, optional\n",
        "            random seed for environment\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        observation_dict: Dict\n",
        "            Dictionary with an observation for each agent\n",
        "        info_dict: Dict with agent specific information\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if random_seed:\n",
        "            self._seed(random_seed)\n",
        "\n",
        "        optionals = {}\n",
        "        if regenerate_rail or self.rail is None:\n",
        "\n",
        "            if \"__call__\" in dir(self.rail_generator):\n",
        "                rail, optionals = self.rail_generator(\n",
        "                    self.width, self.height, self.number_of_agents, self.num_resets, self.np_random)\n",
        "            elif \"generate\" in dir(self.rail_generator):\n",
        "                rail, optionals = self.rail_generator.generate(\n",
        "                    self.width, self.height, self.number_of_agents, self.num_resets, self.np_random)\n",
        "            else:\n",
        "                raise ValueError(\"Could not invoke __call__ or generate on rail_generator\")\n",
        "\n",
        "            self.rail = rail\n",
        "            self.height, self.width = self.rail.grid.shape\n",
        "\n",
        "            # Do a new set_env call on the obs_builder to ensure\n",
        "            # that obs_builder specific instantiations are made according to the\n",
        "            # specifications of the current environment : like width, height, etc\n",
        "            self.obs_builder.set_env(self)\n",
        "\n",
        "        if optionals and 'distance_map' in optionals:\n",
        "            self.distance_map.set(optionals['distance_map'])\n",
        "\n",
        "        if regenerate_schedule or regenerate_rail or self.get_num_agents() == 0:\n",
        "            agents_hints = None\n",
        "            if optionals and 'agents_hints' in optionals:\n",
        "                agents_hints = optionals['agents_hints']\n",
        "\n",
        "            line = self.line_generator(self.rail, self.number_of_agents, agents_hints, \n",
        "                                               self.num_resets, self.np_random)\n",
        "            self.agents = EnvAgent.from_line(line)\n",
        "\n",
        "            # Reset distance map - basically initializing\n",
        "            self.distance_map.reset(self.agents, self.rail)\n",
        "\n",
        "            # NEW : Time Schedule Generation\n",
        "            timetable = timetable_generator(timetable_example, self.agents, self.distance_map, \n",
        "                                               agents_hints, self.np_random)\n",
        "\n",
        "            #self._max_episode_steps = 250\n",
        "\n",
        "            for agent_i, agent in enumerate(self.agents):\n",
        "                agent.earliest_departure = timetable.earliest_departures[agent_i]         \n",
        "                agent.latest_arrival = timetable.latest_arrivals[agent_i]\n",
        "        else:\n",
        "            self.distance_map.reset(self.agents, self.rail)\n",
        "        \n",
        "        # Reset agents to initial states\n",
        "        self.reset_agents()\n",
        "\n",
        "        self.run_once = [0]*(self.number_of_agents)   # Flag to check when a train has started\n",
        "\n",
        "        self.num_resets += 1\n",
        "        self._elapsed_steps = 0\n",
        "        \n",
        "        self.previous_station = [[(-1,0)]] * self.number_of_agents\n",
        "        \n",
        "        self.dones_for_position = [False] * self.number_of_agents\n",
        "        \n",
        "\n",
        "        # Agent positions map\n",
        "        self.agent_positions = np.zeros((self.height, self.width), dtype=int) - 1\n",
        "        self._update_agent_positions_map(ignore_old_positions=False)\n",
        "\n",
        "        self.dones = dict.fromkeys(list(range(self.get_num_agents())) + [\"__all__\"], False)\n",
        "\n",
        "        # Reset the state of the observation builder with the new environment\n",
        "        self.obs_builder.reset()\n",
        "\n",
        "        # Empty the episode store of agent positions\n",
        "        self.cur_episode = []\n",
        "\n",
        "        info_dict = self.get_info_dict()\n",
        "        # Return the new observation vectors for each agent\n",
        "        observation_dict: Dict = self._get_observations()\n",
        "        if hasattr(self, \"renderer\") and self.renderer is not None:\n",
        "            self.renderer = None\n",
        "        return observation_dict, info_dict\n",
        "\n",
        "\n",
        "    def _update_agent_positions_map(self, ignore_old_positions=True):\n",
        "        \"\"\" Update the agent_positions array for agents that changed positions \"\"\"\n",
        "        for agent in self.agents:\n",
        "            if not ignore_old_positions or agent.old_position != agent.position:\n",
        "                if agent.position is not None:\n",
        "                    self.agent_positions[agent.position] = agent.handle\n",
        "                if agent.old_position is not None:\n",
        "                    self.agent_positions[agent.old_position] = -1\n",
        "    \n",
        "    def generate_state_transition_signals(self, agent, preprocessed_action, movement_allowed, target_time):\n",
        "        \"\"\" Generate State Transitions Signals used in the state machine \"\"\"\n",
        "        st_signals = StateTransitionSignals()\n",
        "        \n",
        "        # Malfunction starts when in_malfunction is set to true\n",
        "        st_signals.in_malfunction = agent.malfunction_handler.in_malfunction\n",
        "\n",
        "        # Malfunction counter complete - Malfunction ends next timestep\n",
        "        st_signals.malfunction_counter_complete = agent.malfunction_handler.malfunction_counter_complete\n",
        "\n",
        "        # Earliest departure reached - Train is allowed to move now\n",
        "        st_signals.earliest_departure_reached = self._elapsed_steps >= agent.earliest_departure\n",
        "\n",
        "        # Stop Action Given\n",
        "        st_signals.stop_action_given = (preprocessed_action == RailEnvActions.STOP_MOVING)\n",
        "\n",
        "        # Valid Movement action Given\n",
        "        st_signals.valid_movement_action_given = preprocessed_action.is_moving_action() and movement_allowed\n",
        "\n",
        "        # Target Reached\n",
        "        if self._elapsed_steps >= target_time:\n",
        "            # agent.target = [agent.target[0] - 1, agent.target[1]]\n",
        "            st_signals.target_reached = env_utils.fast_position_equal(agent.position, agent.target)\n",
        "        else:\n",
        "            st_signals.target_reached = False\n",
        "\n",
        "        # Movement conflict - Multiple trains trying to move into same cell\n",
        "        # If speed counter is not in cell exit, the train can enter the cell\n",
        "        st_signals.movement_conflict = (not movement_allowed) and agent.speed_counter.is_cell_exit\n",
        "\n",
        "        return st_signals\n",
        "\n",
        "    def _handle_end_reward(self, agent: EnvAgent, timetable) -> int:\n",
        "        '''\n",
        "        Handles end-of-episode reward for a particular agent.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        agent : EnvAgent\n",
        "        '''\n",
        "        i_agent = agent.handle\n",
        "\n",
        "        if training == 'training0' and i_agent != 0:\n",
        "            reward = 0\n",
        "            return reward\n",
        "        if (training == 'training1' or training == 'training1.1') and i_agent > 1:\n",
        "            reward = 0\n",
        "            return reward\n",
        "\n",
        "        reward = 0\n",
        "\n",
        "        # Reached intermediated stations?\n",
        "        #reward = self.intermediate_station_reward(i_agent, timetable)\n",
        "\n",
        "        # agent done? (arrival_time is not None)\n",
        "        if agent.state == TrainState.DONE:\n",
        "            # if agent arrived earlier or on time = 0\n",
        "            # if agent arrived later = -ve reward based on how late\n",
        "            reward += target_reward\n",
        "            self.dones[i_agent] = True\n",
        "            # DELAY\n",
        "            delay = min(agent.latest_arrival - agent.arrival_time, 0)\n",
        "            delay_penalty = delay * 0.7 * 0.7/3 # FORMULA DATA DAL PROF DA AGGIUSTARE\n",
        "            reward += delay_penalty\n",
        "            #reward = min(agent.latest_arrival - agent.arrival_time, 0)\n",
        "\n",
        "        # Agents not done (arrival_time is None)\n",
        "        else:\n",
        "            # CANCELLED check (never departed)\n",
        "            if (agent.state.is_off_map_state()):\n",
        "                reward += -1 * self.cancellation_factor * \\\n",
        "                    (agent.get_travel_time_on_shortest_path(self.distance_map) + self.cancellation_time_buffer)\n",
        "\n",
        "            # Departed but never reached\n",
        "            if (agent.state.is_on_map_state()) and not agent.state == TrainState.MALFUNCTION:\n",
        "                reward += target_not_reached_penalty\n",
        "                \n",
        "                pasted_agent_positions = self.cur_episode\n",
        "                if pasted_agent_positions == []:\n",
        "                    reward += default_skip_penalty\n",
        "                    return reward\n",
        "                \n",
        "                stations_to_pass = timetable[i_agent][0]\n",
        "                \n",
        "                for positions in range (len(pasted_agent_positions)):\n",
        "                    # If the agent is passed in at least one station (different from the starting one) no problems\n",
        "                    if pasted_agent_positions[positions][i_agent] in stations_to_pass[1:]:\n",
        "                        return reward\n",
        "                    \n",
        "                # else, I have to give a skip penalty for all the skipped stations, for now simplified\n",
        "                # I give an high penalty\n",
        "                reward += - default_skip_penalty\n",
        "                return reward\n",
        "                            \n",
        "                              \n",
        "                # DELAY\n",
        "                #reward = agent.get_current_delay(self._elapsed_steps, self.distance_map)\n",
        "        \n",
        "        return reward\n",
        "\n",
        "    def preprocess_action(self, action, agent):\n",
        "        \"\"\"\n",
        "        Preprocess the provided action\n",
        "            * Change to DO_NOTHING if illegal action\n",
        "            * Block all actions when in waiting state\n",
        "            * Check MOVE_LEFT/MOVE_RIGHT actions on current position else try MOVE_FORWARD\n",
        "        \"\"\"\n",
        "        action = action_preprocessing.preprocess_raw_action(action, agent.state, agent.action_saver.saved_action)\n",
        "        action = action_preprocessing.preprocess_action_when_waiting(action, agent.state)\n",
        "\n",
        "        # Try moving actions on current position\n",
        "        current_position, current_direction = agent.position, agent.direction\n",
        "        if current_position is None: # Agent not added on map yet\n",
        "            current_position, current_direction = agent.initial_position, agent.initial_direction\n",
        "        \n",
        "        action = action_preprocessing.preprocess_moving_action(action, self.rail, current_position, current_direction)\n",
        "\n",
        "        # Check transitions, bounts for executing the action in the given position and directon\n",
        "        if action.is_moving_action() and not check_valid_action(action, self.rail, current_position, current_direction):\n",
        "            action = RailEnvActions.STOP_MOVING\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def clear_rewards_dict(self):\n",
        "        \"\"\" Reset the rewards dictionary \"\"\"\n",
        "        self.rewards_dict = {i_agent: 0 for i_agent in range(len(self.agents))}\n",
        "\n",
        "    def get_info_dict(self):\n",
        "        \"\"\" \n",
        "        Returns dictionary of infos for all agents \n",
        "        dict_keys : action_required - \n",
        "                    malfunction - Counter value for malfunction > 0 means train is in malfunction\n",
        "                    speed - Speed of the train\n",
        "                    state - State from the trains's state machine\n",
        "        \"\"\"\n",
        "        info_dict = {\n",
        "            'action_required': {i: self.action_required(agent) for i, agent in enumerate(self.agents)},\n",
        "            'malfunction': {\n",
        "                i: agent.malfunction_handler.malfunction_down_counter for i, agent in enumerate(self.agents)\n",
        "            },\n",
        "            'speed': {i: agent.speed_counter.speed for i, agent in enumerate(self.agents)},\n",
        "            'state': {i: agent.state for i, agent in enumerate(self.agents)}\n",
        "        }\n",
        "        return info_dict\n",
        "    \n",
        "    def update_step_rewards(self, i_agent):\n",
        "        \"\"\"\n",
        "        Update the rewards dict for agent id i_agent for every timestep\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        action = self.agents[i_agent].action_saver.saved_action\n",
        "        moving = self.agents[i_agent].moving\n",
        "        state = self.agents[i_agent].state\"\"\"\n",
        "\n",
        "        reward = 0\n",
        "\n",
        "        reward = step_penality\n",
        "        \"\"\"\n",
        "        if action == RailEnvActions.REVERSE:\n",
        "            reward += reverse_penality\n",
        "        if not moving or state == TrainState.STOPPED:\n",
        "            reward += stop_penality\"\"\"\n",
        "\n",
        "        self.rewards_dict[i_agent] += reward\n",
        "        \n",
        "    def calculate_train_run(self, timetable, i_agent, specific_station_index):\n",
        "        \"\"\"[Function to calculate a specific train run for a specific agent]\n",
        "\n",
        "        Args:\n",
        "            timetable ([list]): [Timetable]\n",
        "            i_agent ([int]): [i_agent]\n",
        "            specific_station_index ([int]): [Index of the station I want to calculate the train run in which is]\n",
        "        \"\"\"\n",
        "        all_train_run = timetable[i_agent][0]\n",
        "        all_times = timetable[i_agent][1]\n",
        "\n",
        "        previous_station = all_train_run[specific_station_index]\n",
        "        \n",
        "        specific_train_run_stations = []\n",
        "        specific_train_run_times = []\n",
        "        specific_train_run = []\n",
        "        train_run_initial_index = 0\n",
        "        \n",
        "        # Starting finding the first station\n",
        "        if specific_station_index != 0:\n",
        "            for station_reversed in range(specific_station_index + 1):                    \n",
        "                if all_train_run[specific_station_index - station_reversed - 1] == previous_station:\n",
        "                    initial_train_run_station = previous_station\n",
        "                    if station_reversed == 0:\n",
        "                        train_run_initial_index = specific_station_index\n",
        "                        initial_time = all_times[train_run_initial_index]\n",
        "                    else:\n",
        "                        train_run_initial_index = specific_station_index - station_reversed\n",
        "                        initial_time = all_times[train_run_initial_index]\n",
        "                    break\n",
        "                elif station_reversed == specific_station_index:\n",
        "                    initial_train_run_station = previous_station\n",
        "                    train_run_initial_index = specific_station_index - station_reversed\n",
        "                    initial_time = all_times[train_run_initial_index]\n",
        "                    break\n",
        "                else:\n",
        "                    previous_station = all_train_run[specific_station_index - station_reversed - 1] \n",
        "        # If I'm the first station...\n",
        "        else:\n",
        "            initial_train_run_station = previous_station\n",
        "            initial_time = all_times[0]\n",
        "        \n",
        "        previous_station = initial_train_run_station\n",
        "        previous_time = initial_time   \n",
        "        \n",
        "        for stations in range(1 , len(all_train_run)):\n",
        "            if previous_station == all_train_run[train_run_initial_index + stations]:\n",
        "                ending_train_run_station = previous_station\n",
        "                specific_train_run_stations.append(ending_train_run_station)\n",
        "                specific_train_run_times.append(all_times[train_run_initial_index + stations])\n",
        "                break\n",
        "            elif train_run_initial_index + stations == len(all_train_run):\n",
        "                ending_train_run_station = previous_station\n",
        "                specific_train_run_stations.append(ending_train_run_station)\n",
        "                specific_train_run_times.append(all_times[train_run_initial_index + stations])\n",
        "                break\n",
        "            else:\n",
        "                specific_train_run_stations.append(previous_station)\n",
        "                specific_train_run_times.append(previous_time)\n",
        "                previous_station = all_train_run[train_run_initial_index + stations]\n",
        "                previous_time = all_times[train_run_initial_index + stations]\n",
        "                if train_run_initial_index + stations == len(all_train_run) - 1:\n",
        "                    specific_train_run_stations.append(previous_station)\n",
        "                    specific_train_run_times.append(all_times[train_run_initial_index + stations])\n",
        "                    break\n",
        "        specific_train_run.append(specific_train_run_stations)\n",
        "        specific_train_run.append(specific_train_run_times)\n",
        "                \n",
        "        return specific_train_run\n",
        "    \n",
        "    def calculate_skip_penalty(self, timetable, index_of_station_skipped, station_skipped, time_scheduled,\n",
        "                               i_agent, station, train_type, specific_train_run):\n",
        "        \n",
        "        train_importance = train_type\n",
        "        station_importance = 0.7\n",
        "        #station_importance = station.importance   # TODO modificare !!!!\n",
        "        number_of_station_to_pass = len(specific_train_run)\n",
        "        \n",
        "        # array that have to contein all the possible passages from the skipped station for other agents (convoys)\n",
        "        possible_train_passage = []\n",
        "        \n",
        "        for i in range(len(timetable)):\n",
        "            if i != i_agent:\n",
        "                num_of_stations = len(timetable[i_agent][0])\n",
        "                for stations in range(num_of_stations - 1):\n",
        "                    if timetable[i][0][stations] == station_skipped:        # same station\n",
        "                        if timetable[i][1][stations] > time_scheduled:      # greater time, so the successive agent\n",
        "                            if timetable[i][0][stations + 1] == timetable[i_agent][0][index_of_station_skipped + 1]:   # same direction\n",
        "                                possible_train_passage.append(timetable[i][1][stations] - time_scheduled)\n",
        "        \n",
        "        if possible_train_passage == []:\n",
        "            penalty = - default_skip_penalty\n",
        "            return penalty\n",
        "            \n",
        "        else:\n",
        "            for i in range(len(possible_train_passage)):\n",
        "                delay = min(possible_train_passage)\n",
        "                \n",
        "            \n",
        "        penalty = - (delay*train_importance*station_importance)/number_of_station_to_pass\n",
        "        \n",
        "        return penalty\n",
        "    \n",
        "    def calculate_delay_penalty(self, delay, train_run, station, train_type):\n",
        "        \"\"\"[Calculate the penalty of the agent based on the delay, and weighted on the type of train, on the importance of the station and \n",
        "        on the number of stations reached by the train]\n",
        "\n",
        "        Args:\n",
        "            delay ([int]): [delay of the train in a specific station]\n",
        "            train_run ([list]): [specific train run i'm doing with my train]\n",
        "            station ([Station]): [station reached]\n",
        "            train_type ([convoy.type]): [type of the convoy (regional, intercity, high velocity)]\n",
        "        \"\"\"\n",
        "        number_of_station_to_pass = len(train_run) + 1     \n",
        "        \n",
        "        train_importance = train_type\n",
        "        #station_importance = station.importance\n",
        "        station_importance = 0.7  # TODO modificalo !!!!!\n",
        "        \n",
        "        penalty = - (delay*train_importance*station_importance)/number_of_station_to_pass\n",
        "        \n",
        "        return penalty\n",
        "\n",
        "    def end_of_episode_update(self, have_all_agents_ended, timetable):\n",
        "        \"\"\" \n",
        "        Updates made when episode ends\n",
        "        Parameters: have_all_agents_ended - Indicates if all agents have reached done state\n",
        "        \"\"\"\n",
        "        if have_all_agents_ended or \\\n",
        "           ( (self._max_episode_steps is not None) and (self._elapsed_steps >= self._max_episode_steps)):\n",
        "\n",
        "            for i_agent, agent in enumerate(self.agents):\n",
        "                \n",
        "                reward = self._handle_end_reward(agent, timetable)\n",
        "                self.rewards_dict[i_agent] += reward\n",
        "                \n",
        "                #self.dones[i_agent] = True\n",
        "\n",
        "            #self.dones[\"__all__\"] = True\n",
        "\n",
        "    def handle_done_state(self, agent):\n",
        "        \"\"\" Any updates to agent to be made in Done state \"\"\"\n",
        "        if agent.state == TrainState.DONE and agent.arrival_time is None:\n",
        "            agent.arrival_time = self._elapsed_steps\n",
        "            if self.remove_agents_at_target:\n",
        "                agent.position = None\n",
        "\n",
        "    def check_intermediate_station_passage(self, step, i_agent, timetable):\n",
        "            from operator import itemgetter\n",
        "            positions = self.cur_episode\n",
        "            if positions == []:\n",
        "                return\n",
        "            reward = 0\n",
        "            stations_to_pass = timetable[i_agent][0]\n",
        "\n",
        "            if self.agents[i_agent].position in stations_to_pass and self.agents[i_agent].position not in self.previous_station[i_agent]:\n",
        "\n",
        "                if self.agents[i_agent].state == TrainState.DONE:\n",
        "                    self.previous_station[i_agent].append(positions[-1][i_agent])  # Final position reached by the agent\n",
        "                else:\n",
        "                    self.previous_station[i_agent].append(positions[step - 1][i_agent])\n",
        "                \n",
        "                reward += station_passage_reward\n",
        "\n",
        "                station_in_which_i_am = self.agents[i_agent].position\n",
        "\n",
        "                index = self.find_indices(timetable[i_agent][0], station_in_which_i_am)\n",
        "                difference = []\n",
        "                for num_station in range(len(index)):\n",
        "                    difference.append(step - timetable[i_agent][1][index[num_station]])\n",
        "\n",
        "                index_of_min, value_of_min = min(enumerate(difference), key=itemgetter(1))\n",
        "                index_of_my_station = index[index_of_min]\n",
        "                \n",
        "                station = timetable[i_agent][0][index_of_my_station]\n",
        "                train_type = timetable[i_agent][2]\n",
        "                \n",
        "                specific_train_run = self.calculate_train_run(timetable, i_agent, index_of_my_station)\n",
        "                \n",
        "                index_of_my_station_for_my_train_run = specific_train_run[0].index(station)\n",
        "                \n",
        "                reward += self.calculate_delay_penalty(value_of_min, specific_train_run, station, train_type)\n",
        "                # Check if I have skipped a previous station\n",
        "                if index_of_my_station > 1:\n",
        "                    previous_stations = specific_train_run[0][0:index_of_my_station_for_my_train_run]\n",
        "                    flag_station_passed = [False] * (len(previous_stations) - 1)\n",
        "                    for past_positions in range(len(positions)):\n",
        "                        for i_previous_station in range(1, index_of_my_station):\n",
        "                            if positions[past_positions][i_agent] == timetable[i_agent][0][i_previous_station]:\n",
        "                                self.rewards_dict[i_agent] += 0 \n",
        "                                flag_station_passed[i_previous_station - 1] = True\n",
        "                    \n",
        "                    for i_previous_station in range(len(flag_station_passed)):\n",
        "                        if not flag_station_passed[i_previous_station]: \n",
        "                            station_skipped = previous_stations[0][i_previous_station]\n",
        "                            time_scheduled_for_station = previous_stations[1][i_previous_station] \n",
        "                            reward += self.calculate_skip_penalty(timetable, index_of_my_station, station_skipped, \n",
        "                                                             time_scheduled_for_station, i_agent, station, train_type, specific_train_run)\n",
        "                    self.rewards_dict[i_agent] += reward\n",
        "                    return\n",
        "                \n",
        "                self.rewards_dict[i_agent] += reward\n",
        "\n",
        "\n",
        "    def step(self, action_dict_: Dict[int, RailEnvActions]):\n",
        "        \"\"\"\n",
        "        Updates rewards for the agents at a step.\n",
        "        \"\"\"\n",
        "        self._elapsed_steps += 1\n",
        "\n",
        "        # Not allowed to step further once done\n",
        "        if self.dones[\"__all__\"]:\n",
        "            raise Exception(\"Episode is done, cannot call step()\")\n",
        "\n",
        "        self.clear_rewards_dict()\n",
        "\n",
        "        have_all_agents_ended = True # Boolean flag to check if all agents are done\n",
        "\n",
        "        self.motionCheck = ac.MotionCheck()  # reset the motion check\n",
        "\n",
        "        temp_transition_data = {}\n",
        "        \n",
        "        for agent in self.agents:\n",
        "            i_agent = agent.handle\n",
        "\n",
        "            # Build info dict\n",
        "            rail, optionals = self.rail_generator(\n",
        "                self.width, self.height, self.number_of_agents, self.num_resets, self.np_random)\n",
        "\n",
        "            agent = self.agents[i_agent]\n",
        "\n",
        "            # Calculate velocities that the agents have to mantein\n",
        "            velocities = self.check_speed(optionals['agents_hints'])   # TODO variare velocità in base alla stazione da raggiungere     \n",
        "\n",
        "            agent.speed_counter.speed = velocities[i_agent]\n",
        "\n",
        "            # Starting time of the agent\n",
        "            starting_time = optionals['agents_hints']['timetable'][i_agent][1][0]\n",
        "            ending_time = optionals['agents_hints']['timetable'][i_agent][1][-1]\n",
        "\n",
        "            agent.earliest_departure = starting_time\n",
        "            agent.latest_arrival = ending_time\n",
        "\n",
        "            agent.old_position = agent.position\n",
        "            agent.old_direction = agent.direction\n",
        "            # Generate malfunction\n",
        "            agent.malfunction_handler.generate_malfunction(self.malfunction_generator, self.np_random)\n",
        "\n",
        "            # Get action for the agent\n",
        "            action = action_dict_.get(i_agent, RailEnvActions.DO_NOTHING)\n",
        "\n",
        "            preprocessed_action = self.preprocess_action(action, agent)\n",
        "\n",
        "            # Save moving actions in not already saved\n",
        "            agent.action_saver.save_action_if_allowed(preprocessed_action, agent.state)\n",
        "\n",
        "            # Train's next position can change if current stopped in a fractional speed or train is at cell's exit\n",
        "\n",
        "            position_update_allowed = agent.speed_counter.is_cell_exit and \\\n",
        "                        not agent.malfunction_handler.malfunction_down_counter > 0 and \\\n",
        "                        not preprocessed_action == RailEnvActions.STOP_MOVING                            \n",
        "\n",
        "            #position_update_allowed = (agent.speed_counter.is_cell_exit or agent.state == TrainState.STOPPED)\n",
        "\n",
        "            # Calculate new position\n",
        "            # Keep agent in same place if already done\n",
        "            if agent.state == TrainState.DONE:\n",
        "                new_position, new_direction = agent.position, agent.direction\n",
        "            elif agent.state == TrainState.MALFUNCTION:\n",
        "                new_position, new_direction = agent.position, agent.direction\n",
        "            # Add agent to the map if not on it yet\n",
        "            elif agent.position is None and agent.action_saver.is_action_saved:\n",
        "                new_position = agent.initial_position\n",
        "                new_direction = agent.initial_direction       \n",
        "            # If movement is allowed apply saved action independent of other agents\n",
        "            elif agent.action_saver.is_action_saved and position_update_allowed:\n",
        "                saved_action = agent.action_saver.saved_action\n",
        "                # Apply action independent of other agents and get temporary new position and direction\n",
        "                new_position, new_direction  = env_utils.apply_action_independent(saved_action, \n",
        "                                                                             self.rail, \n",
        "                                                                             agent.position, \n",
        "                                                                             agent.direction)\n",
        "                preprocessed_action = saved_action\n",
        "            else:\n",
        "                new_position, new_direction = agent.position, agent.direction\n",
        "\n",
        "            temp_transition_data[i_agent] = env_utils.AgentTransitionData(position=new_position,\n",
        "                                                                direction=new_direction,\n",
        "                                                                preprocessed_action=preprocessed_action)\n",
        "            \n",
        "            # This is for storing and later checking for conflicts of agents trying to occupy same cell                                                    \n",
        "            self.motionCheck.addAgent(i_agent, agent.position, new_position)\n",
        "\n",
        "        # Find conflicts between trains trying to occupy same cell  TODO controlla i bug\n",
        "        self.motionCheck.find_conflicts()\n",
        "        \n",
        "        for agent in self.agents:\n",
        "            i_agent = agent.handle\n",
        "\n",
        "            ## Update positions\n",
        "            if agent.malfunction_handler.in_malfunction:\n",
        "                movement_allowed = False\n",
        "            else:\n",
        "                # TODO check how the check motion is gestito, fai si che una reverse action sia sempre \n",
        "                # possibile ma attenzione quando c'è un treno vicino\n",
        "                movement_allowed = self.motionCheck.check_motion(i_agent, agent.position) \n",
        "\n",
        "\n",
        "            movement_inside_cell = agent.state == TrainState.STOPPED and not agent.speed_counter.is_cell_exit\n",
        "            movement_allowed = movement_allowed or movement_inside_cell\n",
        "\n",
        "            # Fetch the saved transition data\n",
        "            agent_transition_data = temp_transition_data[i_agent]\n",
        "            preprocessed_action = agent_transition_data.preprocessed_action\n",
        "\n",
        "            ## Update states\n",
        "            state_transition_signals = self.generate_state_transition_signals(agent, preprocessed_action, movement_allowed, ending_time)\n",
        "            agent.state_machine.set_transition_signals(state_transition_signals)\n",
        "            agent.state_machine.step()\n",
        "\n",
        "            # Needed when not removing agents at target\n",
        "            movement_allowed = movement_allowed and agent.state != TrainState.DONE\n",
        "\n",
        "            # Agent is being added to map\n",
        "            if agent.state.is_on_map_state():\n",
        "                if agent.state_machine.previous_state.is_off_map_state():\n",
        "                    agent.position = agent.initial_position\n",
        "                    agent.direction = agent.initial_direction\n",
        "            # Speed counter completes\n",
        "                elif movement_allowed and (agent.speed_counter.is_cell_exit):\n",
        "                    agent.position = agent_transition_data.position\n",
        "                    agent.direction = agent_transition_data.direction\n",
        "                    agent.state_machine.update_if_reached(agent.position, agent.target)\n",
        "\n",
        "            # Off map or on map state and position should match\n",
        "            env_utils.state_position_sync_check(agent.state, agent.position, agent.handle)\n",
        "                \n",
        "        self._update_agent_positions_map()\n",
        "        if self.record_steps:\n",
        "            self.record_timestep(action_dict_)\n",
        "            \n",
        "        for agent in self.agents:\n",
        "            \n",
        "            i_agent = agent.handle\n",
        "\n",
        "            ## Update rewards \n",
        "            if agent.state.is_on_map_state():\n",
        "                if agent.state != TrainState.MALFUNCTION:                           \n",
        "                    self.update_step_rewards(i_agent)\n",
        "\n",
        "            # The if condition is important to avoid multiple penalties due to malfunctions occurred in stations\n",
        "            if agent.state.is_on_map_state() or agent.state == TrainState.DONE:\n",
        "                if agent.state != TrainState.MALFUNCTION: \n",
        "                    self.check_intermediate_station_passage(self._elapsed_steps, i_agent, optionals['agents_hints']['timetable'])\n",
        "                \n",
        "            # Handle done state actions, optionally remove agents\n",
        "            self.handle_done_state(agent)\n",
        "            \n",
        "            if training == 'training0':\n",
        "                if i_agent == 0:\n",
        "                    have_all_agents_ended &= (agent.state == TrainState.DONE)\n",
        "\n",
        "            elif training == 'training1' or training == 'training1.1':\n",
        "                if i_agent < 2:\n",
        "                    have_all_agents_ended &= (agent.state == TrainState.DONE)\n",
        "\n",
        "            else:\n",
        "                have_all_agents_ended &= (agent.state == TrainState.DONE)\n",
        "\n",
        "            ## Update counters (malfunction and speed)\n",
        "            agent.speed_counter.update_counter(agent.state, agent.old_position)\n",
        "                                            #    agent.state_machine.previous_state)\n",
        "            agent.malfunction_handler.update_counter()\n",
        "\n",
        "            # Clear old action when starting in new cell\n",
        "            if agent.speed_counter.is_cell_entry and agent.position is not None:\n",
        "                agent.action_saver.clear_saved_action()\n",
        "        \n",
        "        # Check if episode has ended and update rewards and dones\n",
        "        self.end_of_episode_update(have_all_agents_ended, optionals['agents_hints']['timetable'])\n",
        "\n",
        "        return self._get_observations(), self.rewards_dict, self.dones, self.get_info_dict() \n",
        "    \n",
        "\n",
        "\n",
        "    '''def record_timestep(self, dActions):\n",
        "                    \"\"\" \n",
        "                    Record the positions and orientations of all agents in memory, in the cur_episode\n",
        "                    \"\"\"\n",
        "                    list_agents_state = []\n",
        "                    for i_agent in range(self.get_num_agents()):\n",
        "                        agent = self.agents[i_agent]\n",
        "                        # the int cast is to avoid numpy types which may cause problems with msgpack\n",
        "                        # in env v2, agents may have position None, before starting\n",
        "                        if agent.position is None:\n",
        "                            pos = (0, 0)\n",
        "                        else:\n",
        "                            pos = (int(agent.position[0]), int(agent.position[1]))\n",
        "                        # print(\"pos:\", pos, type(pos[0]))\n",
        "                        list_agents_state.append([\n",
        "                                *pos, int(agent.direction), \n",
        "                                agent.malfunction_handler.malfunction_down_counter,  \n",
        "                                int(agent.state),\n",
        "                                int(agent.position in self.motionCheck.svDeadlocked)\n",
        "                                ])\n",
        "            \n",
        "                    self.cur_episode.append(list_agents_state)\n",
        "                    self.list_actions.append(dActions)'''\n",
        "\n",
        "    def record_timestep(self, dActions):\n",
        "        \"\"\" \n",
        "        Record the positions and orientations of all agents in memory, in the cur_episode\n",
        "        \"\"\"\n",
        "        list_agents_state = []\n",
        "        for i_agent in range(self.get_num_agents()):\n",
        "            agent = self.agents[i_agent]\n",
        "            # the int cast is to avoid numpy types which may cause problems with msgpack\n",
        "            # in env v2, agents may have position None, before starting\n",
        "            if agent.state == TrainState.DONE and not self.dones_for_position[i_agent]:\n",
        "                pos = agent.target\n",
        "                self.dones_for_position[i_agent] = True\n",
        "                return\n",
        "                \n",
        "            elif agent.state.is_off_map_state():\n",
        "                pos = (-1, 0) \n",
        "                \n",
        "            elif agent.position == None: \n",
        "                pos = (-1, 0)\n",
        "            else:\n",
        "                pos = (int(agent.position[0]), int(agent.position[1]))\n",
        "                \n",
        "            # print(\"pos:\", pos, type(pos[0]))\n",
        "            list_agents_state.append(pos)\n",
        "\n",
        "        self.cur_episode.append(list_agents_state)\n",
        "        self.list_actions.append(dActions)\n",
        "\n",
        "    def _get_observations(self):\n",
        "        \"\"\"\n",
        "        Utility which returns the dictionary of observations for an agent with respect to environment\n",
        "        \"\"\"\n",
        "        # print(f\"_get_obs - num agents: {self.get_num_agents()} {list(range(self.get_num_agents()))}\")\n",
        "        self.obs_dict = self.obs_builder.get_many(list(range(self.get_num_agents())))\n",
        "        return self.obs_dict\n",
        "\n",
        "    def get_valid_directions_on_grid(self, row: int, col: int) -> List[int]:\n",
        "        \"\"\"\n",
        "        Returns directions in which the agent can move\n",
        "        \"\"\"\n",
        "        return Grid4Transitions.get_entry_directions(self.rail.get_full_transitions(row, col))\n",
        "\n",
        "    def _exp_distirbution_synced(self, rate: float) -> float:\n",
        "        \"\"\"\n",
        "        Generates sample from exponential distribution\n",
        "        We need this to guarantee synchronity between different instances with same seed.\n",
        "        :param rate:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        u = self.np_random.rand()\n",
        "        x = - np.log(1 - u) * rate\n",
        "        return x\n",
        "\n",
        "    def _is_agent_ok(self, agent: EnvAgent) -> bool:\n",
        "        \"\"\"\n",
        "        Check if an agent is ok, meaning it can move and is not malfuncitoinig\n",
        "        Parameters\n",
        "        ----------\n",
        "        agent\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        True if agent is ok, False otherwise\n",
        "\n",
        "        \"\"\"\n",
        "        return agent.malfunction_handler.in_malfunction\n",
        "        \n",
        "\n",
        "    def save(self, filename):\n",
        "        print(\"DEPRECATED call to env.save() - pls call RailEnvPersister.save()\")\n",
        "        persistence.RailEnvPersister.save(self, filename)\n",
        "\n",
        "    def render(self, mode=\"rgb_array\", gl=\"PGL\", agent_render_variant=AgentRenderVariant.ONE_STEP_BEHIND,\n",
        "            show_debug=False, clear_debug_text=True, show=False,\n",
        "            screen_height=600, screen_width=800,\n",
        "            show_observations=False, show_predictions=False,\n",
        "            show_rowcols=False, return_image=True):\n",
        "        \"\"\"\n",
        "        This methods provides the option to render the\n",
        "        environment's behavior as an image or to a window.\n",
        "        Parameters\n",
        "        ----------\n",
        "        mode\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Image if mode is rgb_array, opens a window otherwise\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"renderer\") or self.renderer is None:\n",
        "            self.initialize_renderer(mode=mode, gl=gl,  # gl=\"TKPILSVG\",\n",
        "                                    agent_render_variant=agent_render_variant,\n",
        "                                    show_debug=show_debug,\n",
        "                                    clear_debug_text=clear_debug_text,\n",
        "                                    show=show,\n",
        "                                    screen_height=screen_height,  # Adjust these parameters to fit your resolution\n",
        "                                    screen_width=screen_width)\n",
        "        return self.update_renderer(mode=mode, show=show, show_observations=show_observations,\n",
        "                                    show_predictions=show_predictions,\n",
        "                                    show_rowcols=show_rowcols, return_image=return_image)\n",
        "\n",
        "    def initialize_renderer(self, mode, gl,\n",
        "                agent_render_variant,\n",
        "                show_debug,\n",
        "                clear_debug_text,\n",
        "                show,\n",
        "                screen_height,\n",
        "                screen_width):\n",
        "        # Initiate the renderer\n",
        "        self.renderer = RenderTool(self, gl=gl,  # gl=\"TKPILSVG\",\n",
        "                                agent_render_variant=agent_render_variant,\n",
        "                                show_debug=show_debug,\n",
        "                                clear_debug_text=clear_debug_text,\n",
        "                                screen_height=screen_height,  # Adjust these parameters to fit your resolution\n",
        "                                screen_width=screen_width)  # Adjust these parameters to fit your resolution\n",
        "        self.renderer.show = show\n",
        "        self.renderer.reset()\n",
        "\n",
        "    def update_renderer(self, mode, show, show_observations, show_predictions,\n",
        "                    show_rowcols, return_image):\n",
        "        \"\"\"\n",
        "        This method updates the render.\n",
        "        Parameters\n",
        "        ----------\n",
        "        mode\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Image if mode is rgb_array, None otherwise\n",
        "        \"\"\"\n",
        "        image = self.renderer.render_env(show=show, show_observations=show_observations,\n",
        "                                show_predictions=show_predictions,\n",
        "                                show_rowcols=show_rowcols, return_image=return_image)\n",
        "        if mode == 'rgb_array':\n",
        "            return image[:, :, :3]\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        This methods closes any renderer window.\n",
        "        \"\"\"\n",
        "        if hasattr(self, \"renderer\") and self.renderer is not None:\n",
        "            try:\n",
        "                if self.renderer.show:\n",
        "                    self.renderer.close_window()\n",
        "            except Exception as e:\n",
        "                print(\"Could Not close window due to:\",e)\n",
        "            self.renderer = None\n",
        "\n",
        "\n",
        "    def check_speed(self, agents_hints):\n",
        "\n",
        "    # Velocity depending on the train type and on the line (Take the minimum between the two possible velocities)\n",
        "        train_velocities = [0]*self.number_of_agents\n",
        "\n",
        "        # Check for all the agents\n",
        "        for i_agent, agent in enumerate(self.agents):\n",
        "\n",
        "            # the i_agent\n",
        "            agent = self.agents[i_agent]\n",
        "\n",
        "            # Check if the agent is in the environment or not\n",
        "            if agent.position != None:\n",
        "\n",
        "                # If the agent is in the line i the max velocity is x\n",
        "\n",
        "                # High velocity line case\n",
        "                if (agent.position in av_line):  \n",
        "                    train_velocities[i_agent] = min(1, agents_hints['timetable'][i_agent][2])                \n",
        "                # Regional line case\n",
        "                else:\n",
        "                    train_velocities[i_agent] = min(1/2, agents_hints['timetable'][i_agent][2])\n",
        "\n",
        "            # If agent is not in the environment deafault velocity is 1/2\n",
        "            else:\n",
        "                train_velocities[i_agent] = 1/2\n",
        "\n",
        "        return train_velocities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_SDC43AZXBw"
      },
      "source": [
        "# Simulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HpZdeKva-43"
      },
      "source": [
        "Timer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3yjH2BFlbAHQ"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer\n",
        "\n",
        "\n",
        "class Timer(object):\n",
        "    \"\"\"\n",
        "    Utility to measure times.\n",
        "\n",
        "    TODO:\n",
        "    - add \"lap\" method to make it easier to measure average time (+std) when measuring the same thing multiple times.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.total_time = 0.0\n",
        "        self.start_time = 0.0\n",
        "        self.end_time = 0.0\n",
        "\n",
        "    def start(self):\n",
        "        self.start_time = default_timer()\n",
        "\n",
        "    def end(self):\n",
        "        self.total_time += default_timer() - self.start_time\n",
        "\n",
        "    def get(self):\n",
        "        return self.total_time\n",
        "\n",
        "    def get_current(self):\n",
        "        return default_timer() - self.start_time\n",
        "\n",
        "    def reset(self):\n",
        "        self.__init__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.get()\n",
        "\n",
        "import numpy as np\n",
        "from flatland.envs.observations import TreeObsForRailEnv\n",
        "\n",
        "def max_lt(seq, val):\n",
        "    \"\"\"\n",
        "    Return greatest item in seq for which item < val applies.\n",
        "    None is returned if seq was empty or all items in seq were >= val.\n",
        "    \"\"\"\n",
        "    max = 0\n",
        "    idx = len(seq) - 1\n",
        "    while idx >= 0:\n",
        "        if seq[idx] < val and seq[idx] >= 0 and seq[idx] > max:\n",
        "            max = seq[idx]\n",
        "        idx -= 1\n",
        "    return max\n",
        "\n",
        "\n",
        "def min_gt(seq, val):\n",
        "    \"\"\"\n",
        "    Return smallest item in seq for which item > val applies.\n",
        "    None is returned if seq was empty or all items in seq were >= val.\n",
        "    \"\"\"\n",
        "    min = np.inf\n",
        "    idx = len(seq) - 1\n",
        "    while idx >= 0:\n",
        "        if seq[idx] >= val and seq[idx] < min:\n",
        "            min = seq[idx]\n",
        "        idx -= 1\n",
        "    return min\n",
        "\n",
        "\n",
        "def norm_obs_clip(obs, clip_min=-1, clip_max=1, fixed_radius=0, normalize_to_range=False):\n",
        "    \"\"\"\n",
        "    This function returns the difference between min and max value of an observation\n",
        "    :param obs: Observation that should be normalized\n",
        "    :param clip_min: min value where observation will be clipped\n",
        "    :param clip_max: max value where observation will be clipped\n",
        "    :return: returnes normalized and clipped observatoin\n",
        "    \"\"\"\n",
        "    if fixed_radius > 0:\n",
        "        max_obs = fixed_radius\n",
        "    else:\n",
        "        max_obs = max(1, max_lt(obs, 1000)) + 1\n",
        "\n",
        "    min_obs = 0  # min(max_obs, min_gt(obs, 0))\n",
        "    if normalize_to_range:\n",
        "        min_obs = min_gt(obs, 0)\n",
        "    if min_obs > max_obs:\n",
        "        min_obs = max_obs\n",
        "    if max_obs == min_obs:\n",
        "        return np.clip(np.array(obs) / max_obs, clip_min, clip_max)\n",
        "    norm = np.abs(max_obs - min_obs)\n",
        "    return np.clip((np.array(obs) - min_obs) / norm, clip_min, clip_max)\n",
        "\n",
        "\n",
        "def _split_node_into_feature_groups(node) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    data = np.zeros(6)\n",
        "    distance = np.zeros(1)\n",
        "    agent_data = np.zeros(4)\n",
        "\n",
        "    data[0] = node.dist_own_target_encountered\n",
        "    data[1] = node.dist_other_target_encountered\n",
        "    data[2] = node.dist_other_agent_encountered\n",
        "    data[3] = node.dist_potential_conflict\n",
        "    data[4] = node.dist_unusable_switch\n",
        "    data[5] = node.dist_to_next_branch\n",
        "\n",
        "    distance[0] = node.dist_min_to_target\n",
        "\n",
        "    agent_data[0] = node.num_agents_same_direction\n",
        "    agent_data[1] = node.num_agents_opposite_direction\n",
        "    agent_data[2] = node.num_agents_malfunctioning\n",
        "    agent_data[3] = node.speed_min_fractional\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def _split_subtree_into_feature_groups(node, current_tree_depth: int, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    if node == -np.inf:\n",
        "        remaining_depth = max_tree_depth - current_tree_depth\n",
        "        # reference: https://stackoverflow.com/questions/515214/total-number-of-nodes-in-a-tree-data-structure\n",
        "        num_remaining_nodes = int((4 ** (remaining_depth + 1) - 1) / (4 - 1))\n",
        "        return [-np.inf] * num_remaining_nodes * 6, [-np.inf] * num_remaining_nodes, [-np.inf] * num_remaining_nodes * 4\n",
        "\n",
        "    data, distance, agent_data = _split_node_into_feature_groups(node)\n",
        "\n",
        "    if not node.childs:\n",
        "        return data, distance, agent_data\n",
        "\n",
        "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
        "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(node.childs[direction], current_tree_depth + 1, max_tree_depth)\n",
        "        data = np.concatenate((data, sub_data))\n",
        "        distance = np.concatenate((distance, sub_distance))\n",
        "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def split_tree_into_feature_groups(tree, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    \"\"\"\n",
        "    This function splits the tree into three difference arrays of values\n",
        "    \"\"\"\n",
        "    data, distance, agent_data = _split_node_into_feature_groups(tree)\n",
        "\n",
        "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
        "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(tree.childs[direction], 1, max_tree_depth)\n",
        "        data = np.concatenate((data, sub_data))\n",
        "        distance = np.concatenate((distance, sub_distance))\n",
        "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def normalize_observation(observation, tree_depth: int, observation_radius=0):\n",
        "    \"\"\"\n",
        "    This function normalizes the observation used by the RL algorithm\n",
        "    \"\"\"\n",
        "    data, distance, agent_data = split_tree_into_feature_groups(observation, tree_depth)\n",
        "\n",
        "    data = norm_obs_clip(data, fixed_radius=observation_radius)\n",
        "    distance = norm_obs_clip(distance, normalize_to_range=True)\n",
        "    agent_data = np.clip(agent_data, -1, 1)\n",
        "    normalized_obs = np.concatenate((np.concatenate((data, distance)), agent_data))\n",
        "    return normalized_obs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuASmxBLbk1K"
      },
      "source": [
        "## RL Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLEtssE_rVNF",
        "outputId": "e1ef1a1e-4720-4e16-c76b-ec49e020e625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n",
            "\u001b[31mERROR: torch-0.4.0-{platform}-linux_x86_64.whl is not a valid wheel filename.\u001b[0m\n",
            "Torch 1.10.0+cu111 CUDA 11.1\n",
            "Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "\n",
        "!pip install --upgrade --force-reinstall -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tZVhjovKbnwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d808b3-2e7d-43ee-830d-244344e9e100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  \"\"\"\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from collections import namedtuple, deque, Iterable\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DuelingQNetwork(nn.Module):\n",
        "    \"\"\"Dueling Q-network (https://arxiv.org/abs/1511.06581)\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, hidsize1=128, hidsize2=128):\n",
        "        super(DuelingQNetwork, self).__init__()\n",
        "\n",
        "        # value network\n",
        "        self.fc1_val = nn.Linear(state_size, hidsize1)\n",
        "        self.fc2_val = nn.Linear(hidsize1, hidsize2)\n",
        "        self.fc4_val = nn.Linear(hidsize2, 1)\n",
        "\n",
        "        # advantage network\n",
        "        self.fc1_adv = nn.Linear(state_size, hidsize1)\n",
        "        self.fc2_adv = nn.Linear(hidsize1, hidsize2)\n",
        "        self.fc4_adv = nn.Linear(hidsize2, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        val = F.relu(self.fc1_val(x))\n",
        "        val = F.relu(self.fc2_val(val))\n",
        "        val = self.fc4_val(val)\n",
        "\n",
        "        # advantage calculation\n",
        "        adv = F.relu(self.fc1_adv(x))\n",
        "        adv = F.relu(self.fc2_adv(adv))\n",
        "        adv = self.fc4_adv(adv)\n",
        "\n",
        "        return val + adv - adv.mean()\n",
        "\n",
        "class Policy:\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save(self, filename):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def load(self, filename):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class DDDQNPolicy(Policy):\n",
        "    \"\"\"Dueling Double DQN policy\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, parameters, evaluation_mode=False):\n",
        "        self.evaluation_mode = evaluation_mode\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.double_dqn = True\n",
        "        self.hidsize = 1\n",
        "\n",
        "        if not evaluation_mode:\n",
        "            self.hidsize = parameters.hidden_size\n",
        "            self.buffer_size = parameters.buffer_size\n",
        "            self.batch_size = parameters.batch_size\n",
        "            self.update_every = parameters.update_every\n",
        "            self.learning_rate = parameters.learning_rate\n",
        "            self.tau = parameters.tau\n",
        "            self.gamma = parameters.gamma\n",
        "            self.buffer_min_size = parameters.buffer_min_size\n",
        "\n",
        "        # Device\n",
        "        if parameters.use_gpu and torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda:0\")\n",
        "            # print(\"🐇 Using GPU\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "            # print(\"🐢 Using CPU\")\n",
        "\n",
        "        # Q-Network\n",
        "        self.qnetwork_local = DuelingQNetwork(state_size, action_size, hidsize1=self.hidsize, hidsize2=self.hidsize).to(self.device)\n",
        "\n",
        "        if not evaluation_mode:\n",
        "            self.qnetwork_target = copy.deepcopy(self.qnetwork_local)\n",
        "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.learning_rate)\n",
        "            self.memory = ReplayBuffer(action_size, self.buffer_size, self.batch_size, self.device)\n",
        "\n",
        "            self.t_step = 0\n",
        "            self.loss = 0.0\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        assert not self.evaluation_mode, \"Policy has been initialized for evaluation only.\"\n",
        "\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % self.update_every\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > self.buffer_min_size and len(self.memory) > self.batch_size:\n",
        "                self._learn()\n",
        "\n",
        "    def _learn(self):\n",
        "        experiences = self.memory.sample()\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Get expected Q values from local model\n",
        "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        if self.double_dqn:\n",
        "            # Double DQN\n",
        "            q_best_action = self.qnetwork_local(next_states).max(1)[1]\n",
        "            q_targets_next = self.qnetwork_target(next_states).gather(1, q_best_action.unsqueeze(-1))\n",
        "        else:\n",
        "            # DQN\n",
        "            q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(-1)\n",
        "\n",
        "        # Compute Q targets for current states\n",
        "        q_targets = rewards + (self.gamma * q_targets_next * (1 - dones))\n",
        "\n",
        "        # Compute loss\n",
        "        self.loss = F.mse_loss(q_expected, q_targets)\n",
        "\n",
        "        # Minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        self.loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        self._soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
        "\n",
        "    def _soft_update(self, local_model, target_model, tau):\n",
        "        # Soft update model parameters.\n",
        "        # θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "    def save(self, filename):\n",
        "        torch.save(self.qnetwork_local.state_dict(), filename + \".local\")\n",
        "        torch.save(self.qnetwork_target.state_dict(), filename + \".target\")\n",
        "\n",
        "    def load(self, filename):\n",
        "        if os.path.exists(filename + \".local\"):\n",
        "            self.qnetwork_local.load_state_dict(torch.load(filename + \".local\"))\n",
        "        if os.path.exists(filename + \".target\"):\n",
        "            self.qnetwork_target.load_state_dict(torch.load(filename + \".target\"))\n",
        "\n",
        "    def save_replay_buffer(self, filename):\n",
        "        memory = self.memory.memory\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(list(memory)[-500000:], f)\n",
        "\n",
        "    def load_replay_buffer(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.memory.memory = pickle.load(f)\n",
        "\n",
        "    def test(self):\n",
        "        self.act(np.array([[0] * self.state_size]))\n",
        "        self._learn()\n",
        "\n",
        "\n",
        "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, device):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = Experience(np.expand_dims(state, 0), action, reward, np.expand_dims(next_state, 0), done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(self.__v_stack_impr([e.state for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        actions = torch.from_numpy(self.__v_stack_impr([e.action for e in experiences if e is not None])) \\\n",
        "            .long().to(self.device)\n",
        "        rewards = torch.from_numpy(self.__v_stack_impr([e.reward for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        next_states = torch.from_numpy(self.__v_stack_impr([e.next_state for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        dones = torch.from_numpy(self.__v_stack_impr([e.done for e in experiences if e is not None]).astype(np.uint8)) \\\n",
        "            .float().to(self.device)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n",
        "\n",
        "    def __v_stack_impr(self, states):\n",
        "        sub_dim = len(states[0][0]) if isinstance(states[0], Iterable) else 1\n",
        "        np_states = np.reshape(np.array(states), (len(states), sub_dim))\n",
        "        return np_states\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMZuh-ENcPpx"
      },
      "source": [
        "## Network Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8JIixnrEcK7i"
      },
      "outputs": [],
      "source": [
        "\n",
        "from argparse import ArgumentParser, Namespace\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "\n",
        "\n",
        "parser = ArgumentParser()\n",
        "parser.add_argument(\"-n\", \"--n_episodes\", help=\"number of episodes to run\", default=2500, type=int)\n",
        "parser.add_argument(\"-t\", \"--training_env_config\", help=\"training config id (eg 0 for Test_0)\", default=0, type=int)\n",
        "parser.add_argument(\"-e\", \"--evaluation_env_config\", help=\"evaluation config id (eg 0 for Test_0)\", default=0, type=int)\n",
        "parser.add_argument(\"--n_evaluation_episodes\", help=\"number of evaluation episodes\", default=25, type=int)\n",
        "parser.add_argument(\"--checkpoint_interval\", help=\"checkpoint interval\", default=100, type=int)\n",
        "parser.add_argument(\"--eps_start\", help=\"max exploration\", default=1.0, type=float)\n",
        "parser.add_argument(\"--eps_end\", help=\"min exploration\", default=0.01, type=float)\n",
        "parser.add_argument(\"--eps_decay\", help=\"exploration decay\", default=0.99, type=float)\n",
        "parser.add_argument(\"--buffer_size\", help=\"replay buffer size\", default=int(1e5), type=int)\n",
        "parser.add_argument(\"--buffer_min_size\", help=\"min buffer size to start training\", default=0, type=int)\n",
        "parser.add_argument(\"--restore_replay_buffer\", help=\"replay buffer to restore\", default=\"\", type=str)\n",
        "parser.add_argument(\"--save_replay_buffer\", help=\"save replay buffer at each evaluation interval\", default=False, type=bool)\n",
        "parser.add_argument(\"--batch_size\", help=\"minibatch size\", default=128, type=int)\n",
        "parser.add_argument(\"--gamma\", help=\"discount factor\", default=0.99, type=float)\n",
        "parser.add_argument(\"--tau\", help=\"soft update of target parameters\", default=1e-3, type=float)\n",
        "parser.add_argument(\"--learning_rate\", help=\"learning rate\", default=0.5e-4, type=float)\n",
        "parser.add_argument(\"--hidden_size\", help=\"hidden size (2 fc layers)\", default=128, type=int)\n",
        "parser.add_argument(\"--update_every\", help=\"how often to update the network\", default=8, type=int)\n",
        "parser.add_argument(\"--use_gpu\", help=\"use GPU if available\", default=True, type=bool)\n",
        "parser.add_argument(\"--num_threads\", help=\"number of threads PyTorch can use\", default=1, type=int)\n",
        "parser.add_argument(\"--render\", help=\"render 1 episode in 100\", default=False, type=bool)\n",
        "parser.add_argument(\"--track\", help=\"whether to track using wandb\", default=False, type=bool)\n",
        "training_params = parser.parse_args()\n",
        "\n",
        "\n",
        "obs_params = {\n",
        "    \"observation_tree_depth\": 2,\n",
        "    \"observation_radius\": 10,\n",
        "    \"observation_max_path_depth\": 30\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbIzu6GJhDLn"
      },
      "source": [
        "## Modified Files\n",
        "These files are modified from the real Flatland, so it's important to execute them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9HCQuF5nhSGw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from flatland.envs.step_utils.states import TrainState\n",
        "\n",
        "class SpeedCounter:\n",
        "    def __init__(self, speed):\n",
        "        self._speed = speed\n",
        "        self.counter = None\n",
        "        self.reset_counter()\n",
        "\n",
        "    def update_counter(self, state, old_position):\n",
        "        # Can't start counting when adding train to the map\n",
        "        if state == TrainState.MOVING and old_position is not None:\n",
        "            self.counter += 1\n",
        "            self.counter = self.counter % (self.max_count + 1)\n",
        "\n",
        "            \n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"speed: {self.speed} \\\n",
        "                 max_count: {self.max_count} \\\n",
        "                 is_cell_entry: {self.is_cell_entry} \\\n",
        "                 is_cell_exit: {self.is_cell_exit} \\\n",
        "                 counter: {self.counter}\"\n",
        "\n",
        "    def reset_counter(self):\n",
        "        self.counter = 0\n",
        "\n",
        "    @property\n",
        "    def is_cell_entry(self):\n",
        "        return self.counter == 0\n",
        "\n",
        "    @property\n",
        "    def is_cell_exit(self):\n",
        "        return self.counter == self.max_count\n",
        "\n",
        "    @property\n",
        "    def speed(self):\n",
        "        return self._speed\n",
        "\n",
        "    @speed.setter\n",
        "    def speed(self, value):\n",
        "        self._speed = value\n",
        "\n",
        "    @property\n",
        "    def max_count(self):\n",
        "        return int(1/self._speed) - 1\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\"speed\": self._speed,\n",
        "                \"counter\": self.counter}\n",
        "    \n",
        "    def from_dict(self, load_dict):\n",
        "        self._speed = load_dict['speed']\n",
        "        self.counter = load_dict['counter']\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self._speed == other._speed and self.counter == other.counter\n",
        "\n",
        "class RailEnvActions(IntEnum):\n",
        "    DO_NOTHING = 0  # implies change of direction in a dead-end!\n",
        "    MOVE_LEFT = 1\n",
        "    MOVE_FORWARD = 2\n",
        "    MOVE_RIGHT = 3\n",
        "    STOP_MOVING = 4\n",
        "    REVERSE = 5\n",
        "\n",
        "    @staticmethod\n",
        "    def to_char(a: int):\n",
        "        return {\n",
        "            0: 'B',\n",
        "            1: 'L',\n",
        "            2: 'F',\n",
        "            3: 'R',\n",
        "            4: 'S',\n",
        "            5: 'I',\n",
        "        }[a]\n",
        "\n",
        "    @classmethod\n",
        "    def is_action_valid(cls, action):\n",
        "        return action in cls._value2member_map_\n",
        "\n",
        "    def is_moving_action(self):\n",
        "        return self.value in [self.MOVE_RIGHT, self.MOVE_LEFT, self.MOVE_FORWARD, self.REVERSE]\n",
        "\n",
        "\n",
        "from flatland.envs.rail_trainrun_data_structures import Waypoint\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from typing import Tuple, Optional, NamedTuple, List\n",
        "\n",
        "from attr import attr, attrs, attrib, Factory\n",
        "\n",
        "from flatland.core.grid.grid4 import Grid4TransitionsEnum\n",
        "from flatland.envs.timetable_utils import Line\n",
        "\n",
        "from flatland.envs.step_utils.action_saver import ActionSaver\n",
        "from flatland.envs.step_utils.state_machine import TrainStateMachine\n",
        "from flatland.envs.step_utils.states import TrainState\n",
        "from flatland.envs.step_utils.malfunction_handler import MalfunctionHandler\n",
        "\n",
        "Agent = NamedTuple('Agent', [('initial_position', Tuple[int, int]),\n",
        "                             ('initial_direction', Grid4TransitionsEnum),\n",
        "                             ('direction', Grid4TransitionsEnum),\n",
        "                             ('target', Tuple[int, int]),\n",
        "                             ('moving', bool),\n",
        "                             ('earliest_departure', int),\n",
        "                             ('latest_arrival', int),\n",
        "                             ('handle', int),\n",
        "                             ('position', Tuple[int, int]),\n",
        "                             ('arrival_time', int),\n",
        "                             ('old_direction', Grid4TransitionsEnum),\n",
        "                             ('old_position', Tuple[int, int]),\n",
        "                             ('speed_counter', SpeedCounter),\n",
        "                             ('action_saver', ActionSaver),\n",
        "                             ('state_machine', TrainStateMachine),\n",
        "                             ('malfunction_handler', MalfunctionHandler),\n",
        "                             ])\n",
        "\n",
        "\n",
        "def load_env_agent(agent_tuple: Agent):\n",
        "     return EnvAgent(\n",
        "                        initial_position = agent_tuple.initial_position,\n",
        "                        initial_direction = agent_tuple.initial_direction,\n",
        "                        direction = agent_tuple.direction,\n",
        "                        target = agent_tuple.target,\n",
        "                        moving = agent_tuple.moving,\n",
        "                        earliest_departure = agent_tuple.earliest_departure,\n",
        "                        latest_arrival = agent_tuple.latest_arrival,\n",
        "                        handle = agent_tuple.handle,\n",
        "                        position = agent_tuple.position,\n",
        "                        arrival_time = agent_tuple.arrival_time,\n",
        "                        old_direction = agent_tuple.old_direction,\n",
        "                        old_position = agent_tuple.old_position,\n",
        "                        speed_counter = agent_tuple.speed_counter,\n",
        "                        action_saver = agent_tuple.action_saver,\n",
        "                        state_machine = agent_tuple.state_machine,\n",
        "                        malfunction_handler = agent_tuple.malfunction_handler,\n",
        "                    )\n",
        "\n",
        "@attrs\n",
        "class EnvAgent:\n",
        "    # INIT FROM HERE IN _from_line()\n",
        "    initial_position = attrib(type=Tuple[int, int])\n",
        "    initial_direction = attrib(type=Grid4TransitionsEnum)\n",
        "    direction = attrib(type=Grid4TransitionsEnum)\n",
        "    target = attrib(type=Tuple[int, int])\n",
        "    moving = attrib(default=False, type=bool)\n",
        "\n",
        "    # NEW : EnvAgent - Schedule properties\n",
        "    earliest_departure = attrib(default=None, type=int)  # default None during _from_line()\n",
        "    latest_arrival = attrib(default=None, type=int)  # default None during _from_line()\n",
        "\n",
        "    handle = attrib(default=None)\n",
        "    # INIT TILL HERE IN _from_line()\n",
        "\n",
        "    # Env step facelift\n",
        "    speed_counter = attrib(default = Factory(lambda: SpeedCounter(1.0)), type=SpeedCounter)\n",
        "    action_saver = attrib(default = Factory(lambda: ActionSaver()), type=ActionSaver)\n",
        "    state_machine = attrib(default= Factory(lambda: TrainStateMachine(initial_state=TrainState.WAITING)) , \n",
        "                           type=TrainStateMachine)\n",
        "    malfunction_handler = attrib(default = Factory(lambda: MalfunctionHandler()), type=MalfunctionHandler)\n",
        "\n",
        "    position = attrib(default=None, type=Optional[Tuple[int, int]])\n",
        "\n",
        "    # NEW : EnvAgent Reward Handling\n",
        "    arrival_time = attrib(default=None, type=int)\n",
        "\n",
        "    # used in rendering\n",
        "    old_direction = attrib(default=None)\n",
        "    old_position = attrib(default=None)\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Resets the agents to their initial values of the episode. Called after ScheduleTime generation.\n",
        "        \"\"\"\n",
        "        self.position = None\n",
        "        # TODO: set direction to None: https://gitlab.aicrowd.com/flatland/flatland/issues/280\n",
        "        self.direction = self.initial_direction\n",
        "        self.old_position = None\n",
        "        self.old_direction = None\n",
        "        self.moving = False\n",
        "\n",
        "        self.malfunction_handler.reset()\n",
        "\n",
        "        self.action_saver.clear_saved_action()\n",
        "        self.speed_counter.reset_counter()\n",
        "        self.state_machine.reset()\n",
        "\n",
        "    def to_agent(self) -> Agent:\n",
        "        return Agent(initial_position=self.initial_position, \n",
        "                     initial_direction=self.initial_direction,\n",
        "                     direction=self.direction,\n",
        "                     target=self.target,\n",
        "                     moving=self.moving,\n",
        "                     earliest_departure=self.earliest_departure, \n",
        "                     latest_arrival=self.latest_arrival, \n",
        "                     handle=self.handle,\n",
        "                     position=self.position, \n",
        "                     old_direction=self.old_direction, \n",
        "                     old_position=self.old_position,\n",
        "                     speed_counter=self.speed_counter,\n",
        "                     action_saver=self.action_saver,\n",
        "                     arrival_time=self.arrival_time,\n",
        "                     state_machine=self.state_machine,\n",
        "                     malfunction_handler=self.malfunction_handler)\n",
        "\n",
        "    def get_shortest_path(self, distance_map) -> List[Waypoint]:\n",
        "        from flatland.envs.rail_env_shortest_paths import get_shortest_paths # Circular dep fix\n",
        "        return get_shortest_paths(distance_map=distance_map, agent_handle=self.handle)[self.handle]\n",
        "        \n",
        "    def get_travel_time_on_shortest_path(self, distance_map) -> int:\n",
        "        shortest_path = self.get_shortest_path(distance_map)\n",
        "        if shortest_path is not None:\n",
        "            distance = len(shortest_path)\n",
        "        else:\n",
        "            distance = 0\n",
        "        speed = self.speed_counter.speed\n",
        "        return int(np.ceil(distance / speed))\n",
        "\n",
        "    def get_time_remaining_until_latest_arrival(self, elapsed_steps: int) -> int:\n",
        "        return self.latest_arrival - elapsed_steps\n",
        "\n",
        "    def get_current_delay(self, elapsed_steps: int, distance_map) -> int:\n",
        "        '''\n",
        "        +ve if arrival time is projected before latest arrival\n",
        "        -ve if arrival time is projected after latest arrival\n",
        "        '''\n",
        "        return self.get_time_remaining_until_latest_arrival(elapsed_steps) - \\\n",
        "               self.get_travel_time_on_shortest_path(distance_map)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_line(cls, line: Line):\n",
        "        \"\"\" Create a list of EnvAgent from lists of positions, directions and targets\n",
        "        \"\"\"\n",
        "\n",
        "        #print(line.agent_directions)\n",
        "\n",
        "        num_agents = len(line.agent_positions)\n",
        "        \n",
        "        agent_list = []\n",
        "        for i_agent in range(num_agents):\n",
        "            speed = line.agent_speeds[i_agent] if line.agent_speeds is not None else 1.0\n",
        "            \n",
        "            agent = EnvAgent(initial_position = line.agent_positions[i_agent],\n",
        "                            initial_direction = line.agent_directions[i_agent],\n",
        "                            direction = line.agent_directions[i_agent],\n",
        "                            target = line.agent_targets[i_agent], \n",
        "                            moving = False, \n",
        "                            earliest_departure = None,\n",
        "                            latest_arrival = None,\n",
        "                            handle = i_agent,\n",
        "                            speed_counter = SpeedCounter(speed=speed))\n",
        "            agent_list.append(agent)\n",
        "\n",
        "        return agent_list\n",
        "\n",
        "    @classmethod\n",
        "    def load_legacy_static_agent(cls, static_agents_data: Tuple):\n",
        "        agents = []\n",
        "        for i, static_agent in enumerate(static_agents_data):\n",
        "            if len(static_agent) >= 6:\n",
        "                agent = EnvAgent(initial_position=static_agent[0], initial_direction=static_agent[1],\n",
        "                                direction=static_agent[1], target=static_agent[2], moving=static_agent[3],\n",
        "                                speed_counter=SpeedCounter(static_agent[4]['speed']), handle=i)\n",
        "            else:\n",
        "                agent = EnvAgent(initial_position=static_agent[0], initial_direction=static_agent[1],\n",
        "                                direction=static_agent[1], target=static_agent[2], \n",
        "                                moving=False,\n",
        "                                speed_counter=SpeedCounter(1.0),\n",
        "                                handle=i)\n",
        "            agents.append(agent)\n",
        "        return agents\n",
        "    \n",
        "    def __str__(self):\n",
        "        return f\"\\n \\\n",
        "                 handle(agent index): {self.handle} \\n \\\n",
        "                 initial_position: {self.initial_position}  \\n \\\n",
        "                 initial_direction: {self.initial_direction} \\n \\\n",
        "                 position: {self.position}  \\n \\\n",
        "                 direction: {self.direction}  \\n \\\n",
        "                 target: {self.target} \\n \\\n",
        "                 old_position: {self.old_position} \\n \\\n",
        "                 old_direction {self.old_direction} \\n \\\n",
        "                 earliest_departure: {self.earliest_departure}  \\n \\\n",
        "                 latest_arrival: {self.latest_arrival} \\n \\\n",
        "                 state: {str(self.state)} \\n \\\n",
        "                 malfunction_handler: {self.malfunction_handler} \\n \\\n",
        "                 action_saver: {self.action_saver} \\n \\\n",
        "                 speed_counter: {self.speed_counter}\"\n",
        "\n",
        "    @property\n",
        "    def state(self):\n",
        "        return self.state_machine.state\n",
        "\n",
        "    @state.setter\n",
        "    def state(self, state):\n",
        "        self._set_state(state)\n",
        "    \n",
        "    def _set_state(self, state):\n",
        "        warnings.warn(\"Not recommended to set the state with this function unless completely required\")\n",
        "        self.state_machine.set_state(state)\n",
        "\n",
        "    @property\n",
        "    def malfunction_data(self):\n",
        "        raise ValueError(\"agent.malunction_data is deprecated, please use agent.malfunction_hander instead\")\n",
        "\n",
        "    @property\n",
        "    def speed_data(self):\n",
        "        raise ValueError(\"agent.speed_data is deprecated, please use agent.speed_counter instead\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "TransitionMap and derived classes.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from importlib_resources import path\n",
        "from numpy import array\n",
        "\n",
        "from flatland.core.grid.grid4 import Grid4Transitions\n",
        "from flatland.core.grid.grid4_utils import get_new_position, get_direction\n",
        "from flatland.core.grid.grid_utils import IntVector2DArray, IntVector2D\n",
        "from flatland.core.grid.grid_utils import Vec2dOperations as Vec2d\n",
        "from flatland.core.grid.rail_env_grid import RailEnvTransitions\n",
        "from flatland.core.transitions import Transitions\n",
        "from flatland.utils.ordered_set import OrderedSet\n",
        "from flatland.core.grid.grid4 import Grid4TransitionsEnum\n",
        "\n",
        "\n",
        "# TODO are these general classes or for grid4 only?\n",
        "class TransitionMap:\n",
        "    \"\"\"\n",
        "    Base TransitionMap class.\n",
        "\n",
        "    Generic class that implements a collection of transitions over a set of\n",
        "    cells.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_transitions(self, cell_id):\n",
        "        \"\"\"\n",
        "        Return a tuple of transitions available in a cell specified by\n",
        "        `cell_id` (e.g., a tuple of size of the maximum number of transitions,\n",
        "        with values 0 or 1, or potentially in between,\n",
        "        for stochastic transitions).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cell_id : [cell identifier]\n",
        "            The cell_id object depends on the specific implementation.\n",
        "            It generally is an int (e.g., an index) or a tuple of indices.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            List of the validity of transitions in the cell.\n",
        "\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def set_transitions(self, cell_id, new_transitions):\n",
        "        \"\"\"\n",
        "        Replaces the available transitions in cell `cell_id` with the tuple\n",
        "        `new_transitions'. `new_transitions` must have\n",
        "        one element for each possible transition.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cell_id : [cell identifier]\n",
        "            The cell_id object depends on the specific implementation.\n",
        "            It generally is an int (e.g., an index) or a tuple of indices.\n",
        "        new_transitions : tuple\n",
        "            Tuple of new transitions validitiy for the cell.\n",
        "\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_transition(self, cell_id, transition_index):\n",
        "        \"\"\"\n",
        "        Return the status of whether an agent in cell `cell_id` can perform a\n",
        "        movement along transition `transition_index` (e.g., the NESW direction\n",
        "        of movement, for agents on a grid).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cell_id : [cell identifier]\n",
        "            The cell_id object depends on the specific implementation.\n",
        "            It generally is an int (e.g., an index) or a tuple of indices.\n",
        "        transition_index : int\n",
        "            Index of the transition to probe, as index in the tuple returned by\n",
        "            get_transitions(). e.g., the NESW direction of movement, for agents\n",
        "            on a grid.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int or float (depending on Transitions used)\n",
        "            Validity of the requested transition (e.g.,\n",
        "            0/1 allowed/not allowed, a probability in [0,1], etc...)\n",
        "\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def set_transition(self, cell_id, transition_index, new_transition):\n",
        "        \"\"\"\n",
        "        Replaces the validity of transition to `transition_index` in cell\n",
        "        `cell_id' with the new `new_transition`.\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cell_id : [cell identifier]\n",
        "            The cell_id object depends on the specific implementation.\n",
        "            It generally is an int (e.g., an index) or a tuple of indices.\n",
        "        transition_index : int\n",
        "            Index of the transition to probe, as index in the tuple returned by\n",
        "            get_transitions(). e.g., the NESW direction of movement, for agents\n",
        "            on a grid.\n",
        "        new_transition : int or float (depending on Transitions used)\n",
        "            Validity of the requested transition (e.g.,\n",
        "            0/1 allowed/not allowed, a probability in [0,1], etc...)\n",
        "\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class GridTransitionMap(TransitionMap):\n",
        "    \"\"\"\n",
        "    Implements a TransitionMap over a 2D grid.\n",
        "\n",
        "    GridTransitionMap implements utility functions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, width, height, transitions: Transitions = Grid4Transitions([]), random_seed=None):\n",
        "        \"\"\"\n",
        "        Builder for GridTransitionMap object.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        width : int\n",
        "            Width of the grid.\n",
        "        height : int\n",
        "            Height of the grid.\n",
        "        transitions : Transitions object\n",
        "            The Transitions object to use to encode/decode transitions over the\n",
        "            grid.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.transitions = transitions\n",
        "        self.random_generator = np.random.RandomState()\n",
        "        if random_seed is None:\n",
        "            self.random_generator.seed(12)\n",
        "        else:\n",
        "            self.random_generator.seed(random_seed)\n",
        "        self.grid = np.zeros((height, width), dtype=self.transitions.get_type())\n",
        "\n",
        "    def get_full_transitions(self, row, column):\n",
        "        \"\"\"\n",
        "        Returns the full transitions for the cell at (row, column) in the format transition_map's transitions.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        row: int\n",
        "        column: int\n",
        "            (row,column) specifies the cell in this transition map.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self.transitions.get_type()\n",
        "            The cell content int the format of this map's Transitions.\n",
        "\n",
        "        \"\"\"\n",
        "        return self.grid[row][column]\n",
        "\n",
        "    def get_transitions(self, row, column, orientation):\n",
        "        \"\"\"\n",
        "        Return a tuple of transitions available in a cell specified by\n",
        "        `cell_id` (e.g., a tuple of size of the maximum number of transitions,\n",
        "        with values 0 or 1, or potentially in between,\n",
        "        for stochastic transitions).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cell_id : tuple\n",
        "            The cell_id indices a cell as (column, row, orientation),\n",
        "            where orientation is the direction an agent is facing within a cell.\n",
        "            Alternatively, it can be accessed as (column, row) to return the\n",
        "            full cell content.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            List of the validity of transitions in the cell as given by the maps transitions.\n",
        "\n",
        "        \"\"\"\n",
        "        return self.transitions.get_transitions(self.grid[row][column], orientation)\n",
        "\n",
        "    def set_transitions(self, cell_id, new_transitions):\n",
        "        \"\"\"\n",
        "        Replaces the available transitions in cell `cell_id` with the tuple\n",
        "        `new_transitions'. `new_transitions` must have\n",
        "        one element for each possible transition.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cell_id : tuple\n",
        "            The cell_id indices a cell as (column, row, orientation),\n",
        "            where orientation is the direction an agent is facing within a cell.\n",
        "            Alternatively, it can be accessed as (column, row) to replace the\n",
        "            full cell content.\n",
        "        new_transitions : tuple\n",
        "            Tuple of new transitions validitiy for the cell.\n",
        "\n",
        "        \"\"\"\n",
        "        assert len(cell_id) in (2, 3), \\\n",
        "            'GridTransitionMap.set_transitions() ERROR: cell_id tuple must have length 2 or 3.'\n",
        "        if len(cell_id) == 3:\n",
        "            self.grid[cell_id[0]][cell_id[1]] = self.transitions.set_transitions(self.grid[cell_id[0]][cell_id[1]],\n",
        "                                                                                 cell_id[2],\n",
        "                                                                                 new_transitions)\n",
        "        elif len(cell_id) == 2:\n",
        "            self.grid[cell_id[0]][cell_id[1]] = new_transitions\n",
        "\n",
        "    def get_transition(self, cell_id, transition_index):\n",
        "        \"\"\"\n",
        "        Return the status of whether an agent in cell `cell_id` can perform a\n",
        "        movement along transition `transition_index` (e.g., the NESW direction\n",
        "        of movement, for agents on a grid).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cell_id : tuple\n",
        "            The cell_id indices a cell as (column, row, orientation),\n",
        "            where orientation is the direction an agent is facing within a cell.\n",
        "        transition_index : int\n",
        "            Index of the transition to probe, as index in the tuple returned by\n",
        "            get_transitions(). e.g., the NESW direction of movement, for agents\n",
        "            on a grid.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int or float (depending on Transitions used in the )\n",
        "            Validity of the requested transition (e.g.,\n",
        "            0/1 allowed/not allowed, a probability in [0,1], etc...)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        assert len(cell_id) == 3, \\\n",
        "            'GridTransitionMap.get_transition() ERROR: cell_id tuple must have length 2 or 3.'\n",
        "        return self.transitions.get_transition(self.grid[cell_id[0]][cell_id[1]], cell_id[2], transition_index)\n",
        "\n",
        "    def set_transition(self, cell_id, transition_index, new_transition, remove_deadends=False):\n",
        "        \"\"\"\n",
        "        Replaces the validity of transition to `transition_index` in cell\n",
        "        `cell_id' with the new `new_transition`.\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cell_id : tuple\n",
        "            The cell_id indices a cell as (column, row, orientation),\n",
        "            where orientation is the direction an agent is facing within a cell.\n",
        "        transition_index : int\n",
        "            Index of the transition to probe, as index in the tuple returned by\n",
        "            get_transitions(). e.g., the NESW direction of movement, for agents\n",
        "            on a grid.\n",
        "        new_transition : int or float (depending on Transitions used in the map.)\n",
        "            Validity of the requested transition (e.g.,\n",
        "            0/1 allowed/not allowed, a probability in [0,1], etc...)\n",
        "\n",
        "        \"\"\"\n",
        "        assert len(cell_id) == 3, \\\n",
        "            'GridTransitionMap.set_transition() ERROR: cell_id tuple must have length 3.'\n",
        "        self.grid[cell_id[0]][cell_id[1]] = self.transitions.set_transition(\n",
        "            self.grid[cell_id[0]][cell_id[1]],\n",
        "            cell_id[2],\n",
        "            transition_index,\n",
        "            new_transition,\n",
        "            remove_deadends)\n",
        "\n",
        "    def save_transition_map(self, filename):\n",
        "        \"\"\"\n",
        "        Save the transitions grid as `filename`, in npy format.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filename : string\n",
        "            Name of the file to which to save the transitions grid.\n",
        "\n",
        "        \"\"\"\n",
        "        np.save(filename, self.grid)\n",
        "\n",
        "    def load_transition_map(self, package, resource):\n",
        "        \"\"\"\n",
        "        Load the transitions grid from `filename` (npy format).\n",
        "        The load function only updates the transitions grid, and possibly width and height, but the object has to be\n",
        "        initialized with the correct `transitions` object anyway.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        package : string\n",
        "            Name of the package from which to load the transitions grid.\n",
        "        resource : string\n",
        "            Name of the file from which to load the transitions grid within the package.\n",
        "        override_gridsize : bool\n",
        "            If override_gridsize=True, the width and height of the GridTransitionMap object are replaced with the size\n",
        "            of the map loaded from `filename`. If override_gridsize=False, the transitions grid is either cropped (if\n",
        "            the grid size is larger than (height,width) ) or padded with zeros (if the grid size is smaller than\n",
        "            (height,width) )\n",
        "\n",
        "        \"\"\"\n",
        "        with path(package, resource) as file_in:\n",
        "            new_grid = np.load(file_in)\n",
        "\n",
        "        new_height = new_grid.shape[0]\n",
        "        new_width = new_grid.shape[1]\n",
        "\n",
        "        self.width = new_width\n",
        "        self.height = new_height\n",
        "        self.grid = new_grid\n",
        "\n",
        "    def is_dead_end(self, rcPos: IntVector2DArray):\n",
        "        \"\"\"\n",
        "        Check if the cell is a dead-end.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rcPos: Tuple[int,int]\n",
        "            tuple(row, column) with grid coordinate\n",
        "        Returns\n",
        "        -------\n",
        "        boolean\n",
        "            True if and only if the cell is a dead-end.\n",
        "        \"\"\"\n",
        "        nbits = 0\n",
        "        tmp = self.get_full_transitions(rcPos[0], rcPos[1])\n",
        "        while tmp > 0:\n",
        "            nbits += (tmp & 1)\n",
        "            tmp = tmp >> 1\n",
        "        return nbits == 1\n",
        "\n",
        "    def is_simple_turn(self, rcPos: IntVector2DArray):\n",
        "        \"\"\"\n",
        "        Check if the cell is a left/right simple turn\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            rcPos: Tuple[int,int]\n",
        "                tuple(row, column) with grid coordinate\n",
        "        Returns\n",
        "        -------\n",
        "            boolean\n",
        "                True if and only if the cell is a left/right simple turn.\n",
        "        \"\"\"\n",
        "        tmp = self.get_full_transitions(rcPos[0], rcPos[1])\n",
        "\n",
        "        def is_simple_turn(trans):\n",
        "            all_simple_turns = OrderedSet()\n",
        "            for trans in [int('0100000000000010', 2),  # Case 1b (8)  - simple turn right\n",
        "                          int('0001001000000000', 2)  # Case 1c (9)  - simple turn left]:\n",
        "                          ]:\n",
        "                for _ in range(3):\n",
        "                    trans = self.transitions.rotate_transition(trans, rotation=90)\n",
        "                    all_simple_turns.add(trans)\n",
        "            return trans in all_simple_turns\n",
        "\n",
        "        return is_simple_turn(tmp)\n",
        "\n",
        "    def check_path_exists(self, start: IntVector2DArray, direction: int, end: IntVector2DArray):\n",
        "        \"\"\"\n",
        "        Breath first search for a possible path from one node with a certain orientation to a target node.\n",
        "        :param start: Start cell rom where we want to check the path\n",
        "        :param direction: Start direction for the path we are testing\n",
        "        :param end: Cell that we try to reach from the start cell\n",
        "        :return: True if a path exists, False otherwise\n",
        "        \"\"\"\n",
        "        visited = OrderedSet()\n",
        "        stack = [(start, direction)]\n",
        "        while stack:\n",
        "            node = stack.pop()\n",
        "            node_position = node[0]\n",
        "            node_direction = node[1]\n",
        "\n",
        "            if Vec2d.is_equal(node_position, end):\n",
        "                return True\n",
        "            if node not in visited:\n",
        "                visited.add(node)\n",
        "\n",
        "                moves = self.get_transitions(node_position[0], node_position[1], node_direction)\n",
        "                for move_index in range(4):\n",
        "                    if moves[move_index]:\n",
        "                        stack.append((get_new_position(node_position, move_index),\n",
        "                                      move_index))\n",
        "\n",
        "        return False\n",
        "\n",
        "    def cell_neighbours_valid(self, rcPos: IntVector2DArray, check_this_cell=False):\n",
        "        \"\"\"\n",
        "        Check validity of cell at rcPos = tuple(row, column)\n",
        "        Checks that:\n",
        "        - surrounding cells have inbound transitions for all the outbound transitions of this cell.\n",
        "\n",
        "        These are NOT checked - see transition.is_valid:\n",
        "        - all transitions have the mirror transitions (N->E <=> W->S)\n",
        "        - Reverse transitions (N -> S) only exist for a dead-end\n",
        "        - a cell contains either no dead-ends or exactly one\n",
        "\n",
        "        Returns: True (valid) or False (invalid)\n",
        "        \"\"\"\n",
        "        cell_transition = self.grid[tuple(rcPos)]\n",
        "\n",
        "        if check_this_cell:\n",
        "            if not self.transitions.is_valid(cell_transition):\n",
        "                return False\n",
        "\n",
        "        gDir2dRC = self.transitions.gDir2dRC  # [[-1,0] = N, [0,1]=E, etc]\n",
        "        grcPos = array(rcPos)\n",
        "        grcMax = self.grid.shape\n",
        "\n",
        "        binTrans = self.get_full_transitions(*rcPos)  # 16bit integer - all trans in/out\n",
        "        lnBinTrans = array([binTrans >> 8, binTrans & 0xff], dtype=np.uint8)  # 2 x uint8\n",
        "        g2binTrans = np.unpackbits(lnBinTrans).reshape(4, 4)  # 4x4 x uint8 binary(0,1)\n",
        "        gDirOut = g2binTrans.any(axis=0)  # outbound directions as boolean array (4)\n",
        "        giDirOut = np.argwhere(gDirOut)[:, 0]  # valid outbound directions as array of int\n",
        "\n",
        "        # loop over available outbound directions (indices) for rcPos\n",
        "        for iDirOut in giDirOut:\n",
        "            gdRC = gDir2dRC[iDirOut]  # row,col increment\n",
        "            gPos2 = grcPos + gdRC  # next cell in that direction\n",
        "\n",
        "            # Check the adjacent cell is within bounds\n",
        "            # if not, then this transition is invalid!\n",
        "            if np.any(gPos2 < 0):\n",
        "                return False\n",
        "            if np.any(gPos2 >= grcMax):\n",
        "                return False\n",
        "\n",
        "            # Get the transitions out of gPos2, using iDirOut as the inbound direction\n",
        "            # if there are no available transitions, ie (0,0,0,0), then rcPos is invalid\n",
        "            t4Trans2 = self.get_transitions(*gPos2, iDirOut)\n",
        "            if any(t4Trans2):\n",
        "                continue\n",
        "            else:\n",
        "                return False\n",
        "        # If the cell is empty but has incoming connections we return false\n",
        "        if binTrans < 1:\n",
        "            connected = 0\n",
        "\n",
        "            for iDirOut in np.arange(4):\n",
        "                gdRC = gDir2dRC[iDirOut]  # row,col increment\n",
        "                gPos2 = grcPos + gdRC  # next cell in that direction\n",
        "\n",
        "                # Check the adjacent cell is within bounds\n",
        "                # if not, then ignore it for the count of incoming connections\n",
        "                if np.any(gPos2 < 0):\n",
        "                    continue\n",
        "                if np.any(gPos2 >= grcMax):\n",
        "                    continue\n",
        "\n",
        "                # Get the transitions out of gPos2, using iDirOut as the inbound direction\n",
        "                # if there are no available transitions, ie (0,0,0,0), then rcPos is invalid\n",
        "\n",
        "                for orientation in range(4):\n",
        "                    connected += self.get_transition((gPos2[0], gPos2[1], orientation), mirror(iDirOut))\n",
        "            if connected > 0:\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def fix_neighbours(self, rcPos: IntVector2DArray, check_this_cell=False):\n",
        "        \"\"\"\n",
        "        Check validity of cell at rcPos = tuple(row, column)\n",
        "        Checks that:\n",
        "        - surrounding cells have inbound transitions for all the outbound transitions of this cell.\n",
        "\n",
        "        These are NOT checked - see transition.is_valid:\n",
        "        - all transitions have the mirror transitions (N->E <=> W->S)\n",
        "        - Reverse transitions (N -> S) only exist for a dead-end\n",
        "        - a cell contains either no dead-ends or exactly one\n",
        "\n",
        "        Returns: True (valid) or False (invalid)\n",
        "        \"\"\"\n",
        "        cell_transition = self.grid[tuple(rcPos)]\n",
        "\n",
        "        if check_this_cell:\n",
        "            if not self.transitions.is_valid(cell_transition):\n",
        "                return False\n",
        "\n",
        "        gDir2dRC = self.transitions.gDir2dRC  # [[-1,0] = N, [0,1]=E, etc]\n",
        "        grcPos = array(rcPos)\n",
        "        grcMax = self.grid.shape\n",
        "\n",
        "        binTrans = self.get_full_transitions(*rcPos)  # 16bit integer - all trans in/out\n",
        "        lnBinTrans = array([binTrans >> 8, binTrans & 0xff], dtype=np.uint8)  # 2 x uint8\n",
        "        g2binTrans = np.unpackbits(lnBinTrans).reshape(4, 4)  # 4x4 x uint8 binary(0,1)\n",
        "        gDirOut = g2binTrans.any(axis=0)  # outbound directions as boolean array (4)\n",
        "        giDirOut = np.argwhere(gDirOut)[:, 0]  # valid outbound directions as array of int\n",
        "\n",
        "        # loop over available outbound directions (indices) for rcPos\n",
        "        for iDirOut in giDirOut:\n",
        "            gdRC = gDir2dRC[iDirOut]  # row,col increment\n",
        "            gPos2 = grcPos + gdRC  # next cell in that direction\n",
        "\n",
        "            # Check the adjacent cell is within bounds\n",
        "            # if not, then this transition is invalid!\n",
        "            if np.any(gPos2 < 0):\n",
        "                return False\n",
        "            if np.any(gPos2 >= grcMax):\n",
        "                return False\n",
        "\n",
        "            # Get the transitions out of gPos2, using iDirOut as the inbound direction\n",
        "            # if there are no available transitions, ie (0,0,0,0), then rcPos is invalid\n",
        "            t4Trans2 = self.get_transitions(*gPos2, iDirOut)\n",
        "            if any(t4Trans2):\n",
        "                continue\n",
        "            else:\n",
        "                self.set_transition((gPos2[0], gPos2[1], iDirOut), mirror(iDirOut), 1)\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def fix_transitions(self, rcPos: IntVector2DArray, direction: IntVector2D = -1):\n",
        "        \"\"\"\n",
        "        Fixes broken transitions\n",
        "        \"\"\"\n",
        "        gDir2dRC = self.transitions.gDir2dRC  # [[-1,0] = N, [0,1]=E, etc]\n",
        "        grcPos = array(rcPos)\n",
        "        grcMax = self.grid.shape\n",
        "        # Transition elements\n",
        "        transitions = RailEnvTransitions()\n",
        "        cells = transitions.transition_list\n",
        "        simple_switch_east_south = transitions.rotate_transition(cells[10], 90)\n",
        "        simple_switch_west_south = transitions.rotate_transition(cells[2], 270)\n",
        "        symmetrical = cells[6]\n",
        "        double_slip = cells[5]\n",
        "        three_way_transitions = [simple_switch_east_south, simple_switch_west_south]\n",
        "        # loop over available outbound directions (indices) for rcPos\n",
        "\n",
        "        incoming_connections = np.zeros(4)\n",
        "        for iDirOut in np.arange(4):\n",
        "            gdRC = gDir2dRC[iDirOut]  # row,col increment\n",
        "            gPos2 = grcPos + gdRC  # next cell in that direction\n",
        "\n",
        "            # Check the adjacent cell is within bounds\n",
        "            # if not, then ignore it for the count of incoming connections\n",
        "            if np.any(gPos2 < 0):\n",
        "                continue\n",
        "            if np.any(gPos2 >= grcMax):\n",
        "                continue\n",
        "\n",
        "            # Get the transitions out of gPos2, using iDirOut as the inbound direction\n",
        "            # if there are no available transitions, ie (0,0,0,0), then rcPos is invalid\n",
        "            connected = 0\n",
        "            for orientation in range(4):\n",
        "                connected += self.get_transition((gPos2[0], gPos2[1], orientation), mirror(iDirOut))\n",
        "            if connected > 0:\n",
        "                incoming_connections[iDirOut] = 1\n",
        "\n",
        "        number_of_incoming = np.sum(incoming_connections)\n",
        "        # Only one incoming direction --> Straight line set deadend\n",
        "        if number_of_incoming == 1:\n",
        "            if self.get_full_transitions(*rcPos) == 0:\n",
        "                self.set_transitions(rcPos, 0)\n",
        "            else:\n",
        "                self.set_transitions(rcPos, 0)\n",
        "\n",
        "                for direction in range(4):\n",
        "                    if incoming_connections[direction] > 0:\n",
        "                        self.set_transition((rcPos[0], rcPos[1], mirror(direction)), direction, 1)\n",
        "        # Connect all incoming connections\n",
        "        if number_of_incoming == 2:\n",
        "            self.set_transitions(rcPos, 0)\n",
        "\n",
        "            connect_directions = np.argwhere(incoming_connections > 0)\n",
        "            self.set_transition((rcPos[0], rcPos[1], mirror(connect_directions[0])), connect_directions[1], 1)\n",
        "            self.set_transition((rcPos[0], rcPos[1], mirror(connect_directions[1])), connect_directions[0], 1)\n",
        "\n",
        "        # Find feasible connection for three entries\n",
        "        if number_of_incoming == 3:\n",
        "            self.set_transitions(rcPos, 0)\n",
        "            hole = np.argwhere(incoming_connections < 1)[0][0]\n",
        "            if direction >= 0:\n",
        "                switch_type_idx = (direction - hole + 3) % 4\n",
        "                if switch_type_idx == 0:\n",
        "                    transition = simple_switch_west_south\n",
        "                elif switch_type_idx == 2:\n",
        "                    transition = simple_switch_east_south\n",
        "                else:\n",
        "                    transition = self.random_generator.choice(three_way_transitions, 1)\n",
        "            else:\n",
        "                transition = self.random_generator.choice(three_way_transitions, 1)\n",
        "            transition = transitions.rotate_transition(transition, int(hole * 90))\n",
        "            self.set_transitions((rcPos[0], rcPos[1]), transition)\n",
        "\n",
        "        # Make a double slip switch\n",
        "        if number_of_incoming == 4:\n",
        "            rotation = self.random_generator.randint(2)\n",
        "            transition = transitions.rotate_transition(double_slip, int(rotation * 90))\n",
        "            self.set_transitions((rcPos[0], rcPos[1]), transition)\n",
        "        return True\n",
        "\n",
        "    def validate_new_transition(self, prev_pos: IntVector2D, current_pos: IntVector2D,\n",
        "                                new_pos: IntVector2D, end_pos: IntVector2D):\n",
        "        \"\"\"\n",
        "        Utility function to test that a path drawn by a-start algorithm uses valid transition objects.\n",
        "        We us this to quide a-star as there are many transition elements that are not allowed in RailEnv\n",
        "\n",
        "        :param prev_pos: The previous position we were checking\n",
        "        :param current_pos: The current position we are checking\n",
        "        :param new_pos: Possible child position we move into\n",
        "        :param end_pos: End cell of path we are drawing\n",
        "        :return: True if the transition is valid, False if transition element is illegal\n",
        "        \"\"\"\n",
        "        # start by getting direction used to get to current node\n",
        "        # and direction from current node to possible child node\n",
        "        new_dir = get_direction(current_pos, new_pos)\n",
        "        if prev_pos is not None:\n",
        "            current_dir = get_direction(prev_pos, current_pos)\n",
        "        else:\n",
        "            current_dir = new_dir\n",
        "        # create new transition that would go to child\n",
        "        new_trans = self.grid[current_pos]\n",
        "        if prev_pos is None:\n",
        "            if new_trans == 0:\n",
        "                # need to flip direction because of how end points are defined\n",
        "                new_trans = self.transitions.set_transition(new_trans, mirror(current_dir), new_dir, 1)\n",
        "            else:\n",
        "                # check if matches existing layout\n",
        "                new_trans = self.transitions.set_transition(new_trans, current_dir, new_dir, 1)\n",
        "        else:\n",
        "            # set the forward path\n",
        "            new_trans = self.transitions.set_transition(new_trans, current_dir, new_dir, 1)\n",
        "            # set the backwards path\n",
        "            new_trans = self.transitions.set_transition(new_trans, mirror(new_dir), mirror(current_dir), 1)\n",
        "        if Vec2d.is_equal(new_pos, end_pos):\n",
        "            # need to validate end pos setup as well\n",
        "            new_trans_e = self.grid[end_pos]\n",
        "            if new_trans_e == 0:\n",
        "                # need to flip direction because of how end points are defined\n",
        "                new_trans_e = self.transitions.set_transition(new_trans_e, new_dir, mirror(new_dir), 1)\n",
        "            else:\n",
        "                # check if matches existing layout\n",
        "                new_trans_e = self.transitions.set_transition(new_trans_e, new_dir, new_dir, 1)\n",
        "\n",
        "            if not self.transitions.is_valid(new_trans_e):\n",
        "                return False\n",
        "\n",
        "        # is transition is valid?\n",
        "        return self.transitions.is_valid(new_trans)\n",
        "\n",
        "    # Function to check if a transition is possible/valide or the train/agent can't go there\n",
        "    def check_transition_is_possible(self, previous_position, current_position, new_position):\n",
        "        # Finding direction of the agent\n",
        "\n",
        "        #print(left_rails, right_rails)\n",
        "\n",
        "        new_dir = get_direction(current_position, new_position)\n",
        "        if previous_position is not None:\n",
        "            current_dir = get_direction(previous_position, current_position)\n",
        "        else:\n",
        "            current_dir = new_dir\n",
        "\n",
        "        # checking the possible transitions\n",
        "        # EXAMPLE OF RESULT OF THE FUNCTION\n",
        "        # \n",
        "        #  [0, 1, 1, 0]   \n",
        "        #   N  E  S  W\n",
        "        transitions = self.get_transitions(current_position[0], current_position[1], current_dir)\n",
        "        #print(previous_position, current_position, transitions,)\n",
        "        \n",
        "        # Transition to NORD valid?\n",
        "        if (new_position[0] == (current_position[0] - 1)) and (transitions[0] == 1):\n",
        "            if new_dir  == Grid4TransitionsEnum.NORTH:\n",
        "                if new_position in down_rails:\n",
        "                    return False\n",
        "            return True\n",
        "        # Transition to EAST valid?\n",
        "        if (new_position[1] == (current_position[1] + 1)) and (transitions[1] == 1):\n",
        "            if new_dir  == Grid4TransitionsEnum.EAST:\n",
        "                if new_position in left_rails:\n",
        "                    return False\n",
        "            return True\n",
        "        # Transition to SUD valid?\n",
        "        if (new_position[0] == (current_position[0] + 1)) and (transitions[2] == 1):\n",
        "            if new_dir  == Grid4TransitionsEnum.SOUTH:\n",
        "                if new_position in up_rails:\n",
        "                    return False\n",
        "            return True\n",
        "        # Transition to WEST valid?\n",
        "        if (new_position[1] == (current_position[1] - 1)) and (transitions[3] == 1):\n",
        "            if new_dir  == Grid4TransitionsEnum.WEST:\n",
        "                if new_position in right_rails:\n",
        "                    return False\n",
        "            return True\n",
        "        else:\n",
        "            return False \n",
        "\n",
        "def mirror(dir):\n",
        "    return (dir + 2) % 4\n",
        "# TODO: improvement override __getitem__ and __setitem__ (cell contents, not transitions?)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1G5loAfPIi_"
      },
      "source": [
        "\n",
        "## Simulation import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mi0nPdXHZzXU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# In Flatland you can use custom observation builders and predicitors\n",
        "# Observation builders generate the observation needed by the controller\n",
        "# Preditctors can be used to do short time prediction which can help in avoiding conflicts in the network\n",
        "from flatland.envs.malfunction_generators import malfunction_from_params, MalfunctionParameters, ParamMalfunctionGen\n",
        "from flatland.envs.observations import TreeObsForRailEnv, GlobalObsForRailEnv\n",
        "# Import the railway generators\n",
        "from flatland.utils.rendertools import RenderTool, AgentRenderVariant\n",
        "# Import the agent class\n",
        "from flatland.envs.step_utils.states import TrainState\n",
        "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "j2XE2YoP1mJ3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%matplotlib inline\n",
        "\n",
        "# Helper function to visualize an episode\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
        "\n",
        "def display_episode(frames):\n",
        "    fig, ax = plt.subplots(figsize=(12,12))\n",
        "    imgplot = plt.imshow(frames[0])\n",
        "    def animate(i):\n",
        "        imgplot.set_data(frames[i])\n",
        "    animation = matplotlib.animation.FuncAnimation(fig, animate, frames=len(frames))\n",
        "    return animation\n",
        "\n",
        "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D9FmbZM78L3"
      },
      "source": [
        "## Simulation Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9biEfh5q8AVd"
      },
      "outputs": [],
      "source": [
        "obs_params = {\n",
        "    \"observation_tree_depth\": 2,\n",
        "    \"observation_radius\": 10,\n",
        "    \"observation_max_path_depth\": 30\n",
        "}\n",
        "\n",
        "training_flag = example_training\n",
        "\n",
        "###### TRAINING PARAMETERS #######\n",
        "n_episodes = 1000\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.99\n",
        "max_steps = 250     # 1440 one day\n",
        "checkpoint_interval = 100\n",
        "training_id = '0' \n",
        "render = False\n",
        "\n",
        "######### FLAGS ##########\n",
        "# Flag for the first training\n",
        "training_flag = example_training\n",
        "# Flag active in case of interruptions\n",
        "interruption = True\n",
        "# Flag to select the agent ----> multi agent or external controller\n",
        "multi_agent = True\n",
        "# Flag to save the video or not\n",
        "video_save = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai4RdjiP8NHB"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fmxJfjM2qTJH"
      },
      "outputs": [],
      "source": [
        "# Check the maximum possible delay...180 not good for now\n",
        "def calculate_metric(env, timetable):\n",
        "    positions = env.cur_episode\n",
        "    prev_station = 0\n",
        "    delta = 400 \n",
        "    metric_result = []\n",
        "    for i_agent in range(env.get_num_agents()):\n",
        "        if not env.agents[i_agent].state == TrainState.MALFUNCTION:\n",
        "            station_vector = [delta] * len(timetable[i_agent][0])\n",
        "            for i_station in range(len(timetable[i_agent][0])):\n",
        "                for step in range(len(positions)):\n",
        "                    if positions[step][i_agent] == timetable[i_agent][0][i_station] and positions[step][i_agent] != prev_station:\n",
        "                        prev_station = positions[step][i_agent]\n",
        "                        distance_delay = ((step - timetable[i_agent][1][i_station])**2)**(1/2)\n",
        "                        station_vector[i_station] = distance_delay\n",
        "            metric_result.append(station_vector)\n",
        "    metric_sum = sum(sum(x) for x in metric_result)\n",
        "    dimension = 0\n",
        "    for i in range(len(metric_result)):\n",
        "        for j in range(len(metric_result[i])):\n",
        "            dimension += 1\n",
        "    metric_normalized = 1 - (metric_sum / (delta*dimension))\n",
        "    return metric_normalized\n",
        "\n",
        "def choose_a_random_training_configuration(env, max_steps):\n",
        "    case = 0\n",
        "    if case == 0:\n",
        "        make_a_deterministic_interruption(env.agents[1], max_steps)\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "        return    \n",
        "    elif case == 1:\n",
        "        env.agents[1].initial_position = (6,15)\n",
        "        env.agents[2].initial_position = (5,15)\n",
        "        make_a_deterministic_interruption(env.agents[1], max_steps)\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "        return\n",
        "    elif case == 2:\n",
        "        make_a_deterministic_interruption(env.agents[1], max_steps)\n",
        "        env.agents[2].malfunction_handler.malfunction_down_counter = max_steps\n",
        "        return       \n",
        "    elif case == 3:\n",
        "        env.agents[1].malfunction_handler.malfunction_down_counter = max_steps\n",
        "        env.agents[2].initial_position = (5,15)\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "        return\n",
        "\n",
        "def format_action_prob(action_probs):\n",
        "    action_probs = np.round(action_probs, 3)\n",
        "    actions = [\"↻\", \"←\", \"↑\", \"→\", \"◼\", \"↓\"]\n",
        "\n",
        "    buffer = \"\"\n",
        "    for action, action_prob in zip(actions, action_probs):\n",
        "        buffer += action + \" \" + \"{:.3f}\".format(action_prob) + \" \"\n",
        "\n",
        "    return buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c5rkMFTPLg4",
        "outputId": "1fcbad67-58ba-42f9-fc45-dcca761364b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Calculating the timetable\n",
            "\n",
            "===================================================================================================================================\n",
            "Attention!!! Agent number 1 has a problem in the timetable, times to reach stations 0 and 1 are not right\n",
            "The time to reach the next station SHOULD BE HIGHER. The minimum time to reach the station should be: 15.0\n",
            "===================================================================================================================================\n",
            "Attention!!! Agent number 2 has a problem in the timetable, times to reach stations 0 and 1 are not right\n",
            "The time to reach the next station SHOULD BE HIGHER. The minimum time to reach the station should be: 13.0\n",
            "[[(6, 2), (6, 13), (6, 23)], [3, 30, 55], 0.5]\n",
            "[[(6, 8), (5, 13)], [1, 13], 0.5]\n",
            "[[(5, 8), (5, 13)], [1, 13], 0.5]\n",
            "\n",
            "------- Calculating the action scheduled\n",
            "\n",
            "[<RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>]\n",
            "\n",
            "\n",
            "[<RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>]\n",
            "\n",
            "\n",
            "[<RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>]\n",
            "\n",
            "[<RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>]\n",
            "[<RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>]\n",
            "[<RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.MOVE_FORWARD: 2>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>, <RailEnvActions.STOP_MOVING: 4>]\n",
            "\n",
            " The following agents can register an action:\n",
            "========================================\n",
            "Agent 0 needs to submit an action.\n",
            "Agent 1 needs to submit an action.\n",
            "Agent 2 needs to submit an action.\n",
            "\n",
            "Start episode...\n",
            "\n",
            "Episode Nr. 0\t Score = -213.5400000000004\n",
            "🚂 Episode 0\t 🏆 Score: -0.285 Avg: -0.993\t 💯 Done: 0.00% Avg: 0.00%\t 🎲 Epsilon: 0.990 \t 🔀 Action Probs: ↻ 0.160 ← 0.198 ↑ 0.137 → 0.178 ◼ 0.150 ↓ 0.176 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 1\t Score = -212.84000000000037\n",
            "🚂 Episode 1\t 🏆 Score: -0.284 Avg: -0.986\t 💯 Done: 0.00% Avg: 0.00%\t 🎲 Epsilon: 0.980 \t 🔀 Action Probs: ↻ 0.159 ← 0.193 ↑ 0.189 → 0.171 ◼ 0.130 ↓ 0.157 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 2\t Score = -212.84000000000037\n",
            "🚂 Episode 2\t 🏆 Score: -0.284 Avg: -0.979\t 💯 Done: 0.00% Avg: 0.00%\t 🎲 Epsilon: 0.970 \t 🔀 Action Probs: ↻ 0.213 ← 0.188 ↑ 0.145 → 0.162 ◼ 0.152 ↓ 0.141 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 3\t Score = -212.84000000000037\n",
            "🚂 Episode 3\t 🏆 Score: -0.284 Avg: -0.972\t 💯 Done: 0.00% Avg: 0.00%\t 🎲 Epsilon: 0.961 \t 🔀 Action Probs: ↻ 0.170 ← 0.178 ↑ 0.205 → 0.164 ◼ 0.150 ↓ 0.134 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 4\t Score = -212.84000000000037\n",
            "🚂 Episode 4\t 🏆 Score: -0.284 Avg: -0.965\t 💯 Done: 0.00% Avg: 0.00%\t 🎲 Epsilon: 0.951 \t 🔀 Action Probs: ↻ 0.152 ← 0.162 ↑ 0.148 → 0.183 ◼ 0.158 ↓ 0.197 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 5\t Score = -35.52000000000011\n",
            "🚂 Episode 5\t 🏆 Score: -0.047 Avg: -0.956\t 💯 Done: 33.33% Avg: 0.33%\t 🎲 Epsilon: 0.941 \t 🔀 Action Probs: ↻ 0.161 ← 0.233 ↑ 0.144 → 0.178 ◼ 0.189 ↓ 0.094 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 6\t Score = -212.84000000000037\n",
            "🚂 Episode 6\t 🏆 Score: -0.284 Avg: -0.949\t 💯 Done: 0.00% Avg: 0.33%\t 🎲 Epsilon: 0.932 \t 🔀 Action Probs: ↻ 0.169 ← 0.175 ↑ 0.183 → 0.181 ◼ 0.146 ↓ 0.146 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 7\t Score = -212.84000000000037\n",
            "🚂 Episode 7\t 🏆 Score: -0.284 Avg: -0.942\t 💯 Done: 0.00% Avg: 0.33%\t 🎲 Epsilon: 0.923 \t 🔀 Action Probs: ↻ 0.166 ← 0.200 ↑ 0.162 → 0.136 ◼ 0.188 ↓ 0.148 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 8\t Score = -212.84000000000037\n",
            "🚂 Episode 8\t 🏆 Score: -0.284 Avg: -0.936\t 💯 Done: 0.00% Avg: 0.32%\t 🎲 Epsilon: 0.914 \t 🔀 Action Probs: ↻ 0.137 ← 0.219 ↑ 0.159 → 0.149 ◼ 0.162 ↓ 0.174 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 9\t Score = -212.84000000000037\n",
            "🚂 Episode 9\t 🏆 Score: -0.284 Avg: -0.929\t 💯 Done: 0.00% Avg: 0.32%\t 🎲 Epsilon: 0.904 \t 🔀 Action Probs: ↻ 0.188 ← 0.164 ↑ 0.176 → 0.178 ◼ 0.126 ↓ 0.168 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 10\t Score = -212.84000000000037\n",
            "🚂 Episode 10\t 🏆 Score: -0.284 Avg: -0.923\t 💯 Done: 0.00% Avg: 0.32%\t 🎲 Epsilon: 0.895 \t 🔀 Action Probs: ↻ 0.154 ← 0.178 ↑ 0.130 → 0.229 ◼ 0.160 ↓ 0.150 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 11\t Score = -212.84000000000037\n",
            "🚂 Episode 11\t 🏆 Score: -0.284 Avg: -0.916\t 💯 Done: 0.00% Avg: 0.31%\t 🎲 Epsilon: 0.886 \t 🔀 Action Probs: ↻ 0.171 ← 0.196 ↑ 0.149 → 0.202 ◼ 0.157 ↓ 0.124 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 12\t Score = -33.490000000000094\n",
            "🚂 Episode 12\t 🏆 Score: -0.045 Avg: -0.908\t 💯 Done: 33.33% Avg: 0.64%\t 🎲 Epsilon: 0.878 \t 🔀 Action Probs: ↻ 0.145 ← 0.220 ↑ 0.132 → 0.226 ◼ 0.170 ↓ 0.107 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 13\t Score = -35.52000000000011\n",
            "🚂 Episode 13\t 🏆 Score: -0.047 Avg: -0.899\t 💯 Done: 33.33% Avg: 0.97%\t 🎲 Epsilon: 0.869 \t 🔀 Action Probs: ↻ 0.133 ← 0.189 ↑ 0.150 → 0.233 ◼ 0.183 ↓ 0.111 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 14\t Score = -212.84000000000037\n",
            "🚂 Episode 14\t 🏆 Score: -0.284 Avg: -0.893\t 💯 Done: 0.00% Avg: 0.96%\t 🎲 Epsilon: 0.860 \t 🔀 Action Probs: ↻ 0.159 ← 0.173 ↑ 0.155 → 0.229 ◼ 0.161 ↓ 0.124 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 15\t Score = -37.55000000000011\n",
            "🚂 Episode 15\t 🏆 Score: -0.050 Avg: -0.885\t 💯 Done: 33.33% Avg: 1.28%\t 🎲 Epsilon: 0.851 \t 🔀 Action Probs: ↻ 0.181 ← 0.246 ↑ 0.146 → 0.095 ◼ 0.201 ↓ 0.131 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 16\t Score = -212.84000000000037\n",
            "🚂 Episode 16\t 🏆 Score: -0.284 Avg: -0.878\t 💯 Done: 0.00% Avg: 1.27%\t 🎲 Epsilon: 0.843 \t 🔀 Action Probs: ↻ 0.166 ← 0.137 ↑ 0.121 → 0.145 ◼ 0.270 ↓ 0.160 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 17\t Score = -212.84000000000037\n",
            "🚂 Episode 17\t 🏆 Score: -0.284 Avg: -0.873\t 💯 Done: 0.00% Avg: 1.26%\t 🎲 Epsilon: 0.835 \t 🔀 Action Probs: ↻ 0.154 ← 0.234 ↑ 0.144 → 0.150 ◼ 0.193 ↓ 0.125 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 18\t Score = -212.84000000000037\n",
            "🚂 Episode 18\t 🏆 Score: -0.284 Avg: -0.867\t 💯 Done: 0.00% Avg: 1.25%\t 🎲 Epsilon: 0.826 \t 🔀 Action Probs: ↻ 0.123 ← 0.188 ↑ 0.162 → 0.170 ◼ 0.221 ↓ 0.136 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 19\t Score = -212.84000000000037\n",
            "🚂 Episode 19\t 🏆 Score: -0.284 Avg: -0.861\t 💯 Done: 0.00% Avg: 1.23%\t 🎲 Epsilon: 0.818 \t 🔀 Action Probs: ↻ 0.121 ← 0.261 ↑ 0.136 → 0.128 ◼ 0.223 ↓ 0.130 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 20\t Score = -36.1000000000001\n",
            "🚂 Episode 20\t 🏆 Score: -0.048 Avg: -0.853\t 💯 Done: 33.33% Avg: 1.56%\t 🎲 Epsilon: 0.810 \t 🔀 Action Probs: ↻ 0.086 ← 0.247 ↑ 0.134 → 0.177 ◼ 0.210 ↓ 0.145 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 21\t Score = -212.84000000000037\n",
            "🚂 Episode 21\t 🏆 Score: -0.284 Avg: -0.847\t 💯 Done: 0.00% Avg: 1.54%\t 🎲 Epsilon: 0.802 \t 🔀 Action Probs: ↻ 0.183 ← 0.176 ↑ 0.164 → 0.156 ◼ 0.201 ↓ 0.120 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 22\t Score = -212.84000000000037\n",
            "🚂 Episode 22\t 🏆 Score: -0.284 Avg: -0.841\t 💯 Done: 0.00% Avg: 1.52%\t 🎲 Epsilon: 0.794 \t 🔀 Action Probs: ↻ 0.154 ← 0.268 ↑ 0.219 → 0.122 ◼ 0.120 ↓ 0.116 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 23\t Score = -212.84000000000037\n",
            "🚂 Episode 23\t 🏆 Score: -0.284 Avg: -0.836\t 💯 Done: 0.00% Avg: 1.51%\t 🎲 Epsilon: 0.786 \t 🔀 Action Probs: ↻ 0.163 ← 0.130 ↑ 0.148 → 0.159 ◼ 0.248 ↓ 0.152 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 24\t Score = -212.84000000000037\n",
            "🚂 Episode 24\t 🏆 Score: -0.284 Avg: -0.830\t 💯 Done: 0.00% Avg: 1.49%\t 🎲 Epsilon: 0.778 \t 🔀 Action Probs: ↻ 0.180 ← 0.125 ↑ 0.148 → 0.117 ◼ 0.273 ↓ 0.158 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 25\t Score = -212.84000000000037\n",
            "🚂 Episode 25\t 🏆 Score: -0.284 Avg: -0.825\t 💯 Done: 0.00% Avg: 1.48%\t 🎲 Epsilon: 0.770 \t 🔀 Action Probs: ↻ 0.207 ← 0.134 ↑ 0.138 → 0.140 ◼ 0.203 ↓ 0.178 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 26\t Score = -212.84000000000037\n",
            "🚂 Episode 26\t 🏆 Score: -0.284 Avg: -0.819\t 💯 Done: 0.00% Avg: 1.46%\t 🎲 Epsilon: 0.762 \t 🔀 Action Probs: ↻ 0.156 ← 0.132 ↑ 0.136 → 0.211 ◼ 0.211 ↓ 0.152 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 27\t Score = -36.1000000000001\n",
            "🚂 Episode 27\t 🏆 Score: -0.048 Avg: -0.812\t 💯 Done: 33.33% Avg: 1.78%\t 🎲 Epsilon: 0.755 \t 🔀 Action Probs: ↻ 0.125 ← 0.163 ↑ 0.196 → 0.212 ◼ 0.147 ↓ 0.158 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 28\t Score = -35.2300000000001\n",
            "🚂 Episode 28\t 🏆 Score: -0.047 Avg: -0.804\t 💯 Done: 33.33% Avg: 2.10%\t 🎲 Epsilon: 0.747 \t 🔀 Action Probs: ↻ 0.154 ← 0.183 ↑ 0.091 → 0.114 ◼ 0.097 ↓ 0.360 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 29\t Score = -38.42000000000012\n",
            "🚂 Episode 29\t 🏆 Score: -0.051 Avg: -0.797\t 💯 Done: 33.33% Avg: 2.41%\t 🎲 Epsilon: 0.740 \t 🔀 Action Probs: ↻ 0.129 ← 0.133 ↑ 0.143 → 0.119 ◼ 0.229 ↓ 0.248 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 30\t Score = -212.84000000000037\n",
            "🚂 Episode 30\t 🏆 Score: -0.284 Avg: -0.791\t 💯 Done: 0.00% Avg: 2.39%\t 🎲 Epsilon: 0.732 \t 🔀 Action Probs: ↻ 0.120 ← 0.207 ↑ 0.112 → 0.220 ◼ 0.213 ↓ 0.128 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 31\t Score = -212.84000000000037\n",
            "🚂 Episode 31\t 🏆 Score: -0.284 Avg: -0.786\t 💯 Done: 0.00% Avg: 2.36%\t 🎲 Epsilon: 0.725 \t 🔀 Action Probs: ↻ 0.132 ← 0.164 ↑ 0.154 → 0.199 ◼ 0.193 ↓ 0.158 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 32\t Score = -212.84000000000037\n",
            "🚂 Episode 32\t 🏆 Score: -0.284 Avg: -0.781\t 💯 Done: 0.00% Avg: 2.34%\t 🎲 Epsilon: 0.718 \t 🔀 Action Probs: ↻ 0.124 ← 0.223 ↑ 0.122 → 0.126 ◼ 0.221 ↓ 0.183 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 33\t Score = -212.84000000000037\n",
            "🚂 Episode 33\t 🏆 Score: -0.284 Avg: -0.776\t 💯 Done: 0.00% Avg: 2.32%\t 🎲 Epsilon: 0.711 \t 🔀 Action Probs: ↻ 0.152 ← 0.186 ↑ 0.115 → 0.188 ◼ 0.202 ↓ 0.158 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 34\t Score = -212.84000000000037\n",
            "🚂 Episode 34\t 🏆 Score: -0.284 Avg: -0.771\t 💯 Done: 0.00% Avg: 2.29%\t 🎲 Epsilon: 0.703 \t 🔀 Action Probs: ↻ 0.115 ← 0.272 ↑ 0.134 → 0.117 ◼ 0.171 ↓ 0.191 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 35\t Score = -212.84000000000037\n",
            "🚂 Episode 35\t 🏆 Score: -0.284 Avg: -0.767\t 💯 Done: 0.00% Avg: 2.27%\t 🎲 Epsilon: 0.696 \t 🔀 Action Probs: ↻ 0.177 ← 0.160 ↑ 0.125 → 0.288 ◼ 0.132 ↓ 0.119 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 36\t Score = -212.84000000000037\n",
            "🚂 Episode 36\t 🏆 Score: -0.284 Avg: -0.762\t 💯 Done: 0.00% Avg: 2.25%\t 🎲 Epsilon: 0.689 \t 🔀 Action Probs: ↻ 0.123 ← 0.188 ↑ 0.105 → 0.186 ◼ 0.150 ↓ 0.249 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 37\t Score = -38.71000000000012\n",
            "🚂 Episode 37\t 🏆 Score: -0.052 Avg: -0.755\t 💯 Done: 33.33% Avg: 2.56%\t 🎲 Epsilon: 0.683 \t 🔀 Action Probs: ↻ 0.145 ← 0.140 ↑ 0.140 → 0.333 ◼ 0.130 ↓ 0.111 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 38\t Score = -212.84000000000037\n",
            "🚂 Episode 38\t 🏆 Score: -0.284 Avg: -0.750\t 💯 Done: 0.00% Avg: 2.53%\t 🎲 Epsilon: 0.676 \t 🔀 Action Probs: ↻ 0.157 ← 0.143 ↑ 0.098 → 0.188 ◼ 0.216 ↓ 0.198 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 39\t Score = -212.84000000000037\n",
            "🚂 Episode 39\t 🏆 Score: -0.284 Avg: -0.745\t 💯 Done: 0.00% Avg: 2.51%\t 🎲 Epsilon: 0.669 \t 🔀 Action Probs: ↻ 0.229 ← 0.155 ↑ 0.108 → 0.231 ◼ 0.153 ↓ 0.124 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 40\t Score = -39.87000000000013\n",
            "🚂 Episode 40\t 🏆 Score: -0.053 Avg: -0.738\t 💯 Done: 33.33% Avg: 2.82%\t 🎲 Epsilon: 0.662 \t 🔀 Action Probs: ↻ 0.124 ← 0.106 ↑ 0.097 → 0.240 ◼ 0.189 ↓ 0.244 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 41\t Score = -38.71000000000012\n",
            "🚂 Episode 41\t 🏆 Score: -0.052 Avg: -0.731\t 💯 Done: 33.33% Avg: 3.12%\t 🎲 Epsilon: 0.656 \t 🔀 Action Probs: ↻ 0.142 ← 0.156 ↑ 0.114 → 0.142 ◼ 0.185 ↓ 0.261 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 42\t Score = -41.320000000000135\n",
            "🚂 Episode 42\t 🏆 Score: -0.055 Avg: -0.725\t 💯 Done: 33.33% Avg: 3.42%\t 🎲 Epsilon: 0.649 \t 🔀 Action Probs: ↻ 0.128 ← 0.303 ↑ 0.098 → 0.120 ◼ 0.162 ↓ 0.188 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 43\t Score = -212.84000000000037\n",
            "🚂 Episode 43\t 🏆 Score: -0.284 Avg: -0.720\t 💯 Done: 0.00% Avg: 3.39%\t 🎲 Epsilon: 0.643 \t 🔀 Action Probs: ↻ 0.125 ← 0.183 ↑ 0.127 → 0.167 ◼ 0.160 ↓ 0.238 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 44\t Score = -40.45000000000013\n",
            "🚂 Episode 44\t 🏆 Score: -0.054 Avg: -0.714\t 💯 Done: 33.33% Avg: 3.69%\t 🎲 Epsilon: 0.636 \t 🔀 Action Probs: ↻ 0.074 ← 0.108 ↑ 0.069 → 0.169 ◼ 0.277 ↓ 0.303 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 45\t Score = -212.84000000000037\n",
            "🚂 Episode 45\t 🏆 Score: -0.284 Avg: -0.709\t 💯 Done: 0.00% Avg: 3.65%\t 🎲 Epsilon: 0.630 \t 🔀 Action Probs: ↻ 0.124 ← 0.230 ↑ 0.136 → 0.098 ◼ 0.215 ↓ 0.197 \t Metric 0.33333333333333337 \n",
            "Episode Nr. 46\t Score = -42.77000000000014\n",
            "🚂 Episode 46\t 🏆 Score: -0.057 Avg: -0.703\t 💯 Done: 33.33% Avg: 3.95%\t 🎲 Epsilon: 0.624 \t 🔀 Action Probs: ↻ 0.093 ← 0.134 ↑ 0.093 → 0.215 ◼ 0.259 ↓ 0.206 \t Metric 0.33333333333333337 "
          ]
        }
      ],
      "source": [
        "\n",
        "# The specs for the custom railway generation are taken from structures.py file\n",
        "specs = railway_example\n",
        "\n",
        "widht = len(specs[0])\n",
        "height = len(specs)\n",
        "\n",
        "stations_position = []\n",
        "\n",
        "# Defining the name of the different stations\n",
        "for i in range(1, len(stations)):\n",
        "    stations_position.append(stations[i][0])\n",
        "\n",
        "# Timetable conteins the station where the train should pass, from starting station to aim, and conteins the time at which\n",
        "# each train has to pass in the station, the last number represent the velocity of train (high velocity, intercity or regional)\n",
        "# Each row represent a different train\n",
        "\n",
        "print('------ Calculating the timetable')\n",
        "print()\n",
        "timetable = timetable_example\n",
        "\n",
        "# Number of agents is the rows of the timetable\n",
        "num_of_agents = len(timetable)\n",
        "\n",
        "# Check if the timetable is feaseble or not, the function is in schedule_generators\n",
        "# A timetable is feaseble if the difference of times between two stations is positive and let the trains to reach the successive station\n",
        "# if two stations are very distant from each other the difference of times can't be very small\n",
        "seed = 2\n",
        "\n",
        "# Generating the railway topology, with stations\n",
        "# Arguments of the generator (specs of the railway, position of stations, timetable)\n",
        "rail_custom = rail_custom_generator(specs, stations_position, timetable)\n",
        "\n",
        "transition_map_example, agent_hints = rail_custom(widht, height, num_of_agents)\n",
        "\n",
        "divide_trains_in_station_rails(timetable, transition_map_example)\n",
        "\n",
        "control_timetable(timetable,transition_map_example)\n",
        "\n",
        "for i in range(len(timetable)):\n",
        "    print(timetable[i])\n",
        " \n",
        "time.sleep(3)\n",
        "\n",
        "# We can now initiate the schedule generator with the given speed profiles\n",
        "schedule_generator_custom = custom_schedule_generator(timetable = timetable)\n",
        "\n",
        "print()\n",
        "print('------- Calculating the action scheduled')\n",
        "actions_scheduled = action_to_do(timetable, transition_map_example)\n",
        "\n",
        "# DEBUG\n",
        "for i in range(len(actions_scheduled)):\n",
        "    print()\n",
        "    print(actions_scheduled[i])\n",
        "    print()\n",
        "\n",
        "time.sleep(3)\n",
        "\n",
        "if multi_agent:\n",
        "\n",
        "    observation_parameters = Namespace(**obs_params)\n",
        "\n",
        "    observation_tree_depth = observation_parameters.observation_tree_depth\n",
        "    observation_radius = observation_parameters.observation_radius\n",
        "    observation_max_path_depth = observation_parameters.observation_max_path_depth\n",
        "\n",
        "    # Observation builder\n",
        "    predictor = ShortestPathPredictorForRailEnv(observation_max_path_depth)\n",
        "    Observer = TreeObsForRailEnv(max_depth=observation_tree_depth, predictor=predictor)\n",
        "else:\n",
        "    Observer = GlobalObsForRailEnv()\n",
        "    # Ricordarsi che noi vogliamo applicare il RL solo in un intorno della linea dove c'è stata l'interruzione\n",
        "    # Vogliamo in questo caso un osservatore globale? Forsse meglio valutarne anche uno limitato\n",
        "    # Ragiona se costruire un osservatore che consideri solo i binari possa essere tanto vantaggioso o no?\n",
        "\n",
        "stochastic_data = MalfunctionParameters(\n",
        "    malfunction_rate = 0,  # Rate of malfunction occurence\n",
        "    min_duration = 15,  # Minimal duration of malfunction\n",
        "    max_duration = 40  # Max duration of malfunction\n",
        ")\n",
        "\n",
        "malfunction_generator = ParamMalfunctionGen(stochastic_data)\n",
        "\n",
        "env = RailEnv(  width= widht,\n",
        "                height= height,\n",
        "                rail_generator = rail_custom,\n",
        "                line_generator=schedule_generator_custom,\n",
        "                number_of_agents= num_of_agents,\n",
        "                malfunction_generator = malfunction_generator,\n",
        "                obs_builder_object=Observer,\n",
        "                remove_agents_at_target=True,\n",
        "                record_steps=True,\n",
        "                max_episode_steps = max_steps - 1\n",
        "                )\n",
        "\n",
        "env.reset()\n",
        "\n",
        "\n",
        "# If I want I can delay a specific train a specific time\n",
        "'''\n",
        "delay_a_train(delay = 250, train = env.agents[1], delay_time = 2, time_of_train_generation = 1, actions = actions_scheduled)\n",
        "delay_a_train(delay = 250, train = env.agents[2], delay_time = 2, time_of_train_generation = 1, actions = actions_scheduled)\n",
        "'''\n",
        "\n",
        "for i in range(len(actions_scheduled)):\n",
        "    print(actions_scheduled[i])\n",
        "\n",
        "env_renderer = RenderTool(env,\n",
        "                          screen_height=300,\n",
        "                          screen_width=300)  # Adjust these parameters to fit your resolution\n",
        "\n",
        "\n",
        "# This thing is importand for the RL part, initialize the agent with (state, action) dimension\n",
        "# Initialize the agent with the parameters corresponding to the environment and observation_builder\n",
        "if multi_agent:\n",
        "    n_agents = env.get_num_agents()\n",
        "    n_features_per_node = env.obs_builder.observation_dim\n",
        "    n_nodes = sum([np.power(4, i) for i in range(observation_tree_depth + 1)])\n",
        "    state_size = n_features_per_node * n_nodes\n",
        "\n",
        "    action_size = env.action_space[0]\n",
        "\n",
        "    action_count = [0] * action_size\n",
        "    action_dict = dict()\n",
        "    agent_obs = [None] * n_agents\n",
        "    agent_prev_obs = [None] * n_agents\n",
        "    agent_prev_action = [2] * n_agents\n",
        "    update_values = [False] * n_agents\n",
        "\n",
        "    controller = RandomAgent(state_size, action_size)\n",
        "\n",
        "    # Smoothed values used as target for hyperparameter tuning\n",
        "    smoothed_normalized_score = -1.0\n",
        "    smoothed_eval_normalized_score = -1.0\n",
        "    smoothed_completion = 0.0\n",
        "    smoothed_eval_completion = 0.0\n",
        "\n",
        "    train_params = training_params\n",
        "\n",
        "    policy = DDDQNPolicy(state_size, action_size, train_params)\n",
        "\n",
        "    # TensorBoard writer\n",
        "    writer = SummaryWriter()\n",
        "    writer.add_hparams(vars(train_params), {})\n",
        "    writer.add_hparams(vars(train_params), {})\n",
        "    writer.add_hparams(vars(observation_parameters), {})\n",
        "\n",
        "    training_timer = Timer()\n",
        "    training_timer.start()\n",
        "\n",
        "else:\n",
        "    n_agents = env.get_num_agents()\n",
        "    state_size = (widht * height)\n",
        "    # The number of actions is the combination of the number of actions by the number of agents\n",
        "    action_size = env.action_space[0] ** env.get_num_agents()\n",
        "\n",
        "    action_count = [0] * action_size\n",
        "    action_dict = dict()\n",
        "    agent_obs = [None] * n_agents\n",
        "    agent_prev_obs = [None] * n_agents\n",
        "    agent_prev_action = [2] * n_agents\n",
        "    update_values = [False] * n_agents\n",
        "\n",
        "    controller = RandomAgent(state_size, action_size)\n",
        "\n",
        "    q_table = np.zeros([state_size, action_size])\n",
        "\n",
        "\n",
        "    alpha = 0.1\n",
        "    gamma = 0.6\n",
        "    epsilon = 0.1\n",
        "\n",
        "    # For plotting metrics\n",
        "    all_epochs = []\n",
        "    all_penalties = []\n",
        "\n",
        "\n",
        "# Lets try to enter with all of these agents at the same time\n",
        "action_dict = dict()\n",
        "\n",
        "# Now that you have seen these novel concepts that were introduced you will realize that agents don't need to take\n",
        "# an action at every time step as it will only change the outcome when actions are chosen at cell entry.\n",
        "# Therefore the environment provides information about what agents need to provide an action in the next step.\n",
        "# You can access this in the following way.\n",
        "\n",
        "# Chose an action for each agent\n",
        "for a in range(env.get_num_agents()):\n",
        "    action = controller.act(0)\n",
        "    action_dict.update({a: action})\n",
        "# Do the environment step\n",
        "\n",
        "observations, rewards, dones, information = env.step(action_dict)\n",
        "\n",
        "print(\"\\n The following agents can register an action:\")\n",
        "print(\"========================================\")\n",
        "for info in information['action_required']:\n",
        "    print(\"Agent {} needs to submit an action.\".format(info))\n",
        "\n",
        "# We recommend that you monitor the malfunction data and the action required in order to optimize your training\n",
        "# and controlling code.\n",
        "\n",
        "# Let us now look at an episode playing out \n",
        "\n",
        "print(\"\\nStart episode...\")\n",
        "\n",
        "# Reset the rendering system\n",
        "env_renderer.reset()\n",
        "\n",
        "# Here you can also further enhance the provided observation by means of normalization\n",
        "# See training navigation example in the baseline repository\n",
        "\n",
        "score = 0\n",
        "# Run episode\n",
        "frame_step = 0\n",
        "\n",
        "frames = []\n",
        "\n",
        "os.makedirs(\"output/frames\", exist_ok=True)\n",
        "\n",
        "for episode_idx in range(n_episodes + 1):\n",
        "    \n",
        "    deterministic_interruption_activation = False\n",
        "\n",
        "    step_timer = Timer()\n",
        "    reset_timer = Timer()\n",
        "    learn_timer = Timer()\n",
        "    preproc_timer = Timer()\n",
        "    inference_timer = Timer()\n",
        "\n",
        "    # Reset environment\n",
        "    reset_timer.start()\n",
        "\n",
        "    # Reset environment and get initial observations for all agents\n",
        "    obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "    reset_timer.end()\n",
        "    for idx in range(env.get_num_agents()):\n",
        "        tmp_agent = env.agents[idx]\n",
        "        tmp_agent.speed_counter.speed = 1 / (idx + 1)  # TODO rigestisci le velocità iniziali\n",
        "    env_renderer.reset()\n",
        "\n",
        "    if train_params.render:\n",
        "        env_renderer.set_new_rail()\n",
        "\n",
        "    score = 0\n",
        "    nb_steps = 0\n",
        "    actions_taken = []\n",
        "\n",
        "    if multi_agent:\n",
        "        # Build initial agent-specific observations\n",
        "        for agent in env.get_agent_handles():\n",
        "            if obs[agent]:\n",
        "                agent_obs[agent] = normalize_observation(obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
        "                agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "    else:\n",
        "        for agent in env.get_agent_handles():\n",
        "            agent_obs[agent] = obs[agent]\n",
        "            agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "\n",
        "    # Run episode (one day long, 1 step is 1 minute) 1440\n",
        "    for step in range(max_steps):\n",
        "        if video_save:\n",
        "            env_renderer.gl.save_image(\"output/frames/flatland_frame_step_{:04d}.bmp\".format(step))\n",
        "\n",
        "        inference_timer.start() \n",
        "\n",
        "    # Here define the actions to do\n",
        "        # Broken agents\n",
        "        # Broken agents\n",
        "        if training_flag == 'training0' and not deterministic_interruption_activation:\n",
        "            choose_a_random_training_configuration(env, max_steps)\n",
        "        if training_flag == 'training1':\n",
        "            make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "            make_a_deterministic_interruption(env.agents[3], max_steps)\n",
        "        if training_flag == 'training1.1':\n",
        "            make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "\n",
        "        # Chose an action for each agent in the environment\n",
        "        # If not interruption, the actions to do are stored in a matrix\n",
        "        #       - each row of the matrix is a train\n",
        "        #       - each column represent the action the train has to do at each time instant\n",
        "        \n",
        "        for a in range(env.get_num_agents()):\n",
        "            if env.agents[a].state == TrainState.DONE:\n",
        "                env.dones[a] = True\n",
        "            if env.agents[a].state == TrainState.MALFUNCTION:\n",
        "                interruption = True\n",
        "            if not multi_agent and interruption: # debug \n",
        "                break\n",
        "            if step >= timetable[a][1][0]:\n",
        "                # Normal plan to follow\n",
        "                if not interruption and (step - timetable[a][1][0]) < len(actions_scheduled[a]):\n",
        "                    action = actions_scheduled[a][step - timetable[a][1][0]]\n",
        "                # Interruption\n",
        "                elif interruption:\n",
        "                    if multi_agent:\n",
        "                        if info['action_required'][a]:\n",
        "                            update_values[a] = True\n",
        "                            action = policy.act(agent_obs[a], eps=eps_start)\n",
        "\n",
        "                            action_count[action] += 1\n",
        "                            actions_taken.append(action)\n",
        "                        else:\n",
        "                            # An action is not required if the train hasn't joined the railway network,\n",
        "                            # if it already reached its target, or if is currently malfunctioning.\n",
        "                            update_values[a] = False\n",
        "                            action = 0\n",
        "                    else:\n",
        "                        action = np.random.choice([RailEnvActions.MOVE_FORWARD, RailEnvActions.MOVE_RIGHT, RailEnvActions.MOVE_LEFT, \n",
        "                        RailEnvActions.STOP_MOVING, RailEnvActions.REVERSE])\n",
        "                # choose random from all the possible actions\n",
        "                else:\n",
        "                    action = np.random.choice([RailEnvActions.MOVE_FORWARD, RailEnvActions.MOVE_RIGHT, RailEnvActions.MOVE_LEFT, \n",
        "                        RailEnvActions.STOP_MOVING, RailEnvActions.REVERSE])\n",
        "\n",
        "                action_dict.update({a: action})\n",
        "\n",
        "\n",
        "        inference_timer.end()\n",
        "\n",
        "        # Environment step which returns the observations for all agents, their corresponding\n",
        "        # reward and whether their are done\n",
        "        # Environment step\n",
        "        step_timer.start()\n",
        "        next_obs, all_rewards, done, info = env.step(action_dict)\n",
        "        step_timer.end()\n",
        "\n",
        "        # Update replay buffer and train agent\n",
        "        if multi_agent:\n",
        "            for agent in env.get_agent_handles():\n",
        "                if update_values[agent] or done['__all__']:\n",
        "                    # Only learn from timesteps where somethings happened\n",
        "                    learn_timer.start()\n",
        "                    policy.step(agent_prev_obs[agent], agent_prev_action[agent], all_rewards[agent], agent_obs[agent], done[agent])\n",
        "                    learn_timer.end()\n",
        "\n",
        "                    agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "                    agent_prev_action[agent] = action_dict[agent]\n",
        "\n",
        "                # Preprocess the new observations\n",
        "                if next_obs[agent]:\n",
        "                    preproc_timer.start()\n",
        "                    agent_obs[agent] = normalize_observation(next_obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
        "                    preproc_timer.end()\n",
        "\n",
        "                score += all_rewards[agent]\n",
        "\n",
        "            nb_steps = step\n",
        "        else:\n",
        "            for a in range(env.get_num_agents()):\n",
        "                controller.step((obs[a], action_dict[a], all_rewards[a], next_obs[a], done[a]))\n",
        "                score += all_rewards[a]\n",
        "        obs = next_obs.copy()\n",
        "        if done['__all__']:\n",
        "            break\n",
        "        #break if the first agent has done\n",
        "        if ((training_flag == 'training0') and (env.dones[0] == True)) or \\\n",
        "            ((training_flag == 'training1') and (env.dones[0] == True) and (env.dones[1] == True)) or \\\n",
        "            ((training_flag == 'training1.1') and (env.dones[0] == True) and (env.dones[1] == True)):\n",
        "            break\n",
        "    print()\n",
        "    print('Episode Nr. {}\\t Score = {}'.format(episode_idx, score))\n",
        "\n",
        "    # metric most possible near to 0\n",
        "    metric = calculate_metric(env, timetable)\n",
        "    \n",
        "    if multi_agent:\n",
        "        # Epsilon decay\n",
        "        eps_start = max(eps_end, eps_decay * eps_start)\n",
        "\n",
        "        # Collect information about training\n",
        "        tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "        completion = tasks_finished / max(1, env.get_num_agents())\n",
        "        normalized_score = score / (max_steps * env.get_num_agents())\n",
        "        action_probs = action_count / np.sum(action_count)\n",
        "        action_count = [1] * action_size\n",
        "\n",
        "        smoothing = 0.99\n",
        "        smoothed_normalized_score = smoothed_normalized_score * smoothing + normalized_score * (1.0 - smoothing)\n",
        "        smoothed_completion = smoothed_completion * smoothing + completion * (1.0 - smoothing)\n",
        "\n",
        "        # Print logs\n",
        "        if episode_idx % checkpoint_interval == 0:\n",
        "            '''\n",
        "            torch.save(policy.qnetwork_local, './checkpoints/' + training_id + '-' + str(episode_idx) + '.pth')\n",
        "            if save_replay_buffer:\n",
        "                policy.save_replay_buffer('./replay_buffers/' + training_id + '-' + str(episode_idx) + '.pkl')\n",
        "            '''\n",
        "\n",
        "            if train_params.render:\n",
        "                env_renderer.close_window()\n",
        "\n",
        "        print(\n",
        "            '\\r🚂 Episode {}'\n",
        "            '\\t 🏆 Score: {:.3f}'\n",
        "            ' Avg: {:.3f}'\n",
        "            '\\t 💯 Done: {:.2f}%'\n",
        "            ' Avg: {:.2f}%'\n",
        "            '\\t 🎲 Epsilon: {:.3f} '\n",
        "            '\\t 🔀 Action Probs: {}'\n",
        "            '\\t Metric {}'.format(\n",
        "                episode_idx,\n",
        "                normalized_score,\n",
        "                smoothed_normalized_score,\n",
        "                100 * completion,\n",
        "                100 * smoothed_completion,\n",
        "                eps_start,\n",
        "                format_action_prob(action_probs),\n",
        "                metric\n",
        "            ), end=\" \")\n",
        "\n",
        "    interruption = False\n",
        "    writer.add_scalar(\"Reward\", score, episode_idx)\n",
        "    writer.add_scalar(\"Metric\", metric, episode_idx)\n",
        "    writer.flush()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKvqy_tTyDcb"
      },
      "source": [
        "## Training render"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNa83T4yx7_J"
      },
      "source": [
        "## Reward and metric values "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx6BqGMZtb9P"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir=runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2wENop47ZHr"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8ToFBwB7sLQ"
      },
      "source": [
        "## Test 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2w_qS1Xunmm"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "##### TEST 1 ####\n",
        "#################\n",
        "# Reset environment and get initial observations for all agents\n",
        "env.reset()\n",
        "# Reset the rendering system\n",
        "env_renderer.reset()\n",
        "# Change the position of the interrupted agents\n",
        "env.agents[1].initial_position = (6,10)\n",
        "env.agents[2].initial_position = (5,10)\n",
        "\n",
        "frame_step = 0\n",
        "frames = []\n",
        "score = 0\n",
        "\n",
        "for step in range(max_steps):\n",
        "    # Broken agents\n",
        "    make_a_deterministic_interruption(env.agents[1], max_steps)\n",
        "    make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "    update_values[0] = True\n",
        "    action = policy.act(agent_obs[0])\n",
        "\n",
        "    action_count[action] += 1\n",
        "    actions_taken.append(action)\n",
        "    action_dict.update({0: action})\n",
        "    \n",
        "    next_obs, all_rewards, done, info = env.step(action_dict)\n",
        "    \n",
        "    score += all_rewards[0]\n",
        "\n",
        "    frame = env_renderer.render_env(show=False, show_observations=False, show_inactive_agents=False, show_predictions=False, return_image=True)\n",
        "    frames.append(frame)\n",
        "    frame_step += 1\n",
        "    \n",
        "    if done['__all__']:\n",
        "        break\n",
        "    #break if the first agent has done\n",
        "    if ((training_flag == 'training0') and (env.dones[0] == True)) or \\\n",
        "        ((training_flag == 'training1') and (env.dones[0] == True) and (env.dones[1] == True)) or \\\n",
        "        ((training_flag == 'training1.1') and (env.dones[0] == True) and (env.dones[1] == True)):\n",
        "        break\n",
        "\n",
        "# metric most possible near to 0\n",
        "metric = calculate_metric(env, timetable)\n",
        "\n",
        "tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "\n",
        "print(  'Test 1 concluded:'\n",
        "        '\\t 🏆 Score: {:.3f}'\n",
        "        '\\t Agent completed {}'\n",
        "        '\\t Metric {}'.format(\n",
        "            score,\n",
        "            tasks_finished,\n",
        "            metric\n",
        "        ), end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTImFlkk8c2X"
      },
      "outputs": [],
      "source": [
        "# RENDER\n",
        "%%capture\n",
        "animation = display_episode(frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3c80YLP8tBG"
      },
      "outputs": [],
      "source": [
        "animation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUftVUZWuqV9"
      },
      "source": [
        "## Test 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAprhHdvutU-"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "##### TEST 2 ####\n",
        "#################\n",
        "# Reset the rendering system\n",
        "env_renderer.reset()\n",
        "# Reset environment and get initial observations for all agents\n",
        "obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "# Change the position of the interrupted agents\n",
        "env.agents[1].initial_position = (6,8)\n",
        "env.agents[2].initial_position = (-1,0)\n",
        "\n",
        "frame_step = 0\n",
        "frames = []\n",
        "score = 0\n",
        "\n",
        "for step in range(max_steps):\n",
        "    # Broken agents\n",
        "    if training_flag == 'training0':\n",
        "        make_a_deterministic_interruption(env.agents[1], max_steps)\n",
        "        env.agents[2].malfunction_handler.malfunction_down_counter = max_steps\n",
        "    if training_flag == 'training1':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "        make_a_deterministic_interruption(env.agents[3], max_steps)\n",
        "    if training_flag == 'training1.1':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "\n",
        "    update_values[0] = True\n",
        "    action = policy.act(agent_obs[0])\n",
        "\n",
        "    action_count[action] += 1\n",
        "    actions_taken.append(action)\n",
        "    action_dict.update({0: action})\n",
        "    \n",
        "    next_obs, all_rewards, done, info = env.step(action_dict)\n",
        "    \n",
        "    score += all_rewards[0]\n",
        "\n",
        "    frame = env_renderer.render_env(show=False, show_observations=False, show_inactive_agents=True, show_predictions=False, return_image=True)\n",
        "    frames.append(frame)\n",
        "    frame_step += 1\n",
        "    \n",
        "    if done['__all__']:\n",
        "        break\n",
        "    #break if the first agent has done\n",
        "    if ((training_flag == 'training0') and (env.dones[0] == True)) or \\\n",
        "        ((training_flag == 'training1') and (env.dones[0] == True) and (env.dones[1] == True)) or \\\n",
        "        ((training_flag == 'training1.1') and (env.dones[0] == True) and (env.dones[1] == True)):\n",
        "        break\n",
        "\n",
        "# metric most possible near to 0\n",
        "metric = calculate_metric(env, timetable)\n",
        "\n",
        "tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "\n",
        "print(  'Test 2 concluded:'\n",
        "        '\\t 🏆 Score: {:.3f}'\n",
        "        '\\t Agent completed {}'\n",
        "        '\\t Metric {}'.format(\n",
        "            score,\n",
        "            tasks_finished,\n",
        "            metric\n",
        "        ), end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2Dedr6x1V0G"
      },
      "outputs": [],
      "source": [
        "# RENDER\n",
        "%%capture\n",
        "animation = display_episode(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpmugMNg1YY8"
      },
      "outputs": [],
      "source": [
        "animation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpZFo8eNo59g"
      },
      "source": [
        "## Test 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCn-uMD2o9Q-"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "##### TEST 3 ####\n",
        "#################\n",
        "# Reset the rendering system\n",
        "env_renderer.reset()\n",
        "# Reset environment and get initial observations for all agents\n",
        "obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "# Change the position of the interrupted agents\n",
        "env.agents[1].initial_position = (6,14)\n",
        "env.agents[2].initial_position = (5,14)\n",
        "\n",
        "frame_step = 0\n",
        "frames = []\n",
        "score = 0\n",
        "\n",
        "for step in range(max_steps):\n",
        "    # Broken agents\n",
        "    if training_flag == 'training0':\n",
        "        make_a_deterministic_interruption(env.agents[1], max_steps)\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "    if training_flag == 'training1':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "        make_a_deterministic_interruption(env.agents[3], max_steps)\n",
        "    if training_flag == 'training1.1':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "\n",
        "    update_values[0] = True\n",
        "    action = policy.act(agent_obs[0])\n",
        "\n",
        "    action_count[action] += 1\n",
        "    actions_taken.append(action)\n",
        "    action_dict.update({0: action})\n",
        "    \n",
        "    next_obs, all_rewards, done, info = env.step(action_dict)\n",
        "    \n",
        "    score += all_rewards[0]\n",
        "\n",
        "    frame = env_renderer.render_env(show=False, show_observations=False, show_inactive_agents=True, show_predictions=False, return_image=True)\n",
        "    frames.append(frame)\n",
        "    frame_step += 1\n",
        "    \n",
        "    if done['__all__']:\n",
        "        break\n",
        "    #break if the first agent has done\n",
        "    if ((training_flag == 'training0') and (env.dones[0] == True)) or \\\n",
        "        ((training_flag == 'training1') and (env.dones[0] == True) and (env.dones[1] == True)) or \\\n",
        "        ((training_flag == 'training1.1') and (env.dones[0] == True) and (env.dones[1] == True)):\n",
        "        break\n",
        "\n",
        "# metric most possible near to 0\n",
        "metric = calculate_metric(env, timetable)\n",
        "\n",
        "tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "\n",
        "print(  'Test 3 concluded:'\n",
        "        '\\t 🏆 Score: {:.3f}'\n",
        "        '\\t Agent completed {}'\n",
        "        '\\t Metric {}'.format(\n",
        "            score,\n",
        "            tasks_finished,\n",
        "            metric\n",
        "        ), end=\" \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIeBUbCTugm4"
      },
      "outputs": [],
      "source": [
        "# RENDER\n",
        "%%capture\n",
        "animation = display_episode(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-twQYbWuiYl"
      },
      "outputs": [],
      "source": [
        "animation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xLye0LaphyN"
      },
      "source": [
        "## Test 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UaOxJwdpj02"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "##### TEST 4 ####\n",
        "#################\n",
        "# Reset the rendering system\n",
        "env_renderer.reset()\n",
        "# Reset environment and get initial observations for all agents\n",
        "obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "# Change the position of the interrupted agents\n",
        "env.agents[1].initial_position = (6,14)\n",
        "env.agents[2].initial_position = (-1,0)\n",
        "\n",
        "frame_step = 0\n",
        "frames = []\n",
        "score = 0\n",
        "\n",
        "for step in range(max_steps):\n",
        "    # Broken agents\n",
        "    if training_flag == 'training0':\n",
        "        make_a_deterministic_interruption(env.agents[1], max_steps)\n",
        "        env.agents[2].malfunction_handler.malfunction_down_counter = max_steps\n",
        "    if training_flag == 'training1':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "        make_a_deterministic_interruption(env.agents[3], max_steps)\n",
        "    if training_flag == 'training1.1':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "\n",
        "    update_values[0] = True\n",
        "    action = policy.act(agent_obs[0])\n",
        "\n",
        "    action_count[action] += 1\n",
        "    actions_taken.append(action)\n",
        "    action_dict.update({0: action})\n",
        "    \n",
        "    next_obs, all_rewards, done, info = env.step(action_dict)\n",
        "    \n",
        "    score += all_rewards[0]\n",
        "\n",
        "    frame = env_renderer.render_env(show=False, show_observations=False, show_inactive_agents=True, show_predictions=False, return_image=True)\n",
        "    frames.append(frame)\n",
        "    frame_step += 1\n",
        "    \n",
        "    if done['__all__']:\n",
        "        break\n",
        "    #break if the first agent has done\n",
        "    if ((training_flag == 'training0') and (env.dones[0] == True)) or \\\n",
        "        ((training_flag == 'training1') and (env.dones[0] == True) and (env.dones[1] == True)) or \\\n",
        "        ((training_flag == 'training1.1') and (env.dones[0] == True) and (env.dones[1] == True)):\n",
        "        break\n",
        "\n",
        "\n",
        "# metric most possible near to 0\n",
        "metric = calculate_metric(env, timetable)\n",
        "\n",
        "tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "\n",
        "print(  'Test 4 concluded:'\n",
        "        '\\t 🏆 Score: {:.3f}'\n",
        "        '\\t Agent completed {}'\n",
        "        '\\t Metric {}'.format(\n",
        "            score,\n",
        "            tasks_finished,\n",
        "            metric\n",
        "        ), end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSiGB4A-ulWj"
      },
      "outputs": [],
      "source": [
        "# RENDER\n",
        "%%capture\n",
        "animation = display_episode(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYPnyghkuoib"
      },
      "outputs": [],
      "source": [
        "animation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfV1Rwf2pyZi"
      },
      "source": [
        "## Test 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyzeDw9Jpz4-"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "##### TEST 5 ####\n",
        "#################\n",
        "# Reset the rendering system\n",
        "env_renderer.reset()\n",
        "# Reset environment and get initial observations for all agents\n",
        "obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "# Change the position of the interrupted agents\n",
        "env.agents[2].initial_position = (5,10)\n",
        "env.agents[1].initial_position = (-1,0)\n",
        "\n",
        "frame_step = 0\n",
        "frames = []\n",
        "score = 0\n",
        "\n",
        "for step in range(max_steps):\n",
        "    # Broken agents\n",
        "    if training_flag == 'training0':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "        env.agents[1].malfunction_handler.malfunction_down_counter = max_steps\n",
        "    if training_flag == 'training1':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "        make_a_deterministic_interruption(env.agents[3], max_steps)\n",
        "    if training_flag == 'training1.1':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "\n",
        "    update_values[0] = True\n",
        "    action = policy.act(agent_obs[0])\n",
        "\n",
        "    action_count[action] += 1\n",
        "    actions_taken.append(action)\n",
        "    action_dict.update({0: action})\n",
        "    \n",
        "    next_obs, all_rewards, done, info = env.step(action_dict)\n",
        "    \n",
        "    score += all_rewards[0]\n",
        "\n",
        "    frame = env_renderer.render_env(show=False, show_observations=False, show_inactive_agents=True, show_predictions=False, return_image=True)\n",
        "    frames.append(frame)\n",
        "    frame_step += 1\n",
        "    \n",
        "    if done['__all__']:\n",
        "        break\n",
        "    #break if the first agent has done\n",
        "    if ((training_flag == 'training0') and (env.dones[0] == True)) or \\\n",
        "        ((training_flag == 'training1') and (env.dones[0] == True) and (env.dones[1] == True)) or \\\n",
        "        ((training_flag == 'training1.1') and (env.dones[0] == True) and (env.dones[1] == True)):\n",
        "        break\n",
        "\n",
        "# metric most possible near to 0\n",
        "metric = calculate_metric(env, timetable)\n",
        "\n",
        "tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "\n",
        "print(  'Test 5 concluded:'\n",
        "        '\\t 🏆 Score: {:.3f}'\n",
        "        '\\t Agent completed {}'\n",
        "        '\\t Metric {}'.format(\n",
        "            score,\n",
        "            tasks_finished,\n",
        "            metric\n",
        "        ), end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL5TbUo1urPH"
      },
      "outputs": [],
      "source": [
        "# RENDER\n",
        "%%capture\n",
        "animation = display_episode(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km7S_lCQut9v"
      },
      "outputs": [],
      "source": [
        "animation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqYWxSVgp2gb"
      },
      "source": [
        "## Test 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2RoBn-yp34X"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "##### TEST 6 ####\n",
        "#################\n",
        "# Reset the rendering system\n",
        "env_renderer.reset()\n",
        "# Reset environment and get initial observations for all agents\n",
        "obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "# Change the position of the interrupted agents\n",
        "env.agents[2].initial_position = (5,16)\n",
        "env.agents[1].initial_position = (-1,0)\n",
        "\n",
        "frame_step = 0\n",
        "frames = []\n",
        "score = 0\n",
        "\n",
        "for step in range(max_steps):\n",
        "    # Broken agents\n",
        "    if training_flag == 'training0':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "        env.agents[1].malfunction_handler.malfunction_down_counter = max_steps\n",
        "    if training_flag == 'training1':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "        make_a_deterministic_interruption(env.agents[3], max_steps)\n",
        "    if training_flag == 'training1.1':\n",
        "        make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "\n",
        "    update_values[0] = True\n",
        "    action = policy.act(agent_obs[0])\n",
        "\n",
        "    action_count[action] += 1\n",
        "    actions_taken.append(action)\n",
        "    action_dict.update({0: action})\n",
        "    \n",
        "    next_obs, all_rewards, done, info = env.step(action_dict)\n",
        "    \n",
        "    score += all_rewards[0]\n",
        "\n",
        "    frame = env_renderer.render_env(show=False, show_observations=False, show_inactive_agents=True, show_predictions=False, return_image=True)\n",
        "    frames.append(frame)\n",
        "    frame_step += 1\n",
        "    \n",
        "    if done['__all__']:\n",
        "        break\n",
        "    #break if the first agent has done\n",
        "    if ((training_flag == 'training0') and (env.dones[0] == True)) or \\\n",
        "        ((training_flag == 'training1') and (env.dones[0] == True) and (env.dones[1] == True)) or \\\n",
        "        ((training_flag == 'training1.1') and (env.dones[0] == True) and (env.dones[1] == True)):\n",
        "        break\n",
        "\n",
        "# metric most possible near to 0\n",
        "metric = calculate_metric(env, timetable)\n",
        "\n",
        "tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "\n",
        "print(  'Test 6 concluded:'\n",
        "        '\\t 🏆 Score: {:.3f}'\n",
        "        '\\t Agent completed {}'\n",
        "        '\\t Metric {}'.format(\n",
        "            score,\n",
        "            tasks_finished,\n",
        "            metric\n",
        "        ), end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmiRC_ENuvp1"
      },
      "outputs": [],
      "source": [
        "# RENDER\n",
        "%%capture\n",
        "animation = display_episode(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIpbrzOAuxFi"
      },
      "outputs": [],
      "source": [
        "animation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmcjxW99ygXw"
      },
      "source": [
        "## Test Prossimamente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPM0Y3RIyoly"
      },
      "outputs": [],
      "source": [
        "env_renderer = RenderTool(env, gl=\"PGL\", screen_width=500, screen_height=500, show_debug=False)\n",
        "\n",
        "# Change the position of the interrupted agents\n",
        "env.reset()\n",
        "env_renderer.reset()\n",
        "\n",
        "# Casual Malfunctions for the agents\n",
        "stochastic_data = MalfunctionParameters(\n",
        "    malfunction_rate = 0.001,  # Rate of malfunction occurence\n",
        "    min_duration = 15,  # Minimal duration of malfunction\n",
        "    max_duration = 60  # Max duration of malfunction\n",
        ")\n",
        "\n",
        "malfunction_generator = ParamMalfunctionGen(stochastic_data)\n",
        "\n",
        "env = RailEnv(  width= widht,\n",
        "                height= height,\n",
        "                rail_generator = rail_custom,\n",
        "                line_generator=schedule_generator_custom,\n",
        "                number_of_agents= num_of_agents,\n",
        "                malfunction_generator = malfunction_generator,\n",
        "                obs_builder_object=Observer,\n",
        "                remove_agents_at_target=True,\n",
        "                record_steps=True,\n",
        "                max_episode_steps = max_steps - 1\n",
        "                )\n",
        "\n",
        "env.reset()\n",
        "\n",
        "#################\n",
        "##### TEST 3 ####\n",
        "#################\n",
        "for episodes in range(20):\n",
        "  \n",
        "  frame_step = 0\n",
        "  frames = []\n",
        "\n",
        "  for step in range(max_steps):\n",
        "      # Broken agents\n",
        "      if training_flag == 'training0':\n",
        "          make_a_deterministic_interruption(env.agents[1], max_steps)\n",
        "          make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "      if training_flag == 'training1':\n",
        "          make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "          make_a_deterministic_interruption(env.agents[3], max_steps)\n",
        "      if training_flag == 'training1.1':\n",
        "          make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "      for a in range(env.get_num_agents()):\n",
        "          update_values[a] = True\n",
        "          action = policy.act(agent_obs[a], eps=eps_start)\n",
        "\n",
        "          action_count[action] += 1\n",
        "          actions_taken.append(action)\n",
        "          action_dict.update({a: action})\n",
        "          \n",
        "          next_obs, all_rewards, done, info = env.step(action_dict)\n",
        "\n",
        "          frame = env_renderer.render_env(show=False, show_observations=False, show_inactive_agents=True, show_predictions=False, return_image=True)\n",
        "          frames.append(frame)\n",
        "          frame_step += 1\n",
        "      \n",
        "      if done['__all__']:\n",
        "          break\n",
        "      #break if the first agent has done\n",
        "      if ((training_flag == 'training0') and (env.dones[0] == True)) or \\\n",
        "          ((training_flag == 'training1') and (env.dones[0] == True) and (env.dones[1] == True)) or \\\n",
        "          ((training_flag == 'training1.1') and (env.dones[0] == True) and (env.dones[1] == True)):\n",
        "          break\n",
        "\n",
        "  # metric most possible near to 0\n",
        "  metric = calculate_metric(env, timetable)\n",
        "\n",
        "  tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "\n",
        "  print()\n",
        "  print(  'Test 1 concluded:'\n",
        "          '\\t 🏆 Score: {:.3f}'\n",
        "          '\\t Agent completed {}'\n",
        "          '\\t Metric {}'.format(\n",
        "              score,\n",
        "              tasks_finished,\n",
        "              metric\n",
        "          ), end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA1-zlSP0d_u"
      },
      "outputs": [],
      "source": [
        "# RENDER\n",
        "%%capture\n",
        "animation = display_episode(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaGuqKZv0i-2"
      },
      "outputs": [],
      "source": [
        "animation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8C2D4VVH366B",
        "2e5ILy1PYV9o",
        "25VWq_QpYWH0",
        "tEEkAbhEXvTb",
        "2tCkoOAPX3WZ",
        "Q80GIJJEX8Yk",
        "PcqtL8kcYBb3",
        "fDV9yKClYIy_",
        "_B01cYfcYNyt",
        "CmcjxW99ygXw"
      ],
      "name": "Flatland-Railways.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}