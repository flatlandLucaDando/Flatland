{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJSba3dHGwZq8QcJMQKFBh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flatlandLucaDando/Flatland/blob/flatland_v_3_deterministic/Prova.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwdBB1CvQrs1"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "import random\n",
        "import sys\n",
        "from argparse import ArgumentParser, Namespace\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "import psutil\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch\n",
        "\n",
        "# In Flatland you can use custom observation builders and predicitors\n",
        "# Observation builders generate the observation needed by the controller\n",
        "# Preditctors can be used to do short time prediction which can help in avoiding conflicts in the network\n",
        "from flatland.envs.malfunction_generators import malfunction_from_params, MalfunctionParameters, ParamMalfunctionGen\n",
        "from flatland.envs.observations import TreeObsForRailEnv, GlobalObsForRailEnv\n",
        "# First of all we import the Flatland rail environment\n",
        "from flatland.envs.rail_env import RailEnv\n",
        "from flatland.envs.rail_env import RailEnvActions\n",
        "# Import the railway generators\n",
        "from flatland.envs.custom_rail_generator import rail_custom_generator\n",
        "from flatland.envs.rail_env_utils import delay_a_train, make_a_deterministic_interruption\n",
        "from flatland.utils.rendertools import RenderTool, AgentRenderVariant\n",
        "# Import the schedule generators\n",
        "from flatland.envs.custom_schedule_generator import custom_schedule_generator\n",
        "from flatland.envs.plan_to_follow_utils import action_to_do, divide_trains_in_station_rails, control_timetable\n",
        "# Import the different structures needed\n",
        "from configuration import railway_example, stations, timetable_example, example_training\n",
        "# Import the agent class\n",
        "from flatland.envs.agent import RandomAgent\n",
        "from flatland.envs.step_utils.states import TrainState\n",
        "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n",
        "\n",
        "from flatland.utils.timer import Timer\n",
        "from flatland.utils.observation_utils import normalize_observation\n",
        "from reinforcement_learning.dddqn_policy import DDDQNPolicy\n",
        "# Import training and observation parameters\n",
        "from parameters import training_params, obs_params\n",
        "\n",
        "\n",
        "def format_action_prob(action_probs):\n",
        "    action_probs = np.round(action_probs, 3)\n",
        "    actions = [\"↻\", \"←\", \"↑\", \"→\", \"◼\", \"↓\"]\n",
        "\n",
        "    buffer = \"\"\n",
        "    for action, action_prob in zip(actions, action_probs):\n",
        "        buffer += action + \" \" + \"{:.3f}\".format(action_prob) + \" \"\n",
        "\n",
        "    return buffer\n",
        "\n",
        "\n",
        "###### TRAINING PARAMETERS #######\n",
        "n_episodes = 500\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.99\n",
        "max_steps = 250     # 1440 one day\n",
        "checkpoint_interval = 100\n",
        "training_id = '0' \n",
        "render = False\n",
        "\n",
        "######### FLAGS ##########\n",
        "# Flag for the first training\n",
        "training_flag = example_training\n",
        "# Flag active in case of interruptions\n",
        "interruption = True\n",
        "# Flag to select the agent ----> multi agent or external controller\n",
        "multi_agent = True\n",
        "# Flag to save the video or not\n",
        "video_save = False\n",
        "\n",
        "\n",
        "# The specs for the custom railway generation are taken from structures.py file\n",
        "specs = railway_example\n",
        "\n",
        "widht = len(specs[0])\n",
        "height = len(specs)\n",
        "\n",
        "stations_position = []\n",
        "\n",
        "# Defining the name of the different stations\n",
        "for i in range(1, len(stations)):\n",
        "    stations_position.append(stations[i][0])\n",
        "\n",
        "# Timetable conteins the station where the train should pass, from starting station to aim, and conteins the time at which\n",
        "# each train has to pass in the station, the last number represent the velocity of train (high velocity, intercity or regional)\n",
        "# Each row represent a different train\n",
        "\n",
        "print('------ Calculating the timetable')\n",
        "print()\n",
        "timetable = timetable_example\n",
        "\n",
        "# Number of agents is the rows of the timetable\n",
        "num_of_agents = len(timetable)\n",
        "\n",
        "# Check if the timetable is feaseble or not, the function is in schedule_generators\n",
        "# A timetable is feaseble if the difference of times between two stations is positive and let the trains to reach the successive station\n",
        "# if two stations are very distant from each other the difference of times can't be very small\n",
        "seed = 2\n",
        "\n",
        "# Generating the railway topology, with stations\n",
        "# Arguments of the generator (specs of the railway, position of stations, timetable)\n",
        "rail_custom = rail_custom_generator(specs, stations_position, timetable)\n",
        "\n",
        "transition_map_example, agent_hints = rail_custom(widht, height, num_of_agents)\n",
        "\n",
        "divide_trains_in_station_rails(timetable, transition_map_example)\n",
        "\n",
        "control_timetable(timetable,transition_map_example)\n",
        "\n",
        "for i in range(len(timetable)):\n",
        "    print(timetable[i])\n",
        " \n",
        "time.sleep(3)\n",
        "\n",
        "# We can now initiate the schedule generator with the given speed profiles\n",
        "schedule_generator_custom = custom_schedule_generator(timetable = timetable)\n",
        "\n",
        "print()\n",
        "print('------- Calculating the action scheduled')\n",
        "actions_scheduled = action_to_do(timetable, transition_map_example)\n",
        "\n",
        "# DEBUG\n",
        "for i in range(len(actions_scheduled)):\n",
        "    print()\n",
        "    print(actions_scheduled[i])\n",
        "    print()\n",
        "\n",
        "time.sleep(3)\n",
        "\n",
        "if multi_agent:\n",
        "\n",
        "    observation_parameters = Namespace(**obs_params)\n",
        "\n",
        "    observation_tree_depth = observation_parameters.observation_tree_depth\n",
        "    observation_radius = observation_parameters.observation_radius\n",
        "    observation_max_path_depth = observation_parameters.observation_max_path_depth\n",
        "\n",
        "    # Observation builder\n",
        "    predictor = ShortestPathPredictorForRailEnv(observation_max_path_depth)\n",
        "    Observer = TreeObsForRailEnv(max_depth=observation_tree_depth, predictor=predictor)\n",
        "else:\n",
        "    Observer = GlobalObsForRailEnv()\n",
        "    # Ricordarsi che noi vogliamo applicare il RL solo in un intorno della linea dove c'è stata l'interruzione\n",
        "    # Vogliamo in questo caso un osservatore globale? Forsse meglio valutarne anche uno limitato\n",
        "    # Ragiona se costruire un osservatore che consideri solo i binari possa essere tanto vantaggioso o no?\n",
        "\n",
        "stochastic_data = MalfunctionParameters(\n",
        "    malfunction_rate = 0,  # Rate of malfunction occurence\n",
        "    min_duration = 15,  # Minimal duration of malfunction\n",
        "    max_duration = 40  # Max duration of malfunction\n",
        ")\n",
        "\n",
        "malfunction_generator = ParamMalfunctionGen(stochastic_data)\n",
        "\n",
        "env = RailEnv(  width= widht,\n",
        "                height= height,\n",
        "                rail_generator = rail_custom,\n",
        "                line_generator=schedule_generator_custom,\n",
        "                number_of_agents= num_of_agents,\n",
        "                malfunction_generator = malfunction_generator,\n",
        "                obs_builder_object=Observer,\n",
        "                remove_agents_at_target=True,\n",
        "                record_steps=True,\n",
        "                max_episode_steps = max_steps - 1\n",
        "                )\n",
        "\n",
        "env.reset()\n",
        "\n",
        "\n",
        "# If I want I can delay a specific train a specific time\n",
        "'''\n",
        "delay_a_train(delay = 250, train = env.agents[1], delay_time = 2, time_of_train_generation = 1, actions = actions_scheduled)\n",
        "delay_a_train(delay = 250, train = env.agents[2], delay_time = 2, time_of_train_generation = 1, actions = actions_scheduled)\n",
        "'''\n",
        "\n",
        "for i in range(len(actions_scheduled)):\n",
        "    print(actions_scheduled[i])\n",
        "\n",
        "env_renderer = RenderTool(env,\n",
        "                          screen_height=480,\n",
        "                          screen_width=720)  # Adjust these parameters to fit your resolution\n",
        "\n",
        "# This thing is importand for the RL part, initialize the agent with (state, action) dimension\n",
        "# Initialize the agent with the parameters corresponding to the environment and observation_builder\n",
        "if multi_agent:\n",
        "    n_agents = env.get_num_agents()\n",
        "    n_features_per_node = env.obs_builder.observation_dim\n",
        "    n_nodes = sum([np.power(4, i) for i in range(observation_tree_depth + 1)])\n",
        "    state_size = n_features_per_node * n_nodes\n",
        "\n",
        "    action_size = env.action_space[0]\n",
        "\n",
        "    action_count = [0] * action_size\n",
        "    action_dict = dict()\n",
        "    agent_obs = [None] * n_agents\n",
        "    agent_prev_obs = [None] * n_agents\n",
        "    agent_prev_action = [2] * n_agents\n",
        "    update_values = [False] * n_agents\n",
        "\n",
        "    controller = RandomAgent(state_size, action_size)\n",
        "\n",
        "    # Smoothed values used as target for hyperparameter tuning\n",
        "    smoothed_normalized_score = -1.0\n",
        "    smoothed_eval_normalized_score = -1.0\n",
        "    smoothed_completion = 0.0\n",
        "    smoothed_eval_completion = 0.0\n",
        "\n",
        "    train_params = training_params\n",
        "\n",
        "    policy = DDDQNPolicy(state_size, action_size, train_params)\n",
        "\n",
        "    # TensorBoard writer\n",
        "    writer = SummaryWriter()\n",
        "    writer.add_hparams(vars(train_params), {})\n",
        "    writer.add_hparams(vars(train_params), {})\n",
        "    writer.add_hparams(vars(observation_parameters), {})\n",
        "\n",
        "    training_timer = Timer()\n",
        "    training_timer.start()\n",
        "\n",
        "else:\n",
        "    n_agents = env.get_num_agents()\n",
        "    state_size = (widht * height)\n",
        "    # The number of actions is the combination of the number of actions by the number of agents\n",
        "    action_size = env.action_space[0] ** env.get_num_agents()\n",
        "\n",
        "    action_count = [0] * action_size\n",
        "    action_dict = dict()\n",
        "    agent_obs = [None] * n_agents\n",
        "    agent_prev_obs = [None] * n_agents\n",
        "    agent_prev_action = [2] * n_agents\n",
        "    update_values = [False] * n_agents\n",
        "\n",
        "    controller = RandomAgent(state_size, action_size)\n",
        "\n",
        "    q_table = np.zeros([state_size, action_size])\n",
        "\n",
        "\n",
        "    alpha = 0.1\n",
        "    gamma = 0.6\n",
        "    epsilon = 0.1\n",
        "\n",
        "    # For plotting metrics\n",
        "    all_epochs = []\n",
        "    all_penalties = []\n",
        "\n",
        "\n",
        "# Lets try to enter with all of these agents at the same time\n",
        "action_dict = dict()\n",
        "\n",
        "# Now that you have seen these novel concepts that were introduced you will realize that agents don't need to take\n",
        "# an action at every time step as it will only change the outcome when actions are chosen at cell entry.\n",
        "# Therefore the environment provides information about what agents need to provide an action in the next step.\n",
        "# You can access this in the following way.\n",
        "\n",
        "# Chose an action for each agent\n",
        "for a in range(env.get_num_agents()):\n",
        "    action = controller.act(0)\n",
        "    action_dict.update({a: action})\n",
        "# Do the environment step\n",
        "\n",
        "observations, rewards, dones, information = env.step(action_dict)\n",
        "\n",
        "print(\"\\n The following agents can register an action:\")\n",
        "print(\"========================================\")\n",
        "for info in information['action_required']:\n",
        "    print(\"Agent {} needs to submit an action.\".format(info))\n",
        "\n",
        "# We recommend that you monitor the malfunction data and the action required in order to optimize your training\n",
        "# and controlling code.\n",
        "\n",
        "# Let us now look at an episode playing out \n",
        "\n",
        "print(\"\\nStart episode...\")\n",
        "\n",
        "# Reset the rendering system\n",
        "env_renderer.reset()\n",
        "\n",
        "# Here you can also further enhance the provided observation by means of normalization\n",
        "# See training navigation example in the baseline repository\n",
        "\n",
        "score = 0\n",
        "# Run episode\n",
        "frame_step = 0\n",
        "\n",
        "os.makedirs(\"output/frames\", exist_ok=True)\n",
        "\n",
        "for episode_idx in range(n_episodes + 1):\n",
        "\n",
        "    step_timer = Timer()\n",
        "    reset_timer = Timer()\n",
        "    learn_timer = Timer()\n",
        "    preproc_timer = Timer()\n",
        "    inference_timer = Timer()\n",
        "\n",
        "    # Reset environment\n",
        "    reset_timer.start()\n",
        "\n",
        "    # Reset environment and get initial observations for all agents\n",
        "    obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "    reset_timer.end()\n",
        "    for idx in range(env.get_num_agents()):\n",
        "        tmp_agent = env.agents[idx]\n",
        "        tmp_agent.speed_counter.speed = 1 / (idx + 1)  # TODO rigestisci le velocità iniziali\n",
        "    env_renderer.reset()\n",
        "\n",
        "    if train_params.render:\n",
        "        env_renderer.set_new_rail()\n",
        "\n",
        "    score = 0\n",
        "    nb_steps = 0\n",
        "    actions_taken = []\n",
        "\n",
        "    if multi_agent:\n",
        "        # Build initial agent-specific observations\n",
        "        for agent in env.get_agent_handles():\n",
        "            if obs[agent]:\n",
        "                agent_obs[agent] = normalize_observation(obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
        "                agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "    else:\n",
        "        for agent in env.get_agent_handles():\n",
        "            agent_obs[agent] = obs[agent]\n",
        "            agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "\n",
        "    # Run episode (one day long, 1 step is 1 minute) 1440\n",
        "    for step in range(max_steps):\n",
        "        if video_save:\n",
        "            env_renderer.gl.save_image(\"output/frames/flatland_frame_step_{:04d}.bmp\".format(step))\n",
        "\n",
        "        inference_timer.start() \n",
        "\n",
        "        # TRAINING\n",
        "        if training_flag == 'training0':\n",
        "            make_a_deterministic_interruption(env.agents[1], max_steps)\n",
        "            make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "        if training_flag == 'training1':\n",
        "            make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "            make_a_deterministic_interruption(env.agents[3], max_steps)\n",
        "        if training_flag == 'training1.1':\n",
        "            make_a_deterministic_interruption(env.agents[2], max_steps)\n",
        "\n",
        "\n",
        "    # Here define the actions to do\n",
        "\n",
        "        # Chose an action for each agent in the environment\n",
        "        # If not interruption, the actions to do are stored in a matrix\n",
        "        #       - each row of the matrix is a train\n",
        "        #       - each column represent the action the train has to do at each time instant\n",
        "        '''\n",
        "        print('=================================')\n",
        "        print('Elapsed time is', step, 'minutes')   # DEBUG\n",
        "\n",
        "\n",
        "        # DEBUG print the velocities\n",
        "        for agent in env.agents:\n",
        "            i_agent = agent.handle\n",
        "            print('Velocity of the', i_agent, 'agent is', agent.speed_counter.speed)\n",
        "        ''' \n",
        "        for a in range(env.get_num_agents()):\n",
        "            if env.agents[a].state == TrainState.DONE:\n",
        "                env.dones[a] = True\n",
        "            if env.agents[a].state == TrainState.MALFUNCTION:\n",
        "                interruption = True\n",
        "            if not multi_agent and interruption: # debug \n",
        "                break\n",
        "            if step >= timetable[a][1][0]:\n",
        "                # Normal plan to follow\n",
        "                if not interruption and (step - timetable[a][1][0]) < len(actions_scheduled[a]):\n",
        "                    action = actions_scheduled[a][step - timetable[a][1][0]]\n",
        "                # Interruption\n",
        "                elif interruption:\n",
        "                    if multi_agent:\n",
        "                        if info['action_required'][a]:\n",
        "                            update_values[a] = True\n",
        "                            action = policy.act(agent_obs[a], eps=eps_start)\n",
        "\n",
        "                            action_count[action] += 1\n",
        "                            actions_taken.append(action)\n",
        "                        else:\n",
        "                            # An action is not required if the train hasn't joined the railway network,\n",
        "                            # if it already reached its target, or if is currently malfunctioning.\n",
        "                            update_values[a] = False\n",
        "                            action = 0\n",
        "                    else:\n",
        "                        action = np.random.choice([RailEnvActions.MOVE_FORWARD, RailEnvActions.MOVE_RIGHT, RailEnvActions.MOVE_LEFT, \n",
        "                        RailEnvActions.STOP_MOVING, RailEnvActions.REVERSE])\n",
        "                # choose random from all the possible actions\n",
        "                else:\n",
        "                    action = np.random.choice([RailEnvActions.MOVE_FORWARD, RailEnvActions.MOVE_RIGHT, RailEnvActions.MOVE_LEFT, \n",
        "                        RailEnvActions.STOP_MOVING, RailEnvActions.REVERSE])\n",
        "\n",
        "                action_dict.update({a: action})\n",
        "        '''\n",
        "        if not multi_agent and interruption:\n",
        "            \n",
        "            for agents in range(env.get_num_agents()):\n",
        "                action_dict.update({agent : result[agent]})\n",
        "        '''\n",
        "        inference_timer.end()\n",
        "\n",
        "        # Environment step which returns the observations for all agents, their corresponding\n",
        "        # reward and whether their are done\n",
        "        # Environment step\n",
        "        step_timer.start()\n",
        "        next_obs, all_rewards, done, info = env.step(action_dict)\n",
        "        step_timer.end()\n",
        "\n",
        "        '''\n",
        "        print('================================')\n",
        "        print(env.agents[0])\n",
        "        print(env.agents[1])\n",
        "        print(timetable)\n",
        "        print('================================')\n",
        "        '''\n",
        "\n",
        "        # Render an episode at some interval\n",
        "        if render:\n",
        "            env_renderer.render_env(\n",
        "                    show=True,\n",
        "                    frames=False,\n",
        "                    show_observations=False,\n",
        "                    show_predictions=False\n",
        "                )\n",
        "        # Update replay buffer and train agent\n",
        "        if multi_agent:\n",
        "            for agent in env.get_agent_handles():\n",
        "                if update_values[agent] or done['__all__']:\n",
        "                    # Only learn from timesteps where somethings happened\n",
        "                    learn_timer.start()\n",
        "                    policy.step(agent_prev_obs[agent], agent_prev_action[agent], all_rewards[agent], agent_obs[agent], done[agent])\n",
        "                    learn_timer.end()\n",
        "\n",
        "                    agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "                    agent_prev_action[agent] = action_dict[agent]\n",
        "\n",
        "                # Preprocess the new observations\n",
        "                if next_obs[agent]:\n",
        "                    preproc_timer.start()\n",
        "                    agent_obs[agent] = normalize_observation(next_obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
        "                    preproc_timer.end()\n",
        "\n",
        "                score += all_rewards[agent]\n",
        "\n",
        "            nb_steps = step\n",
        "        else:\n",
        "            for a in range(env.get_num_agents()):\n",
        "                controller.step((obs[a], action_dict[a], all_rewards[a], next_obs[a], done[a]))\n",
        "                score += all_rewards[a]\n",
        "        obs = next_obs.copy()\n",
        "        if done['__all__']:\n",
        "            break\n",
        "        #break if the first agent has done\n",
        "        if ((training_flag == 'training0') and (env.dones[0] == True)) or \\\n",
        "            ((training_flag == 'training1') and (env.dones[0] == True) and (env.dones[1] == True)) or \\\n",
        "            ((training_flag == 'training1.1') and (env.dones[0] == True) and (env.dones[1] == True)):\n",
        "            break\n",
        "    print('Episode Nr. {}\\t Score = {}'.format(episode_idx, score))\n",
        "\n",
        "    if multi_agent:\n",
        "        # Epsilon decay\n",
        "        eps_start = max(eps_end, eps_decay * eps_start)\n",
        "\n",
        "        # Collect information about training\n",
        "        tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "        completion = tasks_finished / max(1, env.get_num_agents())\n",
        "        normalized_score = score / (max_steps * env.get_num_agents())\n",
        "        action_probs = action_count / np.sum(action_count)\n",
        "        action_count = [1] * action_size\n",
        "\n",
        "        smoothing = 0.99\n",
        "        smoothed_normalized_score = smoothed_normalized_score * smoothing + normalized_score * (1.0 - smoothing)\n",
        "        smoothed_completion = smoothed_completion * smoothing + completion * (1.0 - smoothing)\n",
        "\n",
        "        # Print logs\n",
        "        if episode_idx % checkpoint_interval == 0:\n",
        "            torch.save(policy.qnetwork_local, './checkpoints/' + training_id + '-' + str(episode_idx) + '.pth')\n",
        "\n",
        "            '''\n",
        "            if save_replay_buffer:\n",
        "                policy.save_replay_buffer('./replay_buffers/' + training_id + '-' + str(episode_idx) + '.pkl')\n",
        "            '''\n",
        "\n",
        "            if train_params.render:\n",
        "                env_renderer.close_window()\n",
        "\n",
        "        print(\n",
        "            '\\r🚂 Episode {}'\n",
        "            '\\t 🏆 Score: {:.3f}'\n",
        "            ' Avg: {:.3f}'\n",
        "            '\\t 💯 Done: {:.2f}%'\n",
        "            ' Avg: {:.2f}%'\n",
        "            '\\t 🎲 Epsilon: {:.3f} '\n",
        "            '\\t 🔀 Action Probs: {}'.format(\n",
        "                episode_idx,\n",
        "                normalized_score,\n",
        "                smoothed_normalized_score,\n",
        "                100 * completion,\n",
        "                100 * smoothed_completion,\n",
        "                eps_start,\n",
        "                format_action_prob(action_probs)\n",
        "            ), end=\" \")\n",
        "\n",
        "    # interruption = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KI0W1oQQy1H"
      },
      "source": [
        "Prova"
      ]
    }
  ]
}